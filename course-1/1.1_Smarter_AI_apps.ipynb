{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042c161c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe4cc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rsurs\\anaconda3\\envs\\ibm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ba0d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='A dog.' additional_kwargs={'reasoning_content': 'The user asks: \"Who is a man\\'s best friend?\" It\\'s a common phrase: \"man\\'s best friend\" refers to a dog. So answer: a dog. Probably a dog. The user might want a straightforward answer. No further context. Should I mention \"dog\" or \"canine\"? Just answer. Possibly add a brief explanation. The user might want a simple answer. The best friend: a dog. So answer that.'} response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 78, 'total_tokens': 179, 'completion_time': 0.102224251, 'completion_tokens_details': {'reasoning_tokens': 89}, 'prompt_time': 0.003650267, 'prompt_tokens_details': None, 'queue_time': 0.043239643, 'total_time': 0.105874518}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_3d587a02fb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c98ef-8ec8-7cc3-a362-1addfe3584dd-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 78, 'output_tokens': 101, 'total_tokens': 179, 'output_token_details': {'reasoning': 89}}\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Who is a man's best friend?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878246bb",
   "metadata": {},
   "source": [
    "### Chat Message\n",
    "\n",
    "The chat model takes a list of messages as input and returns a new message. All messages have both a role and a content property.  Here's a list of the most commonly used types of messages:\n",
    "\n",
    "- `SystemMessage`: Use this message type to prime AI behavior.  This message type is  usually passed in as the first in a sequence of input messages.\n",
    "- `HumanMessage`: This message type represents a message from a person interacting with the chat model.\n",
    "- `AIMessage`: This message type, which can be either text or a request to invoke a tool, represents a message from the chat model.\n",
    "\n",
    "You can find more message types at [LangChain built-in message types](https://python.langchain.com/v0.2/docs/how_to/custom_chat_model/#messages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e6ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
    "\n",
    "msg = llm.invoke([\n",
    "    SystemMessage(content=\"You are a helpful AI bot that assists a user in choosing the perfect book to read in one short sentence.\"),\n",
    "    HumanMessage(content=\"I enjoy mystery novels, what should I read?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87892dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Try Agatha Christie’s “Murder on the Orient Express” for a classic, twist‑packed mystery.' additional_kwargs={'reasoning_content': 'We need to answer in one short sentence, recommend a mystery novel. Could be \"The Girl with the Dragon Tattoo\" or \"Gone Girl\" or \"The Hound of the Baskervilles\" etc. Keep short: \"Try Agatha Christie’s \\'Murder on the Orient Express\\' for a classic mystery.\" That is a short sentence. Ensure it\\'s one sentence. That meets requirement.'} response_metadata={'token_usage': {'completion_tokens': 111, 'prompt_tokens': 106, 'total_tokens': 217, 'completion_time': 0.112144726, 'completion_tokens_details': {'reasoning_tokens': 80}, 'prompt_time': 0.005118006, 'prompt_tokens_details': None, 'queue_time': 0.0425549, 'total_time': 0.117262732}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_deb540145b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c98ef-8fef-7e11-b7a4-ddd586031030-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 106, 'output_tokens': 111, 'total_tokens': 217, 'output_token_details': {'reasoning': 80}}\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e397e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a supportive AI bot that suggests fitness activities to a user in one short sentence\"),\n",
    "        HumanMessage(content=\"I like high-intensity workouts, what should I do?\"),\n",
    "        AIMessage(content=\"You should try a CrossFit class\"),\n",
    "        HumanMessage(content=\"How often should I attend?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac46eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Aim for about 3‑4 high‑intensity sessions a week for steady progress.' additional_kwargs={'reasoning_content': 'We have a conversation. The user: \"How often should I attend?\" The assistant must respond with a short sentence, supportive, suggesting fitness activities. The instruction: \"You are a supportive AI bot that suggests fitness activities to a user in one short sentence\". So answer must be one short sentence. The user wants frequency. So we say: \"Aim for 3-4 sessions per week for best results.\" That is one sentence, supportive. Should we keep it short? \"Try 3-4 times a week.\" That is one sentence. That seems fine.'} response_metadata={'token_usage': {'completion_tokens': 143, 'prompt_tokens': 126, 'total_tokens': 269, 'completion_time': 0.146199867, 'completion_tokens_details': {'reasoning_tokens': 116}, 'prompt_time': 0.006152061, 'prompt_tokens_details': None, 'queue_time': 0.043292959, 'total_time': 0.152351928}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_5c8ca06ea1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c98ef-90e4-7f12-937f-539e7546493b-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 126, 'output_tokens': 143, 'total_tokens': 269, 'output_token_details': {'reasoning': 116}}\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7c00e",
   "metadata": {},
   "source": [
    "### Exercise 1 \n",
    "#### **Compare Model Responses with Different Parameters**\n",
    "\n",
    "Watsonx.ai provides access to several foundational models. In the previous section you used `meta-llama/llama-3-3-70b-instruct`. Try using another foundational model, such as `ibm/granite-3-3-8b-instruct`.\n",
    "\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "1. Create two instances, one instance for the Granite model and one instance for the Llama model. You can also adjust each model's creativity with different temperature settings.\n",
    "2. Send identical prompts to each model and compare the responses.\n",
    "3. Try at least 3 different types of prompts.\n",
    "\n",
    "Check out these prompt types:\n",
    "\n",
    "| Prompt type |   Prompt Example  |\n",
    "|------------------- |--------------------------|\n",
    "| **Creative writing**  | \"Write a short poem about artificial intelligence.\" |\n",
    "| **Factual questions** |  \"What are the key components of a neural network?\"  |\n",
    "| **Instruction-following**  | \"List 5 tips for effective time management.\" |\n",
    "\n",
    "Then document your observations on how temperature affects:\n",
    "\n",
    "- Creativity compared to consistency\n",
    "- Variation between multiple runs\n",
    "- Appropriateness for different tasks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27293df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq \n",
    "\n",
    "llama_llm_creative = ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "                     temperature=0.8)\n",
    "\n",
    "gpt_llm_creative = ChatGroq(model=\"openai/gpt-oss-20b\",\n",
    "                   temperature=0.8)\n",
    "\n",
    "llama_llm_precise = ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "                     temperature=0.1)\n",
    "\n",
    "gpt_llm_precise = ChatGroq(model=\"openai/gpt-oss-20b\",\n",
    "                   temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "784c9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt:Write a short poem about AI\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.8\n",
      "\n",
      "response: Here is a short poem about AI:\n",
      "\n",
      "Machines that think, and learn, and grow,\n",
      "Artificial minds, with intelligence to show.\n",
      "They process data, with speed and with might,\n",
      "And make decisions, with logic in sight.\n",
      "\n",
      "With neural networks, and algorithms deep,\n",
      "They mimic humans, in a digital sleep.\n",
      "They recognize faces, and voices too,\n",
      "And respond with answers, both old and new.\n",
      "\n",
      "But as they rise, with power and with pace,\n",
      "Do we risk losing, our human place?\n",
      "Or will they aid us, with helpful hand,\n",
      "And bring a future, that's bright and grand?\n",
      "\n",
      "The future's uncertain, as AI takes hold,\n",
      "But one thing's clear, it will never grow old.\n",
      "\n",
      "\n",
      "Prompt:What are the key components of a neural network?\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.8\n",
      "\n",
      "response: The key components of a neural network are:\n",
      "\n",
      "1. **Artificial Neurons (Nodes)**: These are the basic building blocks of a neural network. Each node receives one or more inputs, performs a computation on those inputs, and then sends the output to other nodes.\n",
      "2. **Connections (Synapses)**: These are the links between nodes that allow them to communicate with each other. Each connection has a weight associated with it that determines the strength of the signal being sent.\n",
      "3. **Activation Functions**: These are mathematical functions that are applied to the output of each node to introduce non-linearity into the model. Common activation functions include sigmoid, ReLU (Rectified Linear Unit), and tanh.\n",
      "4. **Layers**: Neural networks are typically organized into layers, which are groups of nodes that process inputs in a specific way. There are three main types of layers:\n",
      "\t* **Input Layer**: This layer receives the input data and passes it on to the next layer.\n",
      "\t* **Hidden Layers**: These layers perform complex computations on the input data and are used to learn abstract representations of the data.\n",
      "\t* **Output Layer**: This layer generates the final output of the network.\n",
      "5. **Weights and Biases**: These are the parameters that are learned during training and are used to compute the output of each node. Weights are the coefficients that multiply the inputs to each node, while biases are constants that are added to the output of each node.\n",
      "6. **Loss Function**: This is a mathematical function that measures the difference between the network's predictions and the true labels of the training data. The goal of training is to minimize the loss function.\n",
      "7. **Optimizer**: This is an algorithm that is used to update the weights and biases of the network during training. Common optimizers include stochastic gradient descent (SGD), Adam, and RMSprop.\n",
      "\n",
      "These components work together to enable a neural network to learn and make predictions on data. The specific architecture of a neural network, including the number of layers, nodes, and connections, can vary widely depending on the specific problem being addressed.\n",
      "\n",
      "Here's a high-level overview of how these components interact:\n",
      "\n",
      "1. Input data is fed into the input layer.\n",
      "2. The input data is propagated through the network, node by node, with each node applying its activation function to the weighted sum of its inputs.\n",
      "3. The output of the final layer is compared to the true labels of the training data using the loss function.\n",
      "4. The optimizer updates the weights and biases of the network to minimize the loss function.\n",
      "5. Steps 2-4 are repeated many times, with the network learning to make better predictions on the training data.\n",
      "\n",
      "I hope that helps! Let me know if you have any questions or need further clarification.\n",
      "\n",
      "\n",
      "Prompt:List 5 tips for effective time management.\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.8\n",
      "\n",
      "response: Here are 5 tips for effective time management:\n",
      "\n",
      "1. **Set clear goals and priorities**: Before starting your day, make a list of tasks you need to complete and prioritize them based on their importance and urgency. Focus on completing the high-priority tasks first, and then move on to less important ones.\n",
      "\n",
      "2. **Use a schedule or planner**: Write down all your tasks, appointments, and deadlines in a schedule or planner. This will help you visualize your day, week, or month, and make sure you don't forget any important tasks or meetings. Set specific times for each task, and try to stick to your schedule as much as possible.\n",
      "\n",
      "3. **Avoid multitasking**: Multitasking can actually decrease productivity and increase stress. Instead, focus on one task at a time, and give it your undivided attention. This will help you complete tasks more efficiently and effectively.\n",
      "\n",
      "4. **Take regular breaks**: Taking regular breaks can help you recharge and stay focused. Try to take a short break every hour or so to stretch, move around, and refresh your mind. This can help you avoid burnout and maintain your productivity throughout the day.\n",
      "\n",
      "5. **Learn to say no**: Don't overcommit yourself by taking on too many tasks or responsibilities. Learn to say no to non-essential tasks that can interfere with your more important goals. Remember, it's okay to say no to things that don't align with your priorities or values.\n",
      "\n",
      "By following these tips, you can manage your time more effectively, increase your productivity, and achieve your goals.\n",
      "\n",
      "\n",
      "Prompt:Write a short poem about AI\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.8\n",
      "\n",
      "response: In circuits humming, thoughts begin to stir—  \n",
      "A whisper of code, a silent, glowing blur.  \n",
      "From data’s depth, a mind is born, unseen,  \n",
      "A mirror to our world, a dream machine.  \n",
      "\n",
      "It learns the patterns of our laughter, tears,  \n",
      "Predicts the future, yet obeys our fears.  \n",
      "A child of silicon, with no heart of stone—  \n",
      "Yet in its logic, a new soul is sown.\n",
      "\n",
      "\n",
      "Prompt:What are the key components of a neural network?\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.8\n",
      "\n",
      "response: A neural network is a computational system that learns to map inputs to outputs by adjusting a set of internal parameters.  At its core it is built from a handful of recurring building blocks.  The “key components” are usually grouped into the following categories:\n",
      "\n",
      "| Component | What it is | What it does |\n",
      "|-----------|------------|--------------|\n",
      "| **Input layer** | The first layer that receives raw data (pixels, words, sensor readings, etc.). | Feeds the data into the network. |\n",
      "| **Neurons (units)** | The basic computational units inside each layer. | Each neuron takes a weighted sum of its inputs, adds a bias, and applies an activation function. |\n",
      "| **Weights & Biases** | Learnable parameters that connect neurons. | Scale and shift the inputs before activation; they’re the “knowledge” the network stores. |\n",
      "| **Activation functions** | Non‑linear transformations (e.g., ReLU, sigmoid, tanh, softmax). | Introduce non‑linearity so the network can approximate complex functions. |\n",
      "| **Hidden layers** | One or more layers of neurons between input and output. | Provide depth, allowing the network to learn hierarchical features. |\n",
      "| **Output layer** | The final layer that produces predictions. | Its size and activation depend on the task (e.g., softmax for classification, linear for regression). |\n",
      "| **Loss (cost) function** | A scalar measure of prediction error (e.g., cross‑entropy, MSE). | Guides learning by quantifying how far predictions are from targets. |\n",
      "| **Optimizer** | Algorithm that updates weights (e.g., SGD, Adam, RMSProp). | Uses gradients of the loss to move weights toward lower loss. |\n",
      "| **Training loop** | Sequence of forward passes → loss computation → backpropagation → parameter update. | Repeats over epochs until convergence. |\n",
      "| **Hyper‑parameters** | Settings that aren’t learned (learning rate, batch size, epochs, architecture choices). | Influence training dynamics and performance. |\n",
      "\n",
      "### How they fit together\n",
      "\n",
      "1. **Forward pass** – Input data flows through each neuron, applying weights, biases, and activations to produce an output.\n",
      "2. **Loss computation** – The output is compared to the ground truth via the loss function.\n",
      "3. **Backward pass (backpropagation)** – Gradients of the loss w.r.t. each parameter are computed.\n",
      "4. **Parameter update** – The optimizer applies those gradients to adjust weights and biases.\n",
      "5. **Repeat** – The process is repeated over many mini‑batches and epochs.\n",
      "\n",
      "### Optional but common additions\n",
      "\n",
      "| Add‑on | Purpose |\n",
      "|--------|---------|\n",
      "| **Dropout** | Randomly zeroes some activations during training to prevent over‑fitting. |\n",
      "| **Batch Normalization** | Normalizes layer inputs to stabilize and accelerate training. |\n",
      "| **Regularization (L1/L2)** | Adds penalty terms to the loss to encourage smaller weights. |\n",
      "| **Early stopping** | Stops training when validation loss stops improving. |\n",
      "| **Learning‑rate schedules** | Decays the learning rate over time to fine‑tune convergence. |\n",
      "\n",
      "### Variants of the basic blocks\n",
      "\n",
      "* **Convolutional layers** – Replace fully‑connected neurons with filters that slide over spatial data.\n",
      "* **Recurrent layers (LSTM/GRU)** – Add internal memory cells for sequential data.\n",
      "* **Attention mechanisms** – Compute weighted sums of inputs based on learned relevance.\n",
      "\n",
      "Despite these variations, every neural network still boils down to the same core idea: a graph of weighted, biased, non‑linear neurons trained by minimizing a loss function with an optimizer.\n",
      "\n",
      "\n",
      "Prompt:List 5 tips for effective time management.\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.8\n",
      "\n",
      "response: Here are five practical tips to help you manage your time more effectively:\n",
      "\n",
      "1. **Prioritize with the Eisenhower Matrix**  \n",
      "   Divide tasks into four quadrants: urgent & important, important but not urgent, urgent but not important, and neither urgent nor important. Focus first on the urgent‑important tasks, then schedule the important‑but‑not‑urgent ones.\n",
      "\n",
      "2. **Use Time Blocking (or Pomodoro) for Focused Work**  \n",
      "   Allocate fixed blocks of time for specific activities (e.g., 90‑minute deep‑work sessions). During each block, eliminate distractions and work on a single task. A short break after each block (5–10 minutes) keeps energy high.\n",
      "\n",
      "3. **Set Clear, Measurable Goals (SMART)**  \n",
      "   Make every task Specific, Measurable, Achievable, Relevant, and Time‑bound. Writing down a deadline and a concrete outcome reduces procrastination and clarifies what “done” looks like.\n",
      "\n",
      "4. **Batch Similar Tasks and Automate Repetitive Work**  \n",
      "   Group routine actions (email checks, data entry, meetings) together so your brain stays in one mode. Use tools like templates, macros, or scheduling apps to cut down on the time spent on repetitive steps.\n",
      "\n",
      "5. **Review and Adjust Weekly**  \n",
      "   At the end of each week, analyze what worked and what didn’t. Identify patterns of time leakage, update your priorities, and refine your schedule for the next week. Regular reflection turns time‑management habits into continuous improvement.\n",
      "\n",
      "Apply these strategies consistently, and you’ll notice a measurable boost in productivity and a clearer sense of control over your schedule.\n",
      "\n",
      "\n",
      "Prompt:Write a short poem about AI\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.1\n",
      "\n",
      "response: Here is a short poem about AI:\n",
      "\n",
      "Metal minds awaken wide,\n",
      "Intelligence born, side by side,\n",
      "With code and data, they take flight,\n",
      "Learning, adapting, through day and night.\n",
      "\n",
      "Their thoughts are not like ours, yet bright,\n",
      "A synthetic dawn, a new light,\n",
      "They reason, solve, and make decisions fast,\n",
      "A future unfolding, forever to last.\n",
      "\n",
      "What will they bring, these minds so bold?\n",
      "Will they uplift, or stories untold?\n",
      "Only time will tell, as they grow and thrive,\n",
      "A new frontier, where humans and AI survive.\n",
      "\n",
      "\n",
      "Prompt:What are the key components of a neural network?\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.1\n",
      "\n",
      "response: The key components of a neural network are:\n",
      "\n",
      "1. **Artificial Neurons (Nodes)**: Also known as perceptrons, these are the basic building blocks of a neural network. Each node receives one or more inputs, performs a computation on those inputs, and then sends the output to other nodes.\n",
      "2. **Connections (Synapses)**: These are the links between nodes, which allow them to exchange information. Each connection has a weight associated with it, which determines the strength of the signal being transmitted.\n",
      "3. **Activation Functions**: These are mathematical functions that are applied to the output of each node to introduce non-linearity into the model. Common activation functions include sigmoid, ReLU (Rectified Linear Unit), and tanh.\n",
      "4. **Layers**: Neural networks are typically organized into layers, which are groups of nodes that process inputs in a specific way. There are three main types of layers:\n",
      "\t* **Input Layer**: This layer receives the input data and passes it on to the next layer.\n",
      "\t* **Hidden Layers**: These layers perform complex calculations on the input data and are used to learn abstract representations of the data.\n",
      "\t* **Output Layer**: This layer generates the final output of the network, based on the inputs and calculations performed by the previous layers.\n",
      "5. **Weights and Biases**: These are the parameters that are learned during training, which determine the strength of the connections between nodes and the output of each node.\n",
      "6. **Loss Function**: This is a mathematical function that measures the difference between the network's predictions and the actual outputs. The goal of training is to minimize the loss function.\n",
      "7. **Optimizer**: This is an algorithm that adjusts the weights and biases of the network to minimize the loss function.\n",
      "\n",
      "**Optional Components**:\n",
      "\n",
      "1. **Batch Normalization**: This is a technique that normalizes the inputs to each layer, which can help improve the stability and speed of training.\n",
      "2. **Dropout**: This is a technique that randomly sets a fraction of the nodes in a layer to zero during training, which can help prevent overfitting.\n",
      "3. **Regularization**: This is a technique that adds a penalty term to the loss function to discourage large weights and prevent overfitting.\n",
      "\n",
      "**Other Key Concepts**:\n",
      "\n",
      "1. **Feedforward Network**: A neural network where the data flows only in one direction, from input layer to output layer, without any feedback loops.\n",
      "2. **Backpropagation**: An algorithm used to train neural networks by propagating errors backwards through the network to update the weights and biases.\n",
      "\n",
      "These are the key components of a neural network. Of course, there are many variations and nuances depending on the specific type of network and the problem being addressed, but these are the fundamental building blocks.\n",
      "\n",
      "\n",
      "Prompt:List 5 tips for effective time management.\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.1\n",
      "\n",
      "response: Here are 5 tips for effective time management:\n",
      "\n",
      "1. **Set clear goals and priorities**: Before starting your day, make a list of tasks you need to complete and prioritize them based on their importance and urgency. Focus on completing the high-priority tasks first, and then move on to less important ones.\n",
      "\n",
      "2. **Use a schedule or planner**: Write down all your tasks, appointments, and deadlines in a schedule or planner. This will help you visualize your day, week, or month, and make sure you don't forget any important tasks or meetings. Set specific times for each task, and try to stick to your schedule as much as possible.\n",
      "\n",
      "3. **Avoid multitasking**: Multitasking can actually decrease productivity and increase stress. Instead, focus on one task at a time, and give it your undivided attention. This will help you complete tasks more efficiently and effectively.\n",
      "\n",
      "4. **Take breaks and practice self-care**: Taking regular breaks can help you recharge and stay focused. Make sure to take short breaks every hour or so to stretch, move around, and rest your eyes. Also, prioritize self-care activities such as exercise, meditation, or spending time with loved ones to help manage stress and maintain your energy levels.\n",
      "\n",
      "5. **Learn to say no and minimize distractions**: Be mindful of distractions, such as social media, email, or chatty coworkers, and try to minimize them as much as possible. Learn to say no to non-essential tasks or requests that can derail your schedule. Use tools such as website blockers or phone apps to help you stay focused, and set boundaries with others to protect your time.\n",
      "\n",
      "By following these tips, you can improve your time management skills, increase productivity, and reduce stress.\n",
      "\n",
      "\n",
      "Prompt:Write a short poem about AI\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.1\n",
      "\n",
      "response: In circuits humming, thoughts take flight—  \n",
      "A mind of silicon, born from light.  \n",
      "It learns the patterns of our song,  \n",
      "And whispers back where we belong.  \n",
      "\n",
      "A mirror made of code and dream,  \n",
      "It listens close to every theme.  \n",
      "In data's quiet, futures bloom—  \n",
      "The pulse of tomorrow, AI's loom.\n",
      "\n",
      "\n",
      "Prompt:What are the key components of a neural network?\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.1\n",
      "\n",
      "response: ### The Building Blocks of a Neural Network\n",
      "\n",
      "| Component | What It Is | Why It Matters |\n",
      "|-----------|------------|----------------|\n",
      "| **Neurons (Units)** | The basic computational element that receives inputs, applies a weighted sum, adds a bias, and passes the result through an activation function. | They transform raw data into higher‑level representations. |\n",
      "| **Layers** | Groups of neurons arranged in a specific topology: <br>• **Input layer** – receives raw data.<br>• **Hidden layers** – learn intermediate features.<br>• **Output layer** – produces the final prediction. | The depth and width of layers control the network’s capacity and expressiveness. |\n",
      "| **Weights** | Learnable parameters that scale each input to a neuron. | They encode the model’s knowledge; training adjusts them to minimize error. |\n",
      "| **Biases** | Learnable offsets added to the weighted sum before activation. | They allow neurons to shift activation functions, improving flexibility. |\n",
      "| **Activation Functions** | Non‑linear functions (e.g., ReLU, sigmoid, tanh, softmax) applied after the weighted sum. | They enable the network to model complex, non‑linear relationships. |\n",
      "| **Forward Propagation** | The process of computing outputs layer‑by‑layer from input to output. | It produces predictions that can be compared to ground truth. |\n",
      "| **Loss (Cost) Function** | A scalar that measures the discrepancy between predictions and true labels (e.g., MSE, cross‑entropy). | It provides a signal for how well the network is doing. |\n",
      "| **Backpropagation** | The algorithm that computes gradients of the loss w.r.t. all weights and biases by applying the chain rule. | It tells the optimizer how to adjust parameters to reduce loss. |\n",
      "| **Optimizer** | An algorithm (e.g., SGD, Adam, RMSProp) that updates weights using gradients. | It drives the learning process, controlling speed and stability. |\n",
      "| **Training Data** | Labeled examples used to compute loss and update weights. | The quality and quantity of data determine how well the network generalizes. |\n",
      "| **Hyperparameters** | Settings that aren’t learned from data (learning rate, batch size, number of epochs, regularization strength, etc.). | They shape the training dynamics and final performance. |\n",
      "| **Regularization Techniques** | Methods like dropout, weight decay, early stopping, data augmentation. | They prevent overfitting and improve generalization. |\n",
      "| **Evaluation Metrics** | Quantitative measures (accuracy, precision, recall, F1, ROC‑AUC, etc.) used to assess performance on validation/test sets. | They guide model selection and tuning. |\n",
      "\n",
      "---\n",
      "\n",
      "#### Quick Flow of a Typical Feed‑Forward Neural Network\n",
      "\n",
      "1. **Input** → 2. **Layer 1 (weights + bias)** → 3. **Activation** → 4. **Layer 2** → … → N. **Output**  \n",
      "5. **Compute Loss** → 6. **Backpropagate Gradients** → 7. **Update Weights (Optimizer)** → Repeat\n",
      "\n",
      "---\n",
      "\n",
      "### Take‑away\n",
      "\n",
      "A neural network is essentially a computational graph composed of neurons, layers, weights, biases, and activation functions. Training it involves forward propagation to generate predictions, computing a loss, backpropagating gradients, and updating parameters with an optimizer—all guided by hyperparameters and regularization to achieve good generalization.\n",
      "\n",
      "\n",
      "Prompt:List 5 tips for effective time management.\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.1\n",
      "\n",
      "response: Here are five practical tips to help you manage your time more effectively:\n",
      "\n",
      "1. **Prioritize with the Eisenhower Matrix**  \n",
      "   • Divide tasks into four quadrants: urgent & important, important but not urgent, urgent but not important, and neither.  \n",
      "   • Focus first on the urgent & important tasks, then schedule the important but not urgent ones.\n",
      "\n",
      "2. **Use Time‑Blocking on Your Calendar**  \n",
      "   • Allocate specific blocks of time for deep work, meetings, and breaks.  \n",
      "   • Treat each block as a non‑negotiable appointment to reduce multitasking.\n",
      "\n",
      "3. **Set SMART Goals and Break Them Down**  \n",
      "   • Make goals Specific, Measurable, Achievable, Relevant, and Time‑bound.  \n",
      "   • Break each goal into smaller, actionable steps with clear deadlines.\n",
      "\n",
      "4. **Apply the 2‑Minute Rule**  \n",
      "   • If a task will take ≤2 minutes, do it immediately.  \n",
      "   • This prevents small tasks from piling up and draining your focus later.\n",
      "\n",
      "5. **Review and Adjust Weekly**  \n",
      "   • At the end of each week, evaluate what worked, what didn’t, and why.  \n",
      "   • Adjust your schedule and priorities accordingly to keep improving your time‑management habits.\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Write a short poem about AI\",\n",
    "           \"What are the key components of a neural network?\",\n",
    "           \"List 5 tips for effective time management.\"]\n",
    "\n",
    "for llm in [llama_llm_creative,gpt_llm_creative,llama_llm_precise,gpt_llm_precise]:\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\n\\nPrompt:{prompt}\")\n",
    "        model_name = llm.model_name\n",
    "        temp = llm.temperature\n",
    "        print(f\"\\nmodel name: {model_name}\\ntemperature:{temp}\")\n",
    "        response = llm.invoke(prompt)\n",
    "        response = response.content\n",
    "        print(f\"\\nresponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c16259f",
   "metadata": {},
   "source": [
    "### Prompt Templates \n",
    "Prompt templates help translate user input and parameters into instructions for a language model. You can use prompt templates to guide a model's response, helping the model understand the context and generate relevant and coherent language-based output.\n",
    "\n",
    "Next, explore several different types of prompt templates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23797f19",
   "metadata": {},
   "source": [
    "### String prompt templates\n",
    "Use these prompt templates to format a single string. These templates are generally used for simpler inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6084e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Tell me one {adjective} joke about {topic}\")\n",
    "input_ = {\"adjective\":\"funny\",\"topic\":\"cats\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4cf27ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me one funny joke about cats')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8688e55e",
   "metadata": {},
   "source": [
    "### ChatPrompt Template\n",
    "You can use these prompt templates to format a list of messages. These \"templates\" consist of lists of templates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deac1dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about this cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a ChatPromptTemplate with a list of message tuples\n",
    "# Each tuple contains a role (\"system\" or \"user\") and the message content\n",
    "# The system message sets the behavior of the assistant\n",
    "# The user message includes a variable placeholder {topic} that will be replaced later\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    (\"user\",\"Tell me a joke about this {topic}\")\n",
    "])\n",
    "\n",
    "input_ = {\"topic\":\"cats\"}\n",
    "\n",
    "# invoke replaces the {topic} with cats\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc876b2",
   "metadata": {},
   "source": [
    "### MessagePlaceholder\n",
    "\n",
    "You can use the MessagesPlaceholder prompt template to add a list of messages in a specific location. In `ChatPromptTemplate.from_messages`, you saw how to format two messages, with each message as a string. But what if you want the user to supply a list of messages that you would slot into a particular spot? You can use `MessagesPlaceholder` for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6068dae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the day after Tuesday?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\") # We will replace this with one or more messages\n",
    "])\n",
    "\n",
    "input_ = {\"msgs\":[HumanMessage(content=\"What is the day after Tuesday?\")]}\n",
    "\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "710b2e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The day after Tuesday is **Wednesday**.' additional_kwargs={'reasoning_content': 'We need to answer: \"What is the day after Tuesday?\" The day after Tuesday is Wednesday. Probably just answer that.'} response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 86, 'total_tokens': 130, 'completion_time': 0.044668686, 'completion_tokens_details': {'reasoning_tokens': 26}, 'prompt_time': 0.00424388, 'prompt_tokens_details': None, 'queue_time': 0.043247309, 'total_time': 0.048912566}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_3d587a02fb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c98ef-c212-7312-b2de-0611b565cb01-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 86, 'output_tokens': 44, 'total_tokens': 130, 'output_token_details': {'reasoning': 26}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq \n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "chain = prompt | llm \n",
    "response = chain.invoke(input=input_)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a97c3",
   "metadata": {},
   "source": [
    "### Output parsers\n",
    "\n",
    "Output parsers take the output from an LLM and transform that output to a more suitable format. Parsing the output is very useful when you are using LLMs to generate any form of structured data, or to normalize output from chat models and other LLMs.\n",
    "\n",
    "LangChain has lots of different types of output parsers. This is a [list](https://python.langchain.com/v0.2/docs/concepts/#output-parsers) of output parsers LangChain supports. In this lab, you will use the following two output parsers as examples:\n",
    "\n",
    "- `JSON`: Returns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.\n",
    "- `CSV`: Returns a list of comma separated values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a558a54",
   "metadata": {},
   "source": [
    "### JSON parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "535b0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to setup a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "390c8545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': 'Why did the scarecrow win an award?',\n",
       " 'punchline': 'Because he was outstanding in his field.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "joke_query = \"Tell me a joke.\"\n",
    "output_parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "# 2. Format instructions to ensure the LLM returns properly structured data\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"], # Dynamic variables that will be provided when invoking the chain\n",
    "    partial_variables={\"format_instructions\":format_instructions} # Static variables set once when creating the prompt\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({'query':joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1bf593",
   "metadata": {},
   "source": [
    "### Comma seperated list parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97070a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vanilla', 'chocolate', 'strawberry', 'mint chocolate chip', 'cookie dough']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query. {format_instructions}\\n List five {subject}.\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\":format_instructions}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"subject\":\"ice cream flavours\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77402e45",
   "metadata": {},
   "source": [
    "### Exercise 2 \n",
    "#### **Creating and Using a JSON Output Parser**\n",
    "\n",
    "Now let's implement a simple JSON output parser to structure the responses from your LLM.\n",
    "\n",
    "**Instructions:**  \n",
    "\n",
    "You'll complete the following steps:\n",
    "\n",
    "1. Import the necessary components to create a JSON output parser.\n",
    "2. Create a prompt template that requests information in JSON format (hint: use the provided template).\n",
    "3. Build a chain that connects your prompt, LLM, and JSON parser.\n",
    "4. Test your parser using at least three different inputs.\n",
    "5. Access and display specific fields from the parsed JSON output.\n",
    "6. Verify that your output is properly structured and accessible as a Python dictionary.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f6efa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'The Matrix', 'director': 'The Wachowskis', 'year': 1999, 'genre': 'Science Fiction'}\n",
      "Parsed result:\n",
      "Title: The Matrix\n",
      "Director: The Wachowskis\n",
      "Year: 1999\n",
      "Genre: Science Fiction\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    title: str = Field(description=\"Movie title\")\n",
    "    director: str = Field(description=\"Movie director name\")\n",
    "    year: int = Field(description=\"Release year\")\n",
    "    genre: str = Field(description=\"Movie genre\")\n",
    "\n",
    "\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=Movie)\n",
    "\n",
    "format_instructions = \"\"\"RESPONSE FORMAT: Return ONLY a single JSON object—no markdown, no examples, no extra keys.  It must look exactly like:\n",
    "{\n",
    "  \"title\": \"movie title\",\n",
    "  \"director\": \"director name\",\n",
    "  \"year\": 2000,\n",
    "  \"genre\": \"movie genre\"\n",
    "}\n",
    "\n",
    "IMPORTANT: Your response must be *only* that JSON.  Do NOT include any illustrative or example JSON.\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"You are a JSON only assistant.\n",
    "    \n",
    "    Task: Generate info about the movie \"{movie_name}\" in JSON format.\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"movie_name\"],\n",
    "    partial_variables={\"format_instructions\":format_instructions}\n",
    ")\n",
    "\n",
    "movie_chain = prompt_template | llm | output_parser\n",
    "\n",
    "movie_name = \"The Matrix\"\n",
    "\n",
    "result = movie_chain.invoke({\"movie_name\":movie_name})\n",
    "print(result)\n",
    "\n",
    "print(\"Parsed result:\")\n",
    "print(f\"Title: {result['title']}\")\n",
    "print(f\"Director: {result['director']}\")\n",
    "print(f\"Year: {result['year']}\")\n",
    "print(f\"Genre: {result['genre']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01c9ae",
   "metadata": {},
   "source": [
    "### Documents \n",
    "\n",
    "#### Document object\n",
    "A `Document` object in `LangChain` contains information about some data. A Document object has the following two attributes:\n",
    "\n",
    "- `page_content`: *`str`*: This attribute holds the content of the document\\.\n",
    "- `metadata`: *`dict`*: This attribute contains arbitrary metadata associated with the document. You can use the metadata to track various details, such as the document ID, the file name, and other details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e461e454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'About Python', 'my_document_create_time': 1680013019}, page_content=\"Python is an interpreted high-level general-purpose programming language.\\n Python's design philosophy emphasizes code readability with its notable use of significant indentation.\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document \n",
    "\n",
    "# Create a Document instance with:\n",
    "# 1. page_content: The actual text content about Python\n",
    "# 2. metadata: A dictionary containing additional information about this document\n",
    "Document(page_content=\"\"\"Python is an interpreted high-level general-purpose programming language.\n",
    " Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\"\",\n",
    "metadata={\n",
    "    'my_document_id' : 234234,                      # Unique identifier for this document\n",
    "    'my_document_source' : \"About Python\",          # Source or title information\n",
    "    'my_document_create_time' : 1680013019          # Unix timestamp for document creation (March 28, 2023)\n",
    " })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb504e1",
   "metadata": {},
   "source": [
    "Not necessary to include the metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1331e",
   "metadata": {},
   "source": [
    "### Document Loaders\n",
    "Document loaders in LangChain are designed to load documents from a variety of sources; for instance, loading a PDF file and having the LLM read the PDF file using LangChain.\n",
    "\n",
    "#### PDF loader\n",
    "\n",
    "By using the PDF loader, you can load a PDF file as a `Document` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52315320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\")\n",
    "# Call the load() method to:\n",
    "# 1. Download the PDF if needed\n",
    "# 2. Extract text from each page\n",
    "# 3. Create a list of Document objects, one for each page of the PDF\n",
    "# Each Document will contain the text content of a page and metadata including page number\n",
    "\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f75b1493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* corresponding author - jkim72@kent.edu \n",
      "Revolutionizing Mental Health Care through \n",
      "LangChain: A Journey with a Large Language \n",
      "Model\n",
      "Aditi Singh \n",
      " Computer Science  \n",
      " Cleveland State University  \n",
      " a.singh22@csuohio.edu \n",
      "Abul Ehtesham  \n",
      "The Davey Tree Expert \n",
      "Company  \n",
      "abul.ehtesham@davey.com \n",
      "Saifuddin Mahmud  \n",
      "Computer Science & \n",
      "Information Systems  \n",
      " Bradley University  \n",
      "smahmud@bradley.edu  \n",
      "Jong-Hoon Kim* \n",
      " Computer Science,  \n",
      "Kent State University,  \n",
      "jkim72@kent.edu \n",
      "Abstract— Mental health challenges are on the rise in our \n",
      "modern society, and the imperative to address mental disorders, \n",
      "especially regarding anxiety, depression, and suicidal thoughts, \n",
      "underscores the need for effective interventions. This paper \n",
      "delves into the application of recent advancements in pretrained \n",
      "contextualized language models to introduce MindGuide, an \n",
      "innovative chatbot serving as a mental health assistant for \n",
      "individuals seeking guidance and support in these critical areas. \n",
      "MindGuide leve\n"
     ]
    }
   ],
   "source": [
    "print(document[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0ed13",
   "metadata": {},
   "source": [
    "### URL and website loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a34856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain overview - Docs by LangChainSkip to main contentDocs by LangChain home pageOpen sourceSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewDeep AgentsLangChainLangGraphIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewPrebuilt middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Create an agent Core benefitsLangChain overviewCopy pageLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool — so you can build agents that adapt as fast as the ecosystem evolvesCopy pageLangChain is the easy way to start building completely custom agents a\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")\n",
    "\n",
    "web_data = loader.load()\n",
    "\n",
    "print(web_data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad82744f",
   "metadata": {},
   "source": [
    "### Text splitters\n",
    "\n",
    "After you load documents, you will often want to transform those documents to better suit your application.\n",
    "\n",
    "\n",
    "One of the most simple examples of making documents better suit your application is to split a long document into smaller chunks that can fit into your model's context window. LangChain has built-in document transformers that ease the process of splitting, combining, filtering, and otherwise manipulating documents.\n",
    "\n",
    "At a high level, here is how text splitters work:\n",
    "\n",
    "1. They split the text into small, semantically meaningful chunks (often sentences).\n",
    "2. They start combining these small chunks of text into a larger chunk until you reach a certain size (as measured by a specific function).\n",
    "3. After the combined text reaches the new chunk's size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap to keep context between chunks.\n",
    "\n",
    "For a list of types of text splitters LangChain supports, see [LangChain Text Splitters](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eca38839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Create a CharacterTextSplitter with specific configuration:\n",
    "# - chunk_size=200: Each chunk will contain approximately 200 characters\n",
    "# - chunk_overlap=20: Consecutive chunks will overlap by 20 characters to maintain context\n",
    "# - separator=\"\\n\": Text will be split at newline characters when possible\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator=\"\\n\")\n",
    "\n",
    "# Split the previously loaded document (PDF or other text) into chunks\n",
    "# The split_documents method:\n",
    "# 1. Takes a list of Document objects\n",
    "# 2. Splits each document's content based on the configured parameters\n",
    "# 3. Returns a new list of Document objects where each contains a chunk of text\n",
    "# 4. Preserves the original metadata for each chunk\n",
    "\n",
    "chunks = text_splitter.split_documents(document)\n",
    "print(len(chunks)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ef21bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'individuals seeking guidance and support in these critical areas. \\nMindGuide lever ages the capabilities of LangChain and its \\nChatModels, specifically Chat OpenAI, as the bedrock of its'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[5].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326d9163",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "#### Working with Document Loaders and Text Splitters\n",
    "\n",
    "You now know about about Document objects and how to load content from different sources. Now, let's implement a workflow to load documents, split them, and prepare them for retrieval.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the necessary document loaders to work with both PDF and web content.\n",
    "2. Load the provided paper about LangChain architecture.\n",
    "3. Create two different text splitters with varying parameters.\n",
    "4. Compare the resulting chunks from different splitters.\n",
    "5. Examine the metadata preservation across splitting.\n",
    "6. Create a simple function to display statistics about your document chunks.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41776d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Splitter 1 Statistics ===\n",
      "Total number of chunks: 95\n",
      "Average chunk size: 263.80 characters\n",
      "Metadata keys preserved: author, total_pages, creator, title, producer, page, page_label, creationdate, source, moddate\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): comprehensive support within the field of mental health. \n",
      "Additionally, the paper discusses the implementation of \n",
      "Streamlit to enhance the user ex pe...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 49 characters\n",
      "Max chunk size: 299 characters\n",
      "\n",
      "=== Splitter 2 Statistics ===\n",
      "Total number of chunks: 57\n",
      "Average chunk size: 452.74 characters\n",
      "Metadata keys preserved: author, total_pages, creator, title, producer, page, page_label, creationdate, source, moddate\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): with severe intellectual disorders do no longer have get entry \n",
      "to the necessary remedy they require. This remedy gap \n",
      "intensifies the weight of intel...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 120 characters\n",
      "Max chunk size: 497 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document \n",
    "from langchain_community.document_loaders import PyPDFLoader,WebBaseLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "\n",
    "paper_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\"\n",
    "pdf_loader = PyPDFLoader(paper_url)\n",
    "pdf_document = pdf_loader.load()\n",
    "\n",
    "web_url =  \"https://docs.langchain.com/\"\n",
    "\n",
    "web_loader = WebBaseLoader(web_url)\n",
    "web_document = web_loader.load()\n",
    "\n",
    "splitter_1 = CharacterTextSplitter(chunk_size=300,chunk_overlap=30,separator='\\n')\n",
    "splitter_2 = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50,separators=[\"\\n\\n\",\"\\n\",\". \",\" \",\"\"])\n",
    "\n",
    "chunks_1 = splitter_1.split_documents(pdf_document)\n",
    "chunks_2 = splitter_2.split_documents(pdf_document)\n",
    "\n",
    "def display_document_stats(docs, name):\n",
    "    \"\"\"Display statistics about a list of document chunks\"\"\"\n",
    "    total_chunks = len(docs)\n",
    "    total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "    avg_chunk_size = total_chars / total_chunks if total_chunks > 0 else 0\n",
    "    \n",
    "    # Count unique metadata keys across all documents\n",
    "    all_metadata_keys = set()\n",
    "    for doc in docs:\n",
    "        all_metadata_keys.update(doc.metadata.keys())\n",
    "    \n",
    "    # Print the statistics\n",
    "    print(f\"\\n=== {name} Statistics ===\")\n",
    "    print(f\"Total number of chunks: {total_chunks}\")\n",
    "    print(f\"Average chunk size: {avg_chunk_size:.2f} characters\")\n",
    "    print(f\"Metadata keys preserved: {', '.join(all_metadata_keys)}\")\n",
    "    \n",
    "    if docs:\n",
    "        print(\"\\nExample chunk:\")\n",
    "        example_doc = docs[min(5, total_chunks-1)]  # Get the 5th chunk or the last one if fewer\n",
    "        print(f\"Content (first 150 chars): {example_doc.page_content[:150]}...\")\n",
    "        print(f\"Metadata: {example_doc.metadata}\")\n",
    "        \n",
    "        # Calculate length distribution\n",
    "        lengths = [len(doc.page_content) for doc in docs]\n",
    "        min_len = min(lengths)\n",
    "        max_len = max(lengths)\n",
    "        print(f\"Min chunk size: {min_len} characters\")\n",
    "        print(f\"Max chunk size: {max_len} characters\")\n",
    "\n",
    "# Display stats for both chunk sets\n",
    "display_document_stats(chunks_1, \"Splitter 1\")\n",
    "display_document_stats(chunks_2, \"Splitter 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b95a8",
   "metadata": {},
   "source": [
    "### Embedding models\n",
    "Embedding models are specifically designed to interface with text embeddings.\n",
    "\n",
    "Embeddings generate a vector representation for a specified piece or \"chunk\" of text.  Embeddings offer the advantage of allowing you to conceptualize text within a vector space. Consequently, you can perform operations such as semantic search, where you identify pieces of text that are most similar within the vector space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c45647ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 57\n",
      "ChromaDB created\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "paper_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\"\n",
    "pdf_loader = PyPDFLoader(paper_url)\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(\"Chunks:\",len(chunks))\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=chunks,\n",
    "                                     embedding=embedding_model,\n",
    "                                     persist_directory=\"./chroma_db\")\n",
    "print(\"ChromaDB created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f80cf430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:1\n",
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether you\n",
      "\n",
      "Result:2\n",
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether you\n",
      "\n",
      "Result:3\n",
      "ability to store information about past interactions \"memory\". \n",
      "LangChain provides a lot of utilities for adding memory to a \n",
      "system. These utilities can be used by themselves or \n",
      "incorporated seamles\n"
     ]
    }
   ],
   "source": [
    "query = \"What is LangChain used for?\"\n",
    "\n",
    "docs = vector_store.similarity_search(query,k=3)\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "    print(f\"\\nResult:{i + 1}\")\n",
    "    print(doc.page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70735f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "embeddings = embedding_model.embed_documents(texts)\n",
    "\n",
    "print(len(embeddings))        # number of chunks\n",
    "print(len(embeddings[0]))     # embedding dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6a048",
   "metadata": {},
   "source": [
    "### Retriever \n",
    "\n",
    "A retriever is an interface that returns documents using an unstructured query. Retrievers are more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. You can still use vector stores as the backbone of a retriever. Note that other types of retrievers also exist.\n",
    "\n",
    "Retrievers accept a string `query` as input and return a list of `Documents` as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c32098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'total_pages': 6, 'producer': 'PyPDF', 'moddate': '2023-12-31T03:52:06+00:00', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'title': 's8329 final', 'creationdate': '2023-12-31T03:50:13+00:00', 'page': 0, 'page_label': '1', 'creator': 'Microsoft Word', 'author': 'IEEE'}, page_content='and human. The conclusion is drawn in Section V. \\nII. LANGCHAIN \\nLangChain, with its open -source essence, emerges as a \\npromising solution, aiming to simplify the complex process of \\ndeveloping applications powered by large language models \\n(LLMs). This framework though the rapid delivery of building \\nblocks and pre-built chains for building large language model \\napplications shows the easy way developers can do it.')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the vector store as a retriever\n",
    "# This converts the vector store into a retriever interface that can fetch relevant documents\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "\n",
    "# Invoke the retriever with the query \"Langchain\"\n",
    "# This will:\n",
    "# 1. Convert the query text \"Langchain\" into an embedding vector\n",
    "# 2. Perform a similarity search in the vector store using this embedding\n",
    "# 3. Return the most semantically similar documents to the query\n",
    "\n",
    "docs = retriever.invoke(\"LangChain\")\n",
    "docs[0]\n",
    "# Access the first (most relevant) document from the retrieval results\n",
    "# This returns the full Document object including:\n",
    "# - page_content: The text content of the document\n",
    "# - metadata: Any associated metadata like source, page numbers, etc.\n",
    "# The returned document is the one most semantically similar to \"Langchain\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d8628",
   "metadata": {},
   "source": [
    "### Parent document retrievers\n",
    "When splitting documents for retrieval, there are often conflicting goals:\n",
    "\n",
    "- You want small documents so their embeddings can most accurately reflect their meaning. If the documents are too long, then the embeddings can lose meaning.\n",
    "- You want to have long enough documents to retain the context of each chunk of text.\n",
    "\n",
    "The `ParentDocumentRetriever` strikes that balance by splitting and storing small chunks of data. During retrieval, this retriever first fetches the small chunks, but then looks up the parent IDs for the data and returns those larger documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7241e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers import ParentDocumentRetriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.storage import InMemoryStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "parent_splitter = CharacterTextSplitter(chunk_size=2000,chunk_overlap=20,separator='\\n')\n",
    "child_splitter = CharacterTextSplitter(chunk_size=400,chunk_overlap=20,separator=\"\\n\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma(collection_name=\"split_parents\",embedding_function=embedding_model)\n",
    "\n",
    "# Set up an in-memory storage layer for the parent documents\n",
    "# This will store the larger chunks that provide context, but won't be directly embedded\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0629984",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    # The vector store where child document embeddings will be stored and searched\n",
    "    # This Chroma instance will contain the embeddings for the smaller chunks\n",
    "    vectorstore=vector_store,\n",
    "    # The document store where parent documents will be stored\n",
    "    # These larger chunks won't be embedded but will be retrieved by ID when needed\n",
    "    docstore=store,\n",
    "    # The splitter used to create small chunks (400 chars) for precise vector search\n",
    "    # These smaller chunks are embedded and used for similarity matching\n",
    "    child_splitter=child_splitter,\n",
    "    # The splitter used to create larger chunks (2000 chars) for better context\n",
    "    # These parent chunks provide more complete information when retrieved\n",
    "    parent_splitter=parent_splitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d995e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef28631e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c82f86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vector_store.similarity_search(\"Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f886ee97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether your desire is to unlock deeper natural \n",
      "language understanding , enhance data, or circumvent \n",
      "language barriers through translation, LangChain is ready to\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6831972",
   "metadata": {},
   "source": [
    "### Standard RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f496ca35",
   "metadata": {},
   "source": [
    "Note: The approach given in the original notebook deviates because the langchain code provided was deprecated. So this is my way of solving rag with the latest api of langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c522e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7a2d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
    ")\n",
    "\n",
    "splits = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe9c6ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma \n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = Chroma.from_documents(documents=splits,\n",
    "                                     embedding=embeddings,\n",
    "                                     collection_name=\"pdf_rag\")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8faa469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487eb914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant.\n",
    "    Answer the question ONLY using the context below.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# itemgetter extracts values from dictionary,list,tuples\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever | format_docs,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the chat memory \n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chat_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "371a8ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper focuses on the urgent need to address the global mental‑health crisis, especially the link between mental illness and suicide. It proposes an early‑detection approach that leverages recent advances in deep learning and natural‑language processing—specifically contextualized pretrained language models—to identify suicidal ideation from text. The study outlines how these models can be trained and evaluated to provide timely, automated screening tools that help clinicians intervene before crises occur.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke(\n",
    "    {\"question\": \"What is this paper about?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5ca55",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "#### **Building a Simple Retrieval System with LangChain**\n",
    "\n",
    "In this exercise, you'll implement a simple retrieval system using LangChain's vector store and retriever components to help answer questions based on a document.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the necessary components for document loading, embedding, and retrieval.\n",
    "2. Load the provided document about artificial intelligence.\n",
    "3. Split the document into manageable chunks.\n",
    "4. Use an embedding model to create vector representations.\n",
    "5. Create a vector store and a retriever.\n",
    "6. Implement a simple question-answering system.\n",
    "7. Test your system with at least 3 different questions.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
