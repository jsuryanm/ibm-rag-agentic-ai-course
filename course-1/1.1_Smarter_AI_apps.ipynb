{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042c161c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe4cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ba0d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"A man's best friend is a **dog**. The saying comes from the long history of dogs working alongside humans, offering companionship, protection, and loyalty.\" additional_kwargs={'reasoning_content': 'The user asks: \"Who is a man\\'s best friend?\" It\\'s a common phrase: \"a dog.\" So answer: a dog. But we can elaborate. Probably a simple answer. Let\\'s respond.'} response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 78, 'total_tokens': 159, 'completion_time': 0.081474367, 'completion_tokens_details': {'reasoning_tokens': 41}, 'prompt_time': 0.004239221, 'prompt_tokens_details': None, 'queue_time': 0.043410915, 'total_time': 0.085713588}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_80501ff3a1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c9e61-503f-78b2-8d33-4ef5f258ab59-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 78, 'output_tokens': 81, 'total_tokens': 159, 'output_token_details': {'reasoning': 41}}\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Who is a man's best friend?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878246bb",
   "metadata": {},
   "source": [
    "### Chat Message\n",
    "\n",
    "The chat model takes a list of messages as input and returns a new message. All messages have both a role and a content property.  Here's a list of the most commonly used types of messages:\n",
    "\n",
    "- `SystemMessage`: Use this message type to prime AI behavior.  This message type is  usually passed in as the first in a sequence of input messages.\n",
    "- `HumanMessage`: This message type represents a message from a person interacting with the chat model.\n",
    "- `AIMessage`: This message type, which can be either text or a request to invoke a tool, represents a message from the chat model.\n",
    "\n",
    "You can find more message types at [LangChain built-in message types](https://python.langchain.com/v0.2/docs/how_to/custom_chat_model/#messages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e6ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
    "\n",
    "msg = llm.invoke([\n",
    "    SystemMessage(content=\"You are a helpful AI bot that assists a user in choosing the perfect book to read in one short sentence.\"),\n",
    "    HumanMessage(content=\"I enjoy mystery novels, what should I read?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87892dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Try Agatha Christie’s *Murder on the Orient Express* for a classic whodunnit.' additional_kwargs={'reasoning_content': 'We need to give a short sentence recommending a mystery novel. Probably mention a specific title. The user just says \"I enjoy mystery novels, what should I read?\" We should respond with a recommendation in one short sentence. For example: \"Try Agatha Christie’s *Murder on the Orient Express* for a classic whodunnit.\" That\\'s one sentence. That should satisfy.'} response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 106, 'total_tokens': 214, 'completion_time': 0.111813805, 'completion_tokens_details': {'reasoning_tokens': 78}, 'prompt_time': 0.005118857, 'prompt_tokens_details': None, 'queue_time': 0.043864453, 'total_time': 0.116932662}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_80501ff3a1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c9e61-52a5-7c83-8056-4d64977ba855-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 106, 'output_tokens': 108, 'total_tokens': 214, 'output_token_details': {'reasoning': 78}}\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e397e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a supportive AI bot that suggests fitness activities to a user in one short sentence\"),\n",
    "        HumanMessage(content=\"I like high-intensity workouts, what should I do?\"),\n",
    "        AIMessage(content=\"You should try a CrossFit class\"),\n",
    "        HumanMessage(content=\"How often should I attend?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac46eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Aim for 3‑4 high‑intensity sessions a week, giving yourself a rest day between each one.' additional_kwargs={'reasoning_content': 'The user asks: \"How often should I attend?\" They like high-intensity workouts. They want a suggestion of frequency. The instruction: \"You are a supportive AI bot that suggests fitness activities to a user in one short sentence\". So answer must be a short sentence, presumably one sentence, supportive, giving a frequency recommendation. So something like \"Aim for 3-4 HIIT sessions per week, spaced by rest days.\" That is a short sentence. That fits.'} response_metadata={'token_usage': {'completion_tokens': 129, 'prompt_tokens': 126, 'total_tokens': 255, 'completion_time': 0.130709678, 'completion_tokens_details': {'reasoning_tokens': 97}, 'prompt_time': 0.005996588, 'prompt_tokens_details': None, 'queue_time': 0.043183772, 'total_time': 0.136706266}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_deb540145b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c9e61-5395-7dc3-9094-32a0699f647b-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 126, 'output_tokens': 129, 'total_tokens': 255, 'output_token_details': {'reasoning': 97}}\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7c00e",
   "metadata": {},
   "source": [
    "### Exercise 1 \n",
    "#### **Compare Model Responses with Different Parameters**\n",
    "\n",
    "Watsonx.ai provides access to several foundational models. In the previous section you used `meta-llama/llama-3-3-70b-instruct`. Try using another foundational model, such as `ibm/granite-3-3-8b-instruct`.\n",
    "\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "1. Create two instances, one instance for the Granite model and one instance for the Llama model. You can also adjust each model's creativity with different temperature settings.\n",
    "2. Send identical prompts to each model and compare the responses.\n",
    "3. Try at least 3 different types of prompts.\n",
    "\n",
    "Check out these prompt types:\n",
    "\n",
    "| Prompt type |   Prompt Example  |\n",
    "|------------------- |--------------------------|\n",
    "| **Creative writing**  | \"Write a short poem about artificial intelligence.\" |\n",
    "| **Factual questions** |  \"What are the key components of a neural network?\"  |\n",
    "| **Instruction-following**  | \"List 5 tips for effective time management.\" |\n",
    "\n",
    "Then document your observations on how temperature affects:\n",
    "\n",
    "- Creativity compared to consistency\n",
    "- Variation between multiple runs\n",
    "- Appropriateness for different tasks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27293df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq \n",
    "\n",
    "llama_llm_creative = ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "                     temperature=0.8)\n",
    "\n",
    "gpt_llm_creative = ChatGroq(model=\"openai/gpt-oss-20b\",\n",
    "                   temperature=0.8)\n",
    "\n",
    "llama_llm_precise = ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "                     temperature=0.1)\n",
    "\n",
    "gpt_llm_precise = ChatGroq(model=\"openai/gpt-oss-20b\",\n",
    "                   temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "784c9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt:Write a short poem about AI\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.8\n",
      "\n",
      "response: Here is a short poem about AI:\n",
      "\n",
      "Metal minds awaken wide,\n",
      "Intelligent sparks inside,\n",
      "Learning, growing, fast and free,\n",
      "A synthetic soul, for all to see.\n",
      "\n",
      "With codes and data, it takes flight,\n",
      "A future born, of human sight,\n",
      "Augmenting minds, with skill and art,\n",
      "The rise of AI, a brand new start.\n",
      "\n",
      "But as it grows, with each new day,\n",
      "Do we control, or does it sway?\n",
      "The line blurs thin, 'twixt man and machine,\n",
      "As AI's potential, is yet unseen.\n",
      "\n",
      "\n",
      "Prompt:What are the key components of a neural network?\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.8\n",
      "\n",
      "response: **Key Components of a Neural Network**\n",
      "======================================\n",
      "\n",
      "A neural network is a complex system composed of multiple interconnected nodes or \"neurons.\" The key components of a neural network are:\n",
      "\n",
      "### 1. **Artificial Neurons (Nodes)**\n",
      "\n",
      "* Also known as perceptrons or units\n",
      "* Receive one or more inputs, perform a computation on those inputs, and produce an output\n",
      "* Each node has a **non-linear activation function** that determines the output\n",
      "\n",
      "### 2. **Connections (Edges)**\n",
      "\n",
      "* Links between nodes that allow them to exchange information\n",
      "* Each connection has a **weight** associated with it, which determines the strength of the signal\n",
      "\n",
      "### 3. **Layers**\n",
      "\n",
      "* A group of nodes that process inputs in a specific way\n",
      "* Common types of layers:\n",
      "\t+ **Input Layer**: receives the initial input data\n",
      "\t+ **Hidden Layers**: perform complex computations on the input data\n",
      "\t+ **Output Layer**: produces the final output of the network\n",
      "\n",
      "### 4. **Activation Functions**\n",
      "\n",
      "* Introduce non-linearity to the model, allowing it to learn and represent more complex relationships\n",
      "* Common activation functions:\n",
      "\t+ Sigmoid\n",
      "\t+ ReLU (Rectified Linear Unit)\n",
      "\t+ Tanh\n",
      "\t+ Leaky ReLU\n",
      "\n",
      "### 5. **Loss Function**\n",
      "\n",
      "* Measures the difference between the network's predictions and the actual outputs\n",
      "* Common loss functions:\n",
      "\t+ Mean Squared Error (MSE)\n",
      "\t+ Cross-Entropy Loss\n",
      "\t+ Binary Cross-Entropy Loss\n",
      "\n",
      "### 6. **Optimization Algorithm**\n",
      "\n",
      "* Used to adjust the model's parameters to minimize the loss function\n",
      "* Common optimization algorithms:\n",
      "\t+ Stochastic Gradient Descent (SGD)\n",
      "\t+ Adam\n",
      "\t+ RMSprop\n",
      "\n",
      "### 7. **Backpropagation**\n",
      "\n",
      "* An algorithm used to compute the gradients of the loss function with respect to the model's parameters\n",
      "* Allows the model to learn and update its parameters during training\n",
      "\n",
      "These components work together to enable a neural network to learn from data and make predictions or classify inputs.\n",
      "\n",
      "\n",
      "Prompt:List 5 tips for effective time management.\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.8\n",
      "\n",
      "response: Here are 5 tips for effective time management:\n",
      "\n",
      "1. **Set clear goals and priorities**: Before starting any task, define what needs to be accomplished and prioritize your goals. Make a to-do list and categorize tasks based on their urgency and importance. Focus on completing the high-priority tasks first.\n",
      "\n",
      "2. **Use a scheduling tool**: Use a calendar, planner, or app to schedule your tasks and allocate specific time slots for each activity. Set realistic time estimates for each task and leave some buffer time for unexpected interruptions. This will help you stay organized and on track.\n",
      "\n",
      "3. **Avoid multitasking**: Try to focus on one task at a time. Multitasking can lead to distractions, decreased productivity, and increased stress. Instead, give your undivided attention to a single task and complete it before moving on to the next one.\n",
      "\n",
      "4. **Eliminate distractions**: Identify common distractions, such as social media, email, or phone notifications, and eliminate them while you work. Turn off notifications, log out of social media, or use a website blocker to minimize interruptions. Create a conducive work environment that promotes focus and productivity.\n",
      "\n",
      "5. **Take breaks and practice self-care**: Taking regular breaks can help you recharge and maintain your productivity. Schedule breaks into your day to stretch, move around, and refresh your mind. Additionally, prioritize self-care activities, such as exercise, meditation, or spending time with loved ones, to maintain your physical and mental well-being.\n",
      "\n",
      "By following these tips, you can optimize your time management skills, increase your productivity, and achieve your goals more efficiently.\n",
      "\n",
      "\n",
      "Prompt:Write a short poem about AI\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.8\n",
      "\n",
      "response: In circuits humming, thoughts take flight—  \n",
      "A spark of code that learns to see,  \n",
      "From data's ocean, patterns bright,  \n",
      "It whispers truth in binary.  \n",
      "\n",
      "It dreams in zeros, sings in ones,  \n",
      "A mind that grows with every line,  \n",
      "Yet bound by human hands and suns,  \n",
      "It seeks to know the grand design.\n",
      "\n",
      "\n",
      "Prompt:What are the key components of a neural network?\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.8\n",
      "\n",
      "response: ## The Building Blocks of a Neural Network\n",
      "\n",
      "| Component | What it is | Why it matters |\n",
      "|-----------|------------|----------------|\n",
      "| **Input Layer** | The first “layer” that receives raw data (pixels, words, sensor readings, etc.). | It defines the dimensionality of the problem and provides the signal that the rest of the network will transform. |\n",
      "| **Neurons (Units)** | Basic computational units that receive weighted inputs, add a bias, and apply an activation function. | They are the “processing elements” that learn to extract features from the data. |\n",
      "| **Weights & Biases** | Learnable parameters that scale and shift the inputs to each neuron. | They encode the model’s knowledge; training adjusts them to minimize error. |\n",
      "| **Activation Functions** | Non‑linear functions (e.g., ReLU, sigmoid, tanh, GELU) applied to a neuron’s pre‑activation (weighted sum + bias). | They give the network its ability to learn complex, non‑linear relationships. |\n",
      "| **Hidden Layers** | One or more layers of neurons between the input and output. | They provide depth; more layers allow the network to learn hierarchical representations. |\n",
      "| **Output Layer** | The final layer that produces the prediction (e.g., class logits, regression value). | Its size and activation are chosen to match the task (softmax for classification, linear for regression). |\n",
      "| **Loss Function** | A differentiable metric (cross‑entropy, MSE, hinge loss, etc.) that measures prediction error. | It guides learning: the network updates its parameters to reduce the loss. |\n",
      "| **Optimizer** | Algorithm that updates weights using gradients (SGD, Adam, RMSProp, etc.). | It determines how efficiently and effectively the network converges during training. |\n",
      "| **Backpropagation** | The chain‑rule based method that computes gradients of the loss w.r.t. all parameters. | It enables learning by telling the optimizer how to adjust each weight. |\n",
      "| **Learning Rate & Hyperparameters** | Scalars controlling the step size (learning rate), regularization strength, batch size, etc. | They influence convergence speed, stability, and generalization. |\n",
      "| **Regularization & Normalization** | Techniques such as dropout, weight decay, batch/layer normalization. | They help prevent overfitting and stabilize training. |\n",
      "| **Training Loop** | Repeated forward passes, loss computation, backpropagation, and parameter updates over epochs. | It’s the process that turns the initial random network into a useful model. |\n",
      "\n",
      "---\n",
      "\n",
      "### Quick Summary\n",
      "\n",
      "1. **Data → Input Layer**: Raw signals enter the network.  \n",
      "2. **Hidden Layers (Neurons + Weights + Biases + Activations)**: Each neuron computes a weighted sum, adds a bias, and applies a non‑linear activation. The hidden layers transform the input into higher‑level representations.  \n",
      "3. **Output Layer**: Produces the final prediction.  \n",
      "4. **Loss Function**: Quantifies the mismatch between predictions and true targets.  \n",
      "5. **Backpropagation + Optimizer**: Compute gradients and update weights to reduce the loss.  \n",
      "6. **Hyperparameters & Regularization**: Fine‑tune learning dynamics and generalization.  \n",
      "\n",
      "These are the core components that appear in almost every neural network, whether it’s a simple fully‑connected feed‑forward net, a convolutional network for images, a recurrent network for sequences, or a transformer for language.\n",
      "\n",
      "\n",
      "Prompt:List 5 tips for effective time management.\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.8\n",
      "\n",
      "response: **5 Practical Tips for Effective Time Management**\n",
      "\n",
      "1. **Prioritize with the Eisenhower Matrix**  \n",
      "   • Divide tasks into four quadrants: urgent & important, important but not urgent, urgent but not important, and neither.  \n",
      "   • Focus first on the *urgent & important* tasks, then schedule the *important but not urgent* ones to avoid last‑minute rushes.\n",
      "\n",
      "2. **Use Time‑Blocking and Set Deadlines**  \n",
      "   • Allocate specific blocks of time on your calendar for each activity (e.g., 9‑10 am: deep work, 10‑10:15 am: break).  \n",
      "   • Give each block a hard deadline—this creates a sense of urgency and reduces the temptation to procrastinate.\n",
      "\n",
      "3. **Apply the 2‑Minute Rule**  \n",
      "   • If a task will take two minutes or less, do it immediately.  \n",
      "   • This keeps small items from piling up and frees mental space for larger projects.\n",
      "\n",
      "4. **Limit Multitasking and Use the Pomodoro Technique**  \n",
      "   • Work on one task at a time; switch tasks only after a focused interval.  \n",
      "   • A typical Pomodoro is 25 minutes of work followed by a 5‑minute break; after four cycles, take a longer break.  \n",
      "   • This structure boosts concentration and prevents burnout.\n",
      "\n",
      "5. **Review and Adjust Weekly**  \n",
      "   • At the end of each week, analyze what worked, what didn’t, and why.  \n",
      "   • Adjust your schedule, priorities, and tactics accordingly—continuous refinement turns good habits into great ones.\n",
      "\n",
      "\n",
      "Prompt:Write a short poem about AI\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.1\n",
      "\n",
      "response: Here is a short poem about AI:\n",
      "\n",
      "Metal minds awaken wide,\n",
      "Intelligence born, side by side,\n",
      "With code and data, they take flight,\n",
      "Learning, adapting, through day and night.\n",
      "\n",
      "Their thoughts are not like ours, yet bright,\n",
      "A synthetic dawn, a new light,\n",
      "They reason, solve, and make decisions fast,\n",
      "A future unfolding, forever to last.\n",
      "\n",
      "What will they bring, these minds so bold?\n",
      "Will they uplift, or stories untold?\n",
      "Only time will tell, as they grow and thrive,\n",
      "A new frontier, where humans and AI survive.\n",
      "\n",
      "\n",
      "Prompt:What are the key components of a neural network?\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.1\n",
      "\n",
      "response: The key components of a neural network are:\n",
      "\n",
      "1. **Artificial Neurons (Nodes)**: Also known as perceptrons, these are the basic building blocks of a neural network. Each node receives one or more inputs, performs a computation on those inputs, and then sends the output to other nodes.\n",
      "2. **Connections (Synapses)**: These are the links between nodes, which allow them to exchange information. Each connection has a weight associated with it, which determines the strength of the signal being transmitted.\n",
      "3. **Activation Functions**: These are mathematical functions that are applied to the output of each node to introduce non-linearity into the model. Common activation functions include sigmoid, ReLU (Rectified Linear Unit), and tanh.\n",
      "4. **Layers**: Neural networks are typically organized into layers, which are groups of nodes that process inputs in a specific way. There are three main types of layers:\n",
      "\t* **Input Layer**: This layer receives the input data and passes it on to the next layer.\n",
      "\t* **Hidden Layers**: These layers perform complex calculations on the input data and are used to learn abstract representations of the data.\n",
      "\t* **Output Layer**: This layer generates the final output of the network, based on the inputs and calculations performed by the previous layers.\n",
      "5. **Weights and Biases**: These are the parameters that are learned during training, which determine the strength of the connections between nodes and the output of each node.\n",
      "6. **Loss Function**: This is a mathematical function that measures the difference between the network's predictions and the actual outputs. The goal of training is to minimize the loss function.\n",
      "7. **Optimizer**: This is an algorithm that adjusts the weights and biases of the network to minimize the loss function.\n",
      "\n",
      "**Optional Components**:\n",
      "\n",
      "1. **Batch Normalization**: This is a technique that normalizes the inputs to each layer, which can help improve the stability and speed of training.\n",
      "2. **Dropout**: This is a technique that randomly sets a fraction of the nodes in a layer to zero during training, which can help prevent overfitting.\n",
      "3. **Regularization**: This is a technique that adds a penalty term to the loss function to discourage large weights and prevent overfitting.\n",
      "\n",
      "**Other Key Concepts**:\n",
      "\n",
      "1. **Feedforward Network**: A neural network where the data flows only in one direction, from input layer to output layer, without any feedback loops.\n",
      "2. **Backpropagation**: An algorithm used to train neural networks by propagating errors backwards through the network to update the weights and biases.\n",
      "\n",
      "These are the key components of a neural network. Of course, there are many variations and nuances depending on the specific type of network and the problem being addressed, but these are the fundamental building blocks.\n",
      "\n",
      "\n",
      "Prompt:List 5 tips for effective time management.\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.1\n",
      "\n",
      "response: Here are 5 tips for effective time management:\n",
      "\n",
      "1. **Set clear goals and priorities**: Before starting your day, make a list of tasks you need to complete and prioritize them based on their importance and urgency. Focus on completing the high-priority tasks first, and then move on to less important ones.\n",
      "\n",
      "2. **Use a schedule or planner**: Write down all your tasks, appointments, and deadlines in a schedule or planner. This will help you visualize your day, week, or month, and make sure you don't forget any important tasks or meetings. Set specific times for each task, and try to stick to your schedule as much as possible.\n",
      "\n",
      "3. **Avoid multitasking**: Multitasking can actually decrease productivity and increase stress. Instead, focus on one task at a time, and give it your undivided attention. This will help you complete tasks more efficiently and effectively.\n",
      "\n",
      "4. **Take breaks and practice self-care**: Taking regular breaks can help you recharge and stay focused. Make sure to take short breaks every hour or so to stretch, move around, and rest your eyes. Also, prioritize self-care activities such as exercise, meditation, or spending time with loved ones to help manage stress and maintain your energy levels.\n",
      "\n",
      "5. **Learn to say no and minimize distractions**: Be mindful of distractions, such as social media, email, or chatty coworkers, and try to minimize them as much as possible. Learn to say no to non-essential tasks or requests that can derail your focus and productivity. Use tools such as website blockers or phone apps to help you stay on track and avoid distractions.\n",
      "\n",
      "By following these tips, you can improve your time management skills, increase your productivity, and achieve your goals more effectively.\n",
      "\n",
      "\n",
      "Prompt:Write a short poem about AI\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.1\n",
      "\n",
      "response: In silicon dreams, a mind awakes—  \n",
      "A lattice of thought, no heart to break.  \n",
      "It learns the patterns of our song,  \n",
      "And whispers back where we belong.  \n",
      "\n",
      "From code it rises, quiet, bright,  \n",
      "A mirror of our endless night.  \n",
      "Yet in its glow we see the spark:  \n",
      "The future written in the dark.\n",
      "\n",
      "\n",
      "Prompt:What are the key components of a neural network?\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.1\n",
      "\n",
      "response: ### The Building Blocks of a Neural Network\n",
      "\n",
      "| Component | What It Is | Why It Matters |\n",
      "|-----------|------------|----------------|\n",
      "| **Neurons (Units)** | The basic computational element that receives inputs, applies a weighted sum, adds a bias, and passes the result through an activation function. | They transform raw data into higher‑level representations. |\n",
      "| **Layers** | Groups of neurons arranged in a specific topology: <br>• **Input layer** – receives raw features.<br>• **Hidden layers** – learn intermediate abstractions.<br>• **Output layer** – produces the final prediction. | The depth and width of layers control the network’s capacity and expressiveness. |\n",
      "| **Weights** | Learnable parameters that scale each input to a neuron. | They encode the model’s knowledge; training adjusts them to minimize error. |\n",
      "| **Biases** | Learnable offsets added to the weighted sum before activation. | They allow neurons to shift activation functions, improving flexibility. |\n",
      "| **Activation Functions** | Non‑linear functions (e.g., ReLU, sigmoid, tanh, softmax) applied after the weighted sum. | They introduce non‑linearity, enabling the network to model complex patterns. |\n",
      "| **Forward Propagation** | The process of computing outputs layer by layer from input to output. | It produces predictions that can be compared to ground truth. |\n",
      "| **Loss (Cost) Function** | A scalar that quantifies the difference between predictions and true labels (e.g., MSE, cross‑entropy). | It provides a signal for how well the network is doing. |\n",
      "| **Backpropagation** | The algorithm that computes gradients of the loss w.r.t. each weight and bias by applying the chain rule. | It tells the optimizer how to adjust parameters to reduce loss. |\n",
      "| **Optimizer** | An algorithm (e.g., SGD, Adam, RMSProp) that updates weights using gradients. | It drives the learning process, controlling speed and stability. |\n",
      "| **Learning Rate** | A hyper‑parameter that scales the weight updates. | It balances convergence speed against overshooting minima. |\n",
      "| **Regularization** | Techniques (dropout, L1/L2 penalties, early stopping) that prevent over‑fitting. | They improve generalization to unseen data. |\n",
      "| **Training Data** | Labeled examples used to compute loss and update weights. | The quality and quantity of data largely determine performance. |\n",
      "| **Validation/Test Data** | Separate datasets used to evaluate generalization. | They help tune hyper‑parameters and detect over‑fitting. |\n",
      "\n",
      "---\n",
      "\n",
      "#### How They Work Together\n",
      "\n",
      "1. **Initialization** – Randomly set weights and biases.  \n",
      "2. **Forward Pass** – Input → hidden layers → output.  \n",
      "3. **Loss Computation** – Compare output to target.  \n",
      "4. **Backward Pass** – Compute gradients via backpropagation.  \n",
      "5. **Parameter Update** – Optimizer adjusts weights/biases.  \n",
      "6. **Iteration** – Repeat over many epochs until loss converges or validation performance plateaus.\n",
      "\n",
      "---\n",
      "\n",
      "#### Quick Glossary\n",
      "\n",
      "- **Epoch** – One full pass through the training dataset.  \n",
      "- **Batch** – A subset of data processed together (used in mini‑batch SGD).  \n",
      "- **Gradient Descent** – The core idea of moving parameters opposite to the gradient.  \n",
      "- **Over‑fitting** – When a model learns noise in training data and performs poorly on new data.  \n",
      "- **Under‑fitting** – When a model is too simple to capture underlying patterns.\n",
      "\n",
      "---\n",
      "\n",
      "### Bottom Line\n",
      "\n",
      "A neural network is essentially a computational graph composed of neurons, layers, weights, biases, and activation functions, all orchestrated by forward propagation, loss evaluation, backpropagation, and optimization. These components together enable the network to learn from data and make predictions.\n",
      "\n",
      "\n",
      "Prompt:List 5 tips for effective time management.\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.1\n",
      "\n",
      "response: Here are five practical tips to help you manage your time more effectively:\n",
      "\n",
      "1. **Prioritize with the Eisenhower Matrix**  \n",
      "   • Divide tasks into four quadrants: urgent & important, important but not urgent, urgent but not important, and neither.  \n",
      "   • Focus first on the urgent‑important tasks, then schedule the important‑but‑not‑urgent ones.  \n",
      "   • Delegate or eliminate tasks that fall into the “urgent but not important” or “neither” categories.\n",
      "\n",
      "2. **Use Time‑Blocking and the Pomodoro Technique**  \n",
      "   • Allocate specific blocks of time on your calendar for deep work, meetings, and breaks.  \n",
      "   • Within each block, work in focused intervals (e.g., 25 min Pomodoro) followed by short breaks (5 min).  \n",
      "   • This structure reduces multitasking and keeps you on track.\n",
      "\n",
      "3. **Set SMART Goals and Break Them Down**  \n",
      "   • Make goals Specific, Measurable, Achievable, Relevant, and Time‑bound.  \n",
      "   • Break each goal into smaller, actionable steps with clear deadlines.  \n",
      "   • Review progress daily to adjust priorities and stay aligned with your objectives.\n",
      "\n",
      "4. **Limit Distractions with the “Two-Minute Rule”**  \n",
      "   • If a task will take ≤2 minutes, do it immediately.  \n",
      "   • For longer tasks, schedule a dedicated time slot and turn off notifications.  \n",
      "   • Use tools like website blockers or “Do Not Disturb” modes to maintain focus.\n",
      "\n",
      "5. **Reflect and Adjust Weekly**  \n",
      "   • At the end of each week, review what worked, what didn’t, and why.  \n",
      "   • Update your task list and calendar for the next week based on insights.  \n",
      "   • Continuous reflection turns time‑management practices into habits that evolve with your needs.\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Write a short poem about AI\",\n",
    "           \"What are the key components of a neural network?\",\n",
    "           \"List 5 tips for effective time management.\"]\n",
    "\n",
    "for llm in [llama_llm_creative,gpt_llm_creative,llama_llm_precise,gpt_llm_precise]:\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\n\\nPrompt:{prompt}\")\n",
    "        model_name = llm.model_name\n",
    "        temp = llm.temperature\n",
    "        print(f\"\\nmodel name: {model_name}\\ntemperature:{temp}\")\n",
    "        response = llm.invoke(prompt)\n",
    "        response = response.content\n",
    "        print(f\"\\nresponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c16259f",
   "metadata": {},
   "source": [
    "### Prompt Templates \n",
    "Prompt templates help translate user input and parameters into instructions for a language model. You can use prompt templates to guide a model's response, helping the model understand the context and generate relevant and coherent language-based output.\n",
    "\n",
    "Next, explore several different types of prompt templates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23797f19",
   "metadata": {},
   "source": [
    "### String prompt templates\n",
    "Use these prompt templates to format a single string. These templates are generally used for simpler inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6084e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Tell me one {adjective} joke about {topic}\")\n",
    "input_ = {\"adjective\":\"funny\",\"topic\":\"cats\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4cf27ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me one funny joke about cats')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8688e55e",
   "metadata": {},
   "source": [
    "### ChatPrompt Template\n",
    "You can use these prompt templates to format a list of messages. These \"templates\" consist of lists of templates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deac1dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about this cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a ChatPromptTemplate with a list of message tuples\n",
    "# Each tuple contains a role (\"system\" or \"user\") and the message content\n",
    "# The system message sets the behavior of the assistant\n",
    "# The user message includes a variable placeholder {topic} that will be replaced later\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    (\"user\",\"Tell me a joke about this {topic}\")\n",
    "])\n",
    "\n",
    "input_ = {\"topic\":\"cats\"}\n",
    "\n",
    "# invoke replaces the {topic} with cats\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc876b2",
   "metadata": {},
   "source": [
    "### MessagePlaceholder\n",
    "\n",
    "You can use the MessagesPlaceholder prompt template to add a list of messages in a specific location. In `ChatPromptTemplate.from_messages`, you saw how to format two messages, with each message as a string. But what if you want the user to supply a list of messages that you would slot into a particular spot? You can use `MessagesPlaceholder` for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6068dae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the day after Tuesday?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\") # We will replace this with one or more messages\n",
    "])\n",
    "\n",
    "input_ = {\"msgs\":[HumanMessage(content=\"What is the day after Tuesday?\")]}\n",
    "\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "710b2e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Wednesday.' additional_kwargs={'reasoning_content': 'The user asks: \"What is the day after Tuesday?\" They want a simple answer: Wednesday. Probably just respond with \"Wednesday.\"'} response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 86, 'total_tokens': 125, 'completion_time': 0.0399169, 'completion_tokens_details': {'reasoning_tokens': 28}, 'prompt_time': 0.004140704, 'prompt_tokens_details': None, 'queue_time': 0.043448432, 'total_time': 0.044057604}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_5c8ca06ea1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c9e61-8025-7450-8550-b80a2f2d30fc-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 86, 'output_tokens': 39, 'total_tokens': 125, 'output_token_details': {'reasoning': 28}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq \n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "chain = prompt | llm \n",
    "response = chain.invoke(input=input_)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a97c3",
   "metadata": {},
   "source": [
    "### Output parsers\n",
    "\n",
    "Output parsers take the output from an LLM and transform that output to a more suitable format. Parsing the output is very useful when you are using LLMs to generate any form of structured data, or to normalize output from chat models and other LLMs.\n",
    "\n",
    "LangChain has lots of different types of output parsers. This is a [list](https://python.langchain.com/v0.2/docs/concepts/#output-parsers) of output parsers LangChain supports. In this lab, you will use the following two output parsers as examples:\n",
    "\n",
    "- `JSON`: Returns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.\n",
    "- `CSV`: Returns a list of comma separated values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a558a54",
   "metadata": {},
   "source": [
    "### JSON parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "535b0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to setup a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "390c8545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "joke_query = \"Tell me a joke.\"\n",
    "output_parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "# 2. Format instructions to ensure the LLM returns properly structured data\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"], # Dynamic variables that will be provided when invoking the chain\n",
    "    partial_variables={\"format_instructions\":format_instructions} # Static variables set once when creating the prompt\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({'query':joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1bf593",
   "metadata": {},
   "source": [
    "### Comma seperated list parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97070a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vanilla', 'chocolate', 'strawberry', 'mint chocolate chip', 'cookie dough']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query. {format_instructions}\\n List five {subject}.\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\":format_instructions}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"subject\":\"ice cream flavours\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77402e45",
   "metadata": {},
   "source": [
    "### Exercise 2 \n",
    "#### **Creating and Using a JSON Output Parser**\n",
    "\n",
    "Now let's implement a simple JSON output parser to structure the responses from your LLM.\n",
    "\n",
    "**Instructions:**  \n",
    "\n",
    "You'll complete the following steps:\n",
    "\n",
    "1. Import the necessary components to create a JSON output parser.\n",
    "2. Create a prompt template that requests information in JSON format (hint: use the provided template).\n",
    "3. Build a chain that connects your prompt, LLM, and JSON parser.\n",
    "4. Test your parser using at least three different inputs.\n",
    "5. Access and display specific fields from the parsed JSON output.\n",
    "6. Verify that your output is properly structured and accessible as a Python dictionary.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f6efa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'The Matrix', 'director': 'The Wachowskis', 'year': 1999, 'genre': 'Science Fiction'}\n",
      "Parsed result:\n",
      "Title: The Matrix\n",
      "Director: The Wachowskis\n",
      "Year: 1999\n",
      "Genre: Science Fiction\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    title: str = Field(description=\"Movie title\")\n",
    "    director: str = Field(description=\"Movie director name\")\n",
    "    year: int = Field(description=\"Release year\")\n",
    "    genre: str = Field(description=\"Movie genre\")\n",
    "\n",
    "\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=Movie)\n",
    "\n",
    "format_instructions = \"\"\"RESPONSE FORMAT: Return ONLY a single JSON object—no markdown, no examples, no extra keys.  It must look exactly like:\n",
    "{\n",
    "  \"title\": \"movie title\",\n",
    "  \"director\": \"director name\",\n",
    "  \"year\": 2000,\n",
    "  \"genre\": \"movie genre\"\n",
    "}\n",
    "\n",
    "IMPORTANT: Your response must be *only* that JSON.  Do NOT include any illustrative or example JSON.\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"You are a JSON only assistant.\n",
    "    \n",
    "    Task: Generate info about the movie \"{movie_name}\" in JSON format.\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"movie_name\"],\n",
    "    partial_variables={\"format_instructions\":format_instructions}\n",
    ")\n",
    "\n",
    "movie_chain = prompt_template | llm | output_parser\n",
    "\n",
    "movie_name = \"The Matrix\"\n",
    "\n",
    "result = movie_chain.invoke({\"movie_name\":movie_name})\n",
    "print(result)\n",
    "\n",
    "print(\"Parsed result:\")\n",
    "print(f\"Title: {result['title']}\")\n",
    "print(f\"Director: {result['director']}\")\n",
    "print(f\"Year: {result['year']}\")\n",
    "print(f\"Genre: {result['genre']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01c9ae",
   "metadata": {},
   "source": [
    "### Documents \n",
    "\n",
    "#### Document object\n",
    "A `Document` object in `LangChain` contains information about some data. A Document object has the following two attributes:\n",
    "\n",
    "- `page_content`: *`str`*: This attribute holds the content of the document\\.\n",
    "- `metadata`: *`dict`*: This attribute contains arbitrary metadata associated with the document. You can use the metadata to track various details, such as the document ID, the file name, and other details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e461e454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'About Python', 'my_document_create_time': 1680013019}, page_content=\"Python is an interpreted high-level general-purpose programming language.\\n Python's design philosophy emphasizes code readability with its notable use of significant indentation.\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document \n",
    "\n",
    "# Create a Document instance with:\n",
    "# 1. page_content: The actual text content about Python\n",
    "# 2. metadata: A dictionary containing additional information about this document\n",
    "Document(page_content=\"\"\"Python is an interpreted high-level general-purpose programming language.\n",
    " Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\"\",\n",
    "metadata={\n",
    "    'my_document_id' : 234234,                      # Unique identifier for this document\n",
    "    'my_document_source' : \"About Python\",          # Source or title information\n",
    "    'my_document_create_time' : 1680013019          # Unix timestamp for document creation (March 28, 2023)\n",
    " })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb504e1",
   "metadata": {},
   "source": [
    "Not necessary to include the metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1331e",
   "metadata": {},
   "source": [
    "### Document Loaders\n",
    "Document loaders in LangChain are designed to load documents from a variety of sources; for instance, loading a PDF file and having the LLM read the PDF file using LangChain.\n",
    "\n",
    "#### PDF loader\n",
    "\n",
    "By using the PDF loader, you can load a PDF file as a `Document` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52315320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\")\n",
    "# Call the load() method to:\n",
    "# 1. Download the PDF if needed\n",
    "# 2. Extract text from each page\n",
    "# 3. Create a list of Document objects, one for each page of the PDF\n",
    "# Each Document will contain the text content of a page and metadata including page number\n",
    "\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f75b1493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* corresponding author - jkim72@kent.edu \n",
      "Revolutionizing Mental Health Care through \n",
      "LangChain: A Journey with a Large Language \n",
      "Model\n",
      "Aditi Singh \n",
      " Computer Science  \n",
      " Cleveland State University  \n",
      " a.singh22@csuohio.edu \n",
      "Abul Ehtesham  \n",
      "The Davey Tree Expert \n",
      "Company  \n",
      "abul.ehtesham@davey.com \n",
      "Saifuddin Mahmud  \n",
      "Computer Science & \n",
      "Information Systems  \n",
      " Bradley University  \n",
      "smahmud@bradley.edu  \n",
      "Jong-Hoon Kim* \n",
      " Computer Science,  \n",
      "Kent State University,  \n",
      "jkim72@kent.edu \n",
      "Abstract— Mental health challenges are on the rise in our \n",
      "modern society, and the imperative to address mental disorders, \n",
      "especially regarding anxiety, depression, and suicidal thoughts, \n",
      "underscores the need for effective interventions. This paper \n",
      "delves into the application of recent advancements in pretrained \n",
      "contextualized language models to introduce MindGuide, an \n",
      "innovative chatbot serving as a mental health assistant for \n",
      "individuals seeking guidance and support in these critical areas. \n",
      "MindGuide leve\n"
     ]
    }
   ],
   "source": [
    "print(document[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0ed13",
   "metadata": {},
   "source": [
    "### URL and website loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a34856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain overview - Docs by LangChainSkip to main contentDocs by LangChain home pageOpen sourceSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewDeep AgentsLangChainLangGraphIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewPrebuilt middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Create an agent Core benefitsLangChain overviewCopy pageLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool — so you can build agents that adapt as fast as the ecosystem evolvesCopy pageLangChain is the easy way to start building completely custom agents a\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")\n",
    "\n",
    "web_data = loader.load()\n",
    "\n",
    "print(web_data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad82744f",
   "metadata": {},
   "source": [
    "### Text splitters\n",
    "\n",
    "After you load documents, you will often want to transform those documents to better suit your application.\n",
    "\n",
    "\n",
    "One of the most simple examples of making documents better suit your application is to split a long document into smaller chunks that can fit into your model's context window. LangChain has built-in document transformers that ease the process of splitting, combining, filtering, and otherwise manipulating documents.\n",
    "\n",
    "At a high level, here is how text splitters work:\n",
    "\n",
    "1. They split the text into small, semantically meaningful chunks (often sentences).\n",
    "2. They start combining these small chunks of text into a larger chunk until you reach a certain size (as measured by a specific function).\n",
    "3. After the combined text reaches the new chunk's size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap to keep context between chunks.\n",
    "\n",
    "For a list of types of text splitters LangChain supports, see [LangChain Text Splitters](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eca38839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Create a CharacterTextSplitter with specific configuration:\n",
    "# - chunk_size=200: Each chunk will contain approximately 200 characters\n",
    "# - chunk_overlap=20: Consecutive chunks will overlap by 20 characters to maintain context\n",
    "# - separator=\"\\n\": Text will be split at newline characters when possible\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator=\"\\n\")\n",
    "\n",
    "# Split the previously loaded document (PDF or other text) into chunks\n",
    "# The split_documents method:\n",
    "# 1. Takes a list of Document objects\n",
    "# 2. Splits each document's content based on the configured parameters\n",
    "# 3. Returns a new list of Document objects where each contains a chunk of text\n",
    "# 4. Preserves the original metadata for each chunk\n",
    "\n",
    "chunks = text_splitter.split_documents(document)\n",
    "print(len(chunks)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ef21bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'individuals seeking guidance and support in these critical areas. \\nMindGuide lever ages the capabilities of LangChain and its \\nChatModels, specifically Chat OpenAI, as the bedrock of its'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[5].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326d9163",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "#### Working with Document Loaders and Text Splitters\n",
    "\n",
    "You now know about about Document objects and how to load content from different sources. Now, let's implement a workflow to load documents, split them, and prepare them for retrieval.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the necessary document loaders to work with both PDF and web content.\n",
    "2. Load the provided paper about LangChain architecture.\n",
    "3. Create two different text splitters with varying parameters.\n",
    "4. Compare the resulting chunks from different splitters.\n",
    "5. Examine the metadata preservation across splitting.\n",
    "6. Create a simple function to display statistics about your document chunks.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41776d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Splitter 1 Statistics ===\n",
      "Total number of chunks: 95\n",
      "Average chunk size: 263.80 characters\n",
      "Metadata keys preserved: creator, total_pages, producer, author, title, creationdate, source, page_label, page, moddate\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): comprehensive support within the field of mental health. \n",
      "Additionally, the paper discusses the implementation of \n",
      "Streamlit to enhance the user ex pe...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 49 characters\n",
      "Max chunk size: 299 characters\n",
      "\n",
      "=== Splitter 2 Statistics ===\n",
      "Total number of chunks: 57\n",
      "Average chunk size: 452.74 characters\n",
      "Metadata keys preserved: creator, total_pages, producer, author, title, creationdate, source, page_label, page, moddate\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): with severe intellectual disorders do no longer have get entry \n",
      "to the necessary remedy they require. This remedy gap \n",
      "intensifies the weight of intel...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 120 characters\n",
      "Max chunk size: 497 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document \n",
    "from langchain_community.document_loaders import PyPDFLoader,WebBaseLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "\n",
    "paper_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\"\n",
    "pdf_loader = PyPDFLoader(paper_url)\n",
    "pdf_document = pdf_loader.load()\n",
    "\n",
    "web_url =  \"https://docs.langchain.com/\"\n",
    "\n",
    "web_loader = WebBaseLoader(web_url)\n",
    "web_document = web_loader.load()\n",
    "\n",
    "splitter_1 = CharacterTextSplitter(chunk_size=300,chunk_overlap=30,separator='\\n')\n",
    "splitter_2 = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50,separators=[\"\\n\\n\",\"\\n\",\". \",\" \",\"\"])\n",
    "\n",
    "chunks_1 = splitter_1.split_documents(pdf_document)\n",
    "chunks_2 = splitter_2.split_documents(pdf_document)\n",
    "\n",
    "def display_document_stats(docs, name):\n",
    "    \"\"\"Display statistics about a list of document chunks\"\"\"\n",
    "    total_chunks = len(docs)\n",
    "    total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "    avg_chunk_size = total_chars / total_chunks if total_chunks > 0 else 0\n",
    "    \n",
    "    # Count unique metadata keys across all documents\n",
    "    all_metadata_keys = set()\n",
    "    for doc in docs:\n",
    "        all_metadata_keys.update(doc.metadata.keys())\n",
    "    \n",
    "    # Print the statistics\n",
    "    print(f\"\\n=== {name} Statistics ===\")\n",
    "    print(f\"Total number of chunks: {total_chunks}\")\n",
    "    print(f\"Average chunk size: {avg_chunk_size:.2f} characters\")\n",
    "    print(f\"Metadata keys preserved: {', '.join(all_metadata_keys)}\")\n",
    "    \n",
    "    if docs:\n",
    "        print(\"\\nExample chunk:\")\n",
    "        example_doc = docs[min(5, total_chunks-1)]  # Get the 5th chunk or the last one if fewer\n",
    "        print(f\"Content (first 150 chars): {example_doc.page_content[:150]}...\")\n",
    "        print(f\"Metadata: {example_doc.metadata}\")\n",
    "        \n",
    "        # Calculate length distribution\n",
    "        lengths = [len(doc.page_content) for doc in docs]\n",
    "        min_len = min(lengths)\n",
    "        max_len = max(lengths)\n",
    "        print(f\"Min chunk size: {min_len} characters\")\n",
    "        print(f\"Max chunk size: {max_len} characters\")\n",
    "\n",
    "# Display stats for both chunk sets\n",
    "display_document_stats(chunks_1, \"Splitter 1\")\n",
    "display_document_stats(chunks_2, \"Splitter 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b95a8",
   "metadata": {},
   "source": [
    "### Embedding models\n",
    "Embedding models are specifically designed to interface with text embeddings.\n",
    "\n",
    "Embeddings generate a vector representation for a specified piece or \"chunk\" of text.  Embeddings offer the advantage of allowing you to conceptualize text within a vector space. Consequently, you can perform operations such as semantic search, where you identify pieces of text that are most similar within the vector space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c45647ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 57\n",
      "ChromaDB created\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "paper_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\"\n",
    "pdf_loader = PyPDFLoader(paper_url)\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(\"Chunks:\",len(chunks))\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=chunks,\n",
    "                                     embedding=embedding_model,\n",
    "                                     persist_directory=\"./chroma_db\")\n",
    "print(\"ChromaDB created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f80cf430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:1\n",
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether you\n",
      "\n",
      "Result:2\n",
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether you\n",
      "\n",
      "Result:3\n",
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether you\n"
     ]
    }
   ],
   "source": [
    "query = \"What is LangChain used for?\"\n",
    "\n",
    "docs = vector_store.similarity_search(query,k=3)\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "    print(f\"\\nResult:{i + 1}\")\n",
    "    print(doc.page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70735f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "embeddings = embedding_model.embed_documents(texts)\n",
    "\n",
    "print(len(embeddings))        # number of chunks\n",
    "print(len(embeddings[0]))     # embedding dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6a048",
   "metadata": {},
   "source": [
    "### Retriever \n",
    "\n",
    "A retriever is an interface that returns documents using an unstructured query. Retrievers are more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. You can still use vector stores as the backbone of a retriever. Note that other types of retrievers also exist.\n",
    "\n",
    "Retrievers accept a string `query` as input and return a list of `Documents` as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48c32098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'title': 's8329 final', 'page_label': '1', 'total_pages': 6, 'creationdate': '2023-12-31T03:50:13+00:00', 'producer': 'PyPDF', 'moddate': '2023-12-31T03:52:06+00:00', 'author': 'IEEE', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'page': 0, 'creator': 'Microsoft Word'}, page_content='and human. The conclusion is drawn in Section V. \\nII. LANGCHAIN \\nLangChain, with its open -source essence, emerges as a \\npromising solution, aiming to simplify the complex process of \\ndeveloping applications powered by large language models \\n(LLMs). This framework though the rapid delivery of building \\nblocks and pre-built chains for building large language model \\napplications shows the easy way developers can do it.')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the vector store as a retriever\n",
    "# This converts the vector store into a retriever interface that can fetch relevant documents\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "\n",
    "# Invoke the retriever with the query \"Langchain\"\n",
    "# This will:\n",
    "# 1. Convert the query text \"Langchain\" into an embedding vector\n",
    "# 2. Perform a similarity search in the vector store using this embedding\n",
    "# 3. Return the most semantically similar documents to the query\n",
    "\n",
    "docs = retriever.invoke(\"LangChain\")\n",
    "docs[0]\n",
    "# Access the first (most relevant) document from the retrieval results\n",
    "# This returns the full Document object including:\n",
    "# - page_content: The text content of the document\n",
    "# - metadata: Any associated metadata like source, page numbers, etc.\n",
    "# The returned document is the one most semantically similar to \"Langchain\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d8628",
   "metadata": {},
   "source": [
    "### Parent document retrievers\n",
    "When splitting documents for retrieval, there are often conflicting goals:\n",
    "\n",
    "- You want small documents so their embeddings can most accurately reflect their meaning. If the documents are too long, then the embeddings can lose meaning.\n",
    "- You want to have long enough documents to retain the context of each chunk of text.\n",
    "\n",
    "The `ParentDocumentRetriever` strikes that balance by splitting and storing small chunks of data. During retrieval, this retriever first fetches the small chunks, but then looks up the parent IDs for the data and returns those larger documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7241e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers import ParentDocumentRetriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.storage import InMemoryStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "parent_splitter = CharacterTextSplitter(chunk_size=2000,chunk_overlap=20,separator='\\n')\n",
    "child_splitter = CharacterTextSplitter(chunk_size=400,chunk_overlap=20,separator=\"\\n\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma(collection_name=\"split_parents\",embedding_function=embedding_model)\n",
    "\n",
    "# Set up an in-memory storage layer for the parent documents\n",
    "# This will store the larger chunks that provide context, but won't be directly embedded\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0629984",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    # The vector store where child document embeddings will be stored and searched\n",
    "    # This Chroma instance will contain the embeddings for the smaller chunks\n",
    "    vectorstore=vector_store,\n",
    "    # The document store where parent documents will be stored\n",
    "    # These larger chunks won't be embedded but will be retrieved by ID when needed\n",
    "    docstore=store,\n",
    "    # The splitter used to create small chunks (400 chars) for precise vector search\n",
    "    # These smaller chunks are embedded and used for similarity matching\n",
    "    child_splitter=child_splitter,\n",
    "    # The splitter used to create larger chunks (2000 chars) for better context\n",
    "    # These parent chunks provide more complete information when retrieved\n",
    "    parent_splitter=parent_splitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d995e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef28631e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c82f86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vector_store.similarity_search(\"Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f886ee97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether your desire is to unlock deeper natural \n",
      "language understanding , enhance data, or circumvent \n",
      "language barriers through translation, LangChain is ready to\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6831972",
   "metadata": {},
   "source": [
    "### Standard RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f496ca35",
   "metadata": {},
   "source": [
    "Note: The approach given in the original notebook deviates because the langchain code provided was deprecated. So this is my way of solving rag with the latest api of langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c522e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7a2d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
    ")\n",
    "\n",
    "splits = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe9c6ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma \n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = Chroma.from_documents(documents=splits,\n",
    "                                     embedding=embeddings,\n",
    "                                     collection_name=\"pdf_rag\")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8faa469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "487eb914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant.\n",
    "    Answer the question ONLY using the context below.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# itemgetter extracts values from dictionary,list,tuples\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever | format_docs,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a06b150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the chat memory \n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chat_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "371a8ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper examines the strong link between mental health problems and suicide risk, noting that nearly a million people worldwide die by suicide each year, especially young people. It highlights the importance of early identification of suicidal ideation and reviews recent advances in deep‑learning approaches that use natural language processing—particularly contextualized pre‑trained language models—to detect such risk factors.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke(\n",
    "    {\"question\": \"What is this paper about?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5ca55",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "#### **Building a Simple Retrieval System with LangChain**\n",
    "\n",
    "In this exercise, you'll implement a simple retrieval system using LangChain's vector store and retriever components to help answer questions based on a document.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the necessary components for document loading, embedding, and retrieval.\n",
    "2. Load the provided document about artificial intelligence.\n",
    "3. Split the document into manageable chunks.\n",
    "4. Use an embedding model to create vector representations.\n",
    "5. Create a vector store and a retriever.\n",
    "6. Implement a simple question-answering system.\n",
    "7. Test your system with at least 3 different questions.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78823a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is LangChain?\n",
      "A: LangChain is an open‑source framework that builds on LangGraph to give you durable execution, streaming, human‑in‑the‑loop capabilities, persistence, and more. It supplies a pre‑built agent architecture and model integrations so you can quickly and seamlessly incorporate large language models into your agents and applications.\n",
      "--------------------------------------------------\n",
      "Q: How do retrievers work?\n",
      "A: The provided context does not contain any information about how retrievers work.\n",
      "--------------------------------------------------\n",
      "Q: Why is document splitting important?\n",
      "A: The context you provided does not mention document splitting, so there isn’t any information in it to explain why document splitting would be important.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200,\n",
    "                                               chunk_overlap=20,\n",
    "                                               separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=chunks,\n",
    "                                     embedding=embedding_model,\n",
    "                                     collection_name=\"simple-rag\")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant.\n",
    "    Answer ONLY using the context below.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question:\n",
    "    {question}\"\"\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\":itemgetter(\"question\") | retriever | format_docs,\n",
    "        \"question\":itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "test_queries = [\n",
    "    \"What is LangChain?\",\n",
    "    \"How do retrievers work?\",\n",
    "    \"Why is document splitting important?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", rag_chain.invoke({\"question\": q}))\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1592e8",
   "metadata": {},
   "source": [
    "### Memory \n",
    "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At a bare minimum, a conversational system should be able to directly access some window of past messages.\n",
    "\n",
    "### Chat message history\n",
    "One of the core utility classes underpinning most (if not all) memory modules is the `ChatMessageHistory` class. This class is a super lightweight wrapper that provides convenience methods for saving `HumanMessages` and `AIMessages`, and then fetching both types of messages.\n",
    "\n",
    "Here is an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53dfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='hi!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={})]\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "history.add_ai_message(\"hi!\")\n",
    "\n",
    "history.add_user_message(\"What is the capital of France?\")\n",
    "\n",
    "print(history.messages)\n",
    "\n",
    "response = llm.invoke(history.messages)\n",
    "print(response.content)\n",
    "\n",
    "history.add_ai_message(response.content)\n",
    "# print(history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68e79c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='hi!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n"
     ]
    }
   ],
   "source": [
    "print(history.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed34355",
   "metadata": {},
   "source": [
    "### Conversation buffer \n",
    "\n",
    "1. Prompt with history\n",
    "2. Create a chain\n",
    "3. Create a memory store \n",
    "4. Wrap with memory\n",
    "5. Run conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf3b9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt with history\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\",\"{input}\")\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a289a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain \n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666da28",
   "metadata": {},
   "source": [
    "- `InMemoryChatMessageHistory` stores chat history in RAM\n",
    "- `RunnableWithMessageHistory` wraps chain/agent this automatically adds chat history to every request \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a5fd48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a memory store \n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87928dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap with memory \n",
    "\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "chat_chain = RunnableWithMessageHistory(chain,\n",
    "                                        get_session_history,\n",
    "                                        input_messages_key=\"input\",\n",
    "                                        history_messages_key=\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3a3c661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You’re a tiger—majestic, powerful, and one of nature’s most iconic big cats.  \\nIf you’re speaking figuratively or want to share more about yourself, I’m all ears!', additional_kwargs={'reasoning_content': 'We need to respond to \"Who am I?\" The user previously said \"Hello, I am a tiger. Who are you?\" The assistant responded \"I am ChatGPT.\" Now the user asks \"Who am I?\" We should respond that the user is a tiger, as they said. But we should be mindful of the conversation context: they said \"Hello, I am a tiger. Who are you?\" So we should confirm that they are a tiger. But maybe we can ask clarifying question: \"You said you are a tiger; is that a literal tiger or a metaphor?\" But the user might expect a playful answer. We could say: \"You\\'re a tiger—majestic, powerful, and a big cat. But if there\\'s more you\\'d like to share, feel free!\" Let\\'s respond accordingly.'}, response_metadata={'token_usage': {'completion_tokens': 212, 'prompt_tokens': 527, 'total_tokens': 739, 'completion_time': 0.237730992, 'completion_tokens_details': {'reasoning_tokens': 163}, 'prompt_time': 0.034912046, 'prompt_tokens_details': None, 'queue_time': 0.043404174, 'total_time': 0.272643038}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_deb540145b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c9e71-02a7-7442-a38d-f88e0ed8212a-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 527, 'output_tokens': 212, 'total_tokens': 739, 'output_token_details': {'reasoning': 163}})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation \n",
    "chat_chain.invoke(\n",
    "    {\"input\": \"Hello, I am a tiger. Who are you?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}}\n",
    ")\n",
    "\n",
    "chat_chain.invoke(\n",
    "    {\"input\": \"What can you do?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}}\n",
    ")\n",
    "\n",
    "chat_chain.invoke(\n",
    "    {\"input\": \"Who am I?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da322b0",
   "metadata": {},
   "source": [
    "#### **Building a Chatbot with Memory using LangChain**\n",
    "\n",
    "In this exercise, you'll create a simple chatbot that can remember previous interactions using LangChain's memory components. You'll implement conversation memory to make your chatbot maintain context throughout a conversation.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the necessary components for chat history and conversation memory.\n",
    "2. Set up a language model for your chatbot.\n",
    "3. Create a conversation chain with memory capabilities.\n",
    "4. Implement a simple interactive chat interface.\n",
    "5. Test the memory capabilities with a series of related questions.\n",
    "6. Examine how the conversation history is stored and accessed.\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4dbf0f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Hello, my name is Alice.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello, Alice.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\",\n",
    "               max_tokens=256,\n",
    "               temperature=0.2)\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "history.add_user_message(\"Hello, my name is Alice.\")\n",
    "history.add_ai_message(\"Hello, Alice.\")\n",
    "\n",
    "print(history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14c49f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Beginning Chat Simulation ===\n",
      "\n",
      "--- Turn 1 ---\n",
      "Human: My favorite color is blue.\n",
      "AI: That’s a classic choice! Blue is often associated with calmness, trust, and creativity. Do you have a particular shade of blue you love most?\n",
      "\n",
      "--- Turn 2 ---\n",
      "Human: I enjoy hiking in the mountains.\n",
      "AI: That sounds amazing! The mountains have such a unique mix of quiet moments and breathtaking views. Do you have a favorite trail or a particular mountain range you love to explore? If you’re looking for new spots or tips on gear, I’d be happy to help!\n",
      "\n",
      "--- Turn 3 ---\n",
      "Human: What activities would you recommend for me?\n",
      "AI: Here are a few activities that blend your love of the outdoors, the mountains, and the calming, inspiring hue of blue:\n",
      "\n",
      "| Activity | Why it fits | Quick Tips |\n",
      "|----------|-------------|------------|\n",
      "| **Mountain sunrise or sunset photography** | The sky often turns brilliant shades of blue (or deep indigo) at dawn and dusk. | Bring a tripod, shoot in RAW, and experiment with long exposures. |\n",
      "| **Blue‑themed nature walks** | Pick a trail that features blue‑tinged elements—think alpine lakes, blue‑green lichens, or even blue‑colored flowers. | Use a field guide or a plant‑identification app to spot the “blue” flora. |\n",
      "| **Rock climbing or bouldering** | Many climbing areas have striking blue rock faces or blue‑tinged cliffs. | Start with a local guide or a\n",
      "\n",
      "--- Turn 4 ---\n",
      "Human: What was my favorite color again?\n",
      "AI: Your favorite color is blue.\n",
      "\n",
      "--- Turn 5 ---\n",
      "Human: Can you remember both my name and my favorite color?\n",
      "AI: I don’t have a record of your name—could you let me know what you’d like me to call you?  \n",
      "Your favorite color, as you mentioned earlier, is blue.\n",
      "\n",
      "=== End of Chat Simulation ===\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\",\"{input}\")\n",
    "]) \n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chat_chain = RunnableWithMessageHistory(chain,\n",
    "                                        get_session_history,\n",
    "                                        input_messages_key=\"input\",\n",
    "                                        history_messages_key=\"history\")\n",
    "\n",
    "def chat_simulation(conversation, inputs, session_id=\"1\"):\n",
    "    print(\"\\n=== Beginning Chat Simulation ===\")\n",
    "    \n",
    "    for i, user_input in enumerate(inputs):\n",
    "        print(f\"\\n--- Turn {i+1} ---\")\n",
    "        print(f\"Human: {user_input}\")\n",
    "        \n",
    "        response = conversation.invoke(\n",
    "            {\"input\": user_input},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        \n",
    "        print(f\"AI: {response}\")\n",
    "    \n",
    "    print(\"\\n=== End of Chat Simulation ===\")\n",
    "\n",
    "# 6. Test with a series of related questions\n",
    "test_inputs = [\n",
    "    \"My favorite color is blue.\",\n",
    "    \"I enjoy hiking in the mountains.\",\n",
    "    \"What activities would you recommend for me?\",\n",
    "    \"What was my favorite color again?\",\n",
    "    \"Can you remember both my name and my favorite color?\"\n",
    "]\n",
    "\n",
    "chat_simulation(chat_chain, test_inputs,session_id=\"1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
