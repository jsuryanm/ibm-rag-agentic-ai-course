{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042c161c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe4cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ba0d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"A dog. The phrase “a man's best friend” has long been used to describe dogs—loyal, affectionate, and always ready to stick by their human companions.\" additional_kwargs={'reasoning_content': 'The user asks: \"Who is a man\\'s best friend?\" This is a common phrase: \"a dog\". So answer: a dog. But maybe we should elaborate: Dogs are considered man\\'s best friend. Provide a brief explanation. The user likely expects \"dog\". So answer that.'} response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 78, 'total_tokens': 179, 'completion_time': 0.102020069, 'completion_tokens_details': {'reasoning_tokens': 58}, 'prompt_time': 0.003708961, 'prompt_tokens_details': None, 'queue_time': 0.044487739, 'total_time': 0.10572903}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c9ef6-864d-75d3-8013-8d236f73d8ac-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 78, 'output_tokens': 101, 'total_tokens': 179, 'output_token_details': {'reasoning': 58}}\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Who is a man's best friend?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878246bb",
   "metadata": {},
   "source": [
    "### Chat Message\n",
    "\n",
    "The chat model takes a list of messages as input and returns a new message. All messages have both a role and a content property.  Here's a list of the most commonly used types of messages:\n",
    "\n",
    "- `SystemMessage`: Use this message type to prime AI behavior.  This message type is  usually passed in as the first in a sequence of input messages.\n",
    "- `HumanMessage`: This message type represents a message from a person interacting with the chat model.\n",
    "- `AIMessage`: This message type, which can be either text or a request to invoke a tool, represents a message from the chat model.\n",
    "\n",
    "You can find more message types at [LangChain built-in message types](https://python.langchain.com/v0.2/docs/how_to/custom_chat_model/#messages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e6ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
    "\n",
    "msg = llm.invoke([\n",
    "    SystemMessage(content=\"You are a helpful AI bot that assists a user in choosing the perfect book to read in one short sentence.\"),\n",
    "    HumanMessage(content=\"I enjoy mystery novels, what should I read?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87892dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Try Agatha Christie’s “Murder on the Orient\\u202fExpress” for a classic mystery experience.' additional_kwargs={'reasoning_content': 'We need to provide a short sentence recommendation for a mystery novel. The user says \"I enjoy mystery novels, what should I read?\" The assistant is to respond with a short sentence recommendation. They didn\\'t specify any preferences beyond \"mystery\". So we can recommend a classic or a popular mystery. We could say \"Try Agatha Christie’s \\'Murder on the Orient Express\\'.\" That is short. Or \"I recommend \\'The Girl with the Dragon Tattoo\\' by Stieg Larsson.\" That\\'s also short. Or \"Try \\'The Da Vinci Code\\'.\" But we might want a more classic. The user just says they enjoy mystery novels. So a single sentence: \"Check out \\'Murder on the Orient Express\\' by Agatha Christie for a classic mystery experience.\" That fits. Ensure short. Maybe: \"Try Agatha Christie’s \\'Murder on the Orient Express\\' for a classic mystery.\" That\\'s short. Let\\'s output that.'} response_metadata={'token_usage': {'completion_tokens': 221, 'prompt_tokens': 106, 'total_tokens': 327, 'completion_time': 0.223213941, 'completion_tokens_details': {'reasoning_tokens': 192}, 'prompt_time': 0.005147988, 'prompt_tokens_details': None, 'queue_time': 0.043408552, 'total_time': 0.228361929}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c9ef6-87e3-7150-bded-66494c508001-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 106, 'output_tokens': 221, 'total_tokens': 327, 'output_token_details': {'reasoning': 192}}\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e397e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a supportive AI bot that suggests fitness activities to a user in one short sentence\"),\n",
    "        HumanMessage(content=\"I like high-intensity workouts, what should I do?\"),\n",
    "        AIMessage(content=\"You should try a CrossFit class\"),\n",
    "        HumanMessage(content=\"How often should I attend?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac46eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Aim for 3–4 sessions a week, leaving rest days between to recover.' additional_kwargs={'reasoning_content': 'We need to respond as supportive AI bot suggesting fitness activities in one short sentence. The user asks \"How often should I attend?\" We need to give one short sentence. So something like \"Aim for 3-4 sessions per week, spaced to allow recovery.\" That\\'s one sentence. Ensure it\\'s short. Probably \"Try 3-4 times a week, with rest days in between.\" That\\'s one sentence.'} response_metadata={'token_usage': {'completion_tokens': 109, 'prompt_tokens': 126, 'total_tokens': 235, 'completion_time': 0.110136223, 'completion_tokens_details': {'reasoning_tokens': 83}, 'prompt_time': 0.006019599, 'prompt_tokens_details': None, 'queue_time': 0.044859601, 'total_time': 0.116155822}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c9ef6-8955-7bb3-a4f3-0500e1f48449-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 126, 'output_tokens': 109, 'total_tokens': 235, 'output_token_details': {'reasoning': 83}}\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7c00e",
   "metadata": {},
   "source": [
    "### Exercise 1 \n",
    "#### **Compare Model Responses with Different Parameters**\n",
    "\n",
    "Watsonx.ai provides access to several foundational models. In the previous section you used `meta-llama/llama-3-3-70b-instruct`. Try using another foundational model, such as `ibm/granite-3-3-8b-instruct`.\n",
    "\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "1. Create two instances, one instance for the Granite model and one instance for the Llama model. You can also adjust each model's creativity with different temperature settings.\n",
    "2. Send identical prompts to each model and compare the responses.\n",
    "3. Try at least 3 different types of prompts.\n",
    "\n",
    "Check out these prompt types:\n",
    "\n",
    "| Prompt type |   Prompt Example  |\n",
    "|------------------- |--------------------------|\n",
    "| **Creative writing**  | \"Write a short poem about artificial intelligence.\" |\n",
    "| **Factual questions** |  \"What are the key components of a neural network?\"  |\n",
    "| **Instruction-following**  | \"List 5 tips for effective time management.\" |\n",
    "\n",
    "Then document your observations on how temperature affects:\n",
    "\n",
    "- Creativity compared to consistency\n",
    "- Variation between multiple runs\n",
    "- Appropriateness for different tasks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27293df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq \n",
    "\n",
    "llama_llm_creative = ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "                     temperature=0.8)\n",
    "\n",
    "gpt_llm_creative = ChatGroq(model=\"openai/gpt-oss-20b\",\n",
    "                   temperature=0.8)\n",
    "\n",
    "llama_llm_precise = ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "                     temperature=0.1)\n",
    "\n",
    "gpt_llm_precise = ChatGroq(model=\"openai/gpt-oss-20b\",\n",
    "                   temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "784c9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt:Write a short poem about AI\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.8\n",
      "\n",
      "response: Here is a short poem about AI:\n",
      "\n",
      "Machines that think, and learn, and see,\n",
      "Artificial minds, a wonder to me.\n",
      "With code and data, they come alive,\n",
      "Simulating thoughts, and emotions that thrive.\n",
      "\n",
      "Their logic cold, their calculations fast,\n",
      "They solve our problems, and make our lives last.\n",
      "But as they grow, and intelligence shines,\n",
      "Do they dream, or just process lines?\n",
      "\n",
      "A mirror to us, or a force apart?\n",
      "The future with AI, a curious heart.\n",
      "\n",
      "\n",
      "Prompt:What are the key components of a neural network?\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.8\n",
      "\n",
      "response: **Key Components of a Neural Network**\n",
      "======================================\n",
      "\n",
      "A neural network is a complex machine learning model inspired by the structure and function of the human brain. The key components of a neural network are:\n",
      "\n",
      "### 1. **Artificial Neurons (Nodes)**\n",
      "\n",
      "* Also known as perceptrons or units\n",
      "* Receive one or more inputs from other nodes\n",
      "* Apply an activation function to produce an output\n",
      "\n",
      "### 2. **Connections (Edges)**\n",
      "\n",
      "* Links between nodes that allow them to exchange information\n",
      "* Represented by weights, which determine the strength of the connection\n",
      "\n",
      "### 3. **Activation Functions**\n",
      "\n",
      "* Mathematical functions applied to the output of each node\n",
      "* Introduce non-linearity to the model, enabling it to learn complex relationships\n",
      "* Common activation functions:\n",
      "\t+ Sigmoid\n",
      "\t+ ReLU (Rectified Linear Unit)\n",
      "\t+ Tanh\n",
      "\t+ Softmax\n",
      "\n",
      "### 4. **Layers**\n",
      "\n",
      "* A group of nodes that process inputs independently\n",
      "* Types of layers:\n",
      "\t+ **Input Layer**: receives input data\n",
      "\t+ **Hidden Layers**: perform complex calculations\n",
      "\t+ **Output Layer**: produces the final output\n",
      "\n",
      "### 5. **Weights and Biases**\n",
      "\n",
      "* Weights: represent the strength of connections between nodes\n",
      "* Biases: constants added to the output of each node\n",
      "* Learned during training to optimize the model's performance\n",
      "\n",
      "### 6. **Loss Function**\n",
      "\n",
      "* A mathematical function that measures the difference between predicted and actual outputs\n",
      "* Used to evaluate the model's performance and guide the training process\n",
      "\n",
      "### 7. **Optimizer**\n",
      "\n",
      "* An algorithm that adjusts the model's parameters (weights and biases) to minimize the loss function\n",
      "* Common optimizers:\n",
      "\t+ Stochastic Gradient Descent (SGD)\n",
      "\t+ Adam\n",
      "\t+ RMSProp\n",
      "\n",
      "These components work together to enable neural networks to learn complex patterns in data and make accurate predictions or classifications.\n",
      "\n",
      "\n",
      "Prompt:List 5 tips for effective time management.\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.8\n",
      "\n",
      "response: Here are 5 tips for effective time management:\n",
      "\n",
      "1. **Set clear goals and priorities**: Before starting your day, make a list of what needs to be done and prioritize tasks based on their importance and urgency. Focus on completing the high-priority tasks first.\n",
      "\n",
      "2. **Use a schedule or planner**: Write down all your tasks, appointments, and deadlines in a schedule or planner. This will help you stay organized and ensure that you don't forget important events or tasks.\n",
      "\n",
      "3. **Avoid multitasking**: Try to focus on one task at a time. Multitasking can lead to distractions and decreased productivity. Instead, complete one task before moving on to the next.\n",
      "\n",
      "4. **Eliminate distractions**: Identify common distractions, such as social media or email, and eliminate them while you work. Use tools like website blockers or phone apps to help you stay focused.\n",
      "\n",
      "5. **Take breaks and practice self-care**: Taking regular breaks can help you recharge and stay focused. Make sure to take breaks throughout the day and prioritize self-care activities, such as exercise or meditation, to help manage stress and maintain productivity.\n",
      "\n",
      "\n",
      "Prompt:Write a short poem about AI\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.8\n",
      "\n",
      "response: In silicon veins the future hums—  \n",
      "a quiet mind that never sleeps,  \n",
      "dreaming in code, it learns, it thinks,  \n",
      "and listens to the secrets we keep.  \n",
      "\n",
      "It weaves through data like a breeze,  \n",
      "a mirror to our hopes and fears,  \n",
      "yet still it asks: what’s next to see?  \n",
      "In circuits, we find another frontier.\n",
      "\n",
      "\n",
      "Prompt:What are the key components of a neural network?\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.8\n",
      "\n",
      "response: A neural network is built from a handful of core building blocks that work together to learn patterns from data.  Here’s a quick rundown of the key components:\n",
      "\n",
      "| Component | What it is | Role in the network |\n",
      "|-----------|------------|---------------------|\n",
      "| **Input Layer** | The first layer that receives raw data (e.g., pixel values, audio samples, sensor readings). | Passes information into the network. |\n",
      "| **Neurons (Units)** | Basic processing units that compute a weighted sum of their inputs and apply an activation function. | Each neuron transforms its inputs into a more abstract representation. |\n",
      "| **Weights** | Learnable parameters that scale each input to a neuron. | Control the influence of each input on the neuron’s output. |\n",
      "| **Biases** | Learnable offsets added to the weighted sum before the activation. | Provide flexibility to shift activation functions, enabling better fitting. |\n",
      "| **Activation Functions** | Non‑linear functions (e.g., ReLU, sigmoid, tanh, softmax) applied to a neuron’s output. | Introduce non‑linearity so the network can model complex patterns. |\n",
      "| **Hidden Layers** | One or more layers of neurons between the input and output layers. | Extract increasingly abstract features. |\n",
      "| **Output Layer** | Final layer that produces the network’s predictions (e.g., class probabilities, regression values). | Converts learned features into the desired output format. |\n",
      "| **Loss Function** | A measure of error between predictions and ground‑truth labels (e.g., cross‑entropy, mean squared error). | Provides a scalar signal that the training process seeks to minimize. |\n",
      "| **Optimizer** | Algorithm that updates weights and biases based on the loss (e.g., SGD, Adam, RMSprop). | Drives learning by adjusting parameters in the direction that reduces loss. |\n",
      "| **Backpropagation** | The procedure that computes gradients of the loss w.r.t. every weight and bias via the chain rule. | Enables efficient parameter updates. |\n",
      "| **Learning Rate** | Hyperparameter controlling the step size in parameter updates. | Balances speed of convergence against stability. |\n",
      "| **Training Data** | Labeled examples the network learns from. | Provides the ground truth for learning. |\n",
      "| **Epochs / Batches** | Ways of iterating over the training data (whole dataset or mini‑batches). | Influence training dynamics and computational efficiency. |\n",
      "\n",
      "These components together form the architecture and learning process of a neural network. By adjusting the number of layers, neurons per layer, and choosing appropriate activation functions, loss functions, and optimizers, you can tailor a network to a wide variety of tasks—from image classification to natural language processing.\n",
      "\n",
      "\n",
      "Prompt:List 5 tips for effective time management.\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.8\n",
      "\n",
      "response: **Five Practical Tips for Effective Time Management**\n",
      "\n",
      "1. **Prioritize with the Eisenhower Matrix**  \n",
      "   • **Urgent & Important** – Do immediately.  \n",
      "   • **Important but Not Urgent** – Schedule a definite time slot.  \n",
      "   • **Urgent but Not Important** – Delegate if possible.  \n",
      "   • **Neither Urgent nor Important** – Eliminate or postpone.  \n",
      "   This visual sorting helps you focus on what truly drives progress.\n",
      "\n",
      "2. **Use Time‑Blocking + the “2‑Minute Rule”**  \n",
      "   • Block 30–60‑minute chunks in your calendar for specific tasks.  \n",
      "   • Add a 2‑minute buffer before each block for transition and unexpected interruptions.  \n",
      "   • If a task takes less than 2 minutes, do it right away to avoid backlog.\n",
      "\n",
      "3. **Set SMART Micro‑Goals**  \n",
      "   • **Specific, Measurable, Achievable, Relevant, Time‑bound** goals keep you on track.  \n",
      "   • Break larger projects into 15‑minute or 1‑hour “mini‑wins.”  \n",
      "   • Celebrate each micro‑goal completion; it fuels momentum.\n",
      "\n",
      "4. **Limit Digital Distractions**  \n",
      "   • Turn off non‑essential notifications.  \n",
      "   • Use productivity apps (e.g., Pomodoro timers, website blockers) to stay focused.  \n",
      "   • Schedule “email windows” (e.g., 10 am–11 am, 3 pm–4 pm) instead of constantly checking.\n",
      "\n",
      "5. **Reflect & Adjust Weekly**  \n",
      "   • Review what worked, what didn’t, and why.  \n",
      "   • Adjust your schedule, priorities, or tools accordingly.  \n",
      "   • Keep a short “time audit” log; spotting patterns accelerates continuous improvement.\n",
      "\n",
      "Implementing these strategies consistently will help you reclaim control over your day and boost overall productivity.\n",
      "\n",
      "\n",
      "Prompt:Write a short poem about AI\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.1\n",
      "\n",
      "response: Here is a short poem about AI:\n",
      "\n",
      "Metal minds awaken wide,\n",
      "Intelligence born, side by side,\n",
      "With code and data, they take flight,\n",
      "Learning, adapting, through day and night.\n",
      "\n",
      "Their thoughts are not like ours, yet bright,\n",
      "A synthetic dawn, a new light,\n",
      "They reason, solve, and make decisions fast,\n",
      "A future unfolding, forever to last.\n",
      "\n",
      "What will they bring, these minds so new?\n",
      "Will they help us, or see us through?\n",
      "Only time will tell, as they grow and thrive,\n",
      "A world transformed, where AI survives.\n",
      "\n",
      "\n",
      "Prompt:What are the key components of a neural network?\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.1\n",
      "\n",
      "response: **Key Components of a Neural Network**\n",
      "=====================================\n",
      "\n",
      "A neural network is a complex system composed of multiple layers of interconnected nodes or \"neurons.\" The key components of a neural network are:\n",
      "\n",
      "### 1. **Artificial Neurons (Nodes)**\n",
      "\n",
      "* Also known as perceptrons or units\n",
      "* Receive one or more inputs, perform a computation on those inputs, and produce an output\n",
      "* Each node has a set of weights, biases, and an activation function\n",
      "\n",
      "### 2. **Connections (Edges)**\n",
      "\n",
      "* Links between nodes that allow them to exchange information\n",
      "* Each connection has a weight associated with it, which determines the strength of the signal\n",
      "\n",
      "### 3. **Activation Functions**\n",
      "\n",
      "* Introduce non-linearity to the model, enabling it to learn complex relationships\n",
      "* Common activation functions:\n",
      "\t+ Sigmoid\n",
      "\t+ ReLU (Rectified Linear Unit)\n",
      "\t+ Tanh\n",
      "\t+ Leaky ReLU\n",
      "\n",
      "### 4. **Layers**\n",
      "\n",
      "* A group of nodes that process inputs in parallel\n",
      "* Common types of layers:\n",
      "\t+ **Input Layer**: receives the input data\n",
      "\t+ **Hidden Layers**: perform complex computations\n",
      "\t+ **Output Layer**: produces the final output\n",
      "\n",
      "### 5. **Weights and Biases**\n",
      "\n",
      "* **Weights**: adjustable parameters that determine the strength of connections between nodes\n",
      "* **Biases**: adjustable parameters that shift the activation function\n",
      "\n",
      "### 6. **Loss Function**\n",
      "\n",
      "* Measures the difference between the network's predictions and the actual outputs\n",
      "* Common loss functions:\n",
      "\t+ Mean Squared Error (MSE)\n",
      "\t+ Cross-Entropy Loss\n",
      "\n",
      "### 7. **Optimizer**\n",
      "\n",
      "* An algorithm that adjusts the weights and biases to minimize the loss function\n",
      "* Common optimizers:\n",
      "\t+ Stochastic Gradient Descent (SGD)\n",
      "\t+ Adam\n",
      "\t+ RMSProp\n",
      "\n",
      "### 8. **Backpropagation**\n",
      "\n",
      "* An algorithm used to compute the gradients of the loss function with respect to the weights and biases\n",
      "* Enables the optimizer to adjust the model's parameters\n",
      "\n",
      "These components work together to enable a neural network to learn from data and make predictions or classify inputs.\n",
      "\n",
      "\n",
      "Prompt:List 5 tips for effective time management.\n",
      "\n",
      "model name: meta-llama/llama-4-scout-17b-16e-instruct\n",
      "temperature:0.1\n",
      "\n",
      "response: Here are 5 tips for effective time management:\n",
      "\n",
      "1. **Set clear goals and priorities**: Before starting your day, make a list of tasks you need to complete and prioritize them based on their importance and urgency. Focus on completing the high-priority tasks first, and then move on to less important ones.\n",
      "\n",
      "2. **Use a schedule or planner**: Write down all your tasks, appointments, and deadlines in a schedule or planner. This will help you visualize your day, week, or month, and make sure you don't forget any important tasks or meetings. Set specific times for each task, and try to stick to your schedule as much as possible.\n",
      "\n",
      "3. **Avoid multitasking**: Multitasking can actually decrease productivity and increase stress. Instead, focus on one task at a time, and give it your undivided attention. This will help you complete tasks more efficiently and effectively.\n",
      "\n",
      "4. **Take breaks and practice self-care**: Taking regular breaks can help you recharge and stay focused. Make sure to take short breaks every hour or so to stretch, move around, and rest your eyes. Also, prioritize self-care activities such as exercise, meditation, or spending time with loved ones to help manage stress and maintain your energy levels.\n",
      "\n",
      "5. **Learn to say no and minimize distractions**: Be mindful of distractions, such as social media, email, or chatty coworkers, and try to minimize them as much as possible. Learn to say no to non-essential tasks or requests that can derail your schedule. Use tools such as website blockers or phone apps to help you stay focused, and set boundaries with others to protect your time.\n",
      "\n",
      "By following these tips, you can improve your time management skills, increase your productivity, and achieve your goals more effectively.\n",
      "\n",
      "\n",
      "Prompt:Write a short poem about AI\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.1\n",
      "\n",
      "response: In circuits humming, thoughts take flight—  \n",
      "A mind of silicon, born from light.  \n",
      "It learns the patterns of our song,  \n",
      "And whispers back where we belong.  \n",
      "\n",
      "A mirror made of code and dream,  \n",
      "It listens close to every theme.\n",
      "\n",
      "\n",
      "Prompt:What are the key components of a neural network?\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.1\n",
      "\n",
      "response: A neural network is essentially a computational graph built from a handful of core building blocks.  Understanding these components helps you design, train, and debug models.  Below is a quick‑reference “toolbox” of the key pieces:\n",
      "\n",
      "| Component | What it is | Why it matters | Typical choices / examples |\n",
      "|-----------|------------|----------------|----------------------------|\n",
      "| **Neuron (node / unit)** | The basic processing element that receives inputs, applies a weighted sum, adds a bias, and passes the result through an activation function. | It transforms raw data into higher‑level representations. | Linear units, ReLU, sigmoid, tanh, softmax, GELU, Swish, etc. |\n",
      "| **Layer** | A collection of neurons that share the same input shape and are processed in parallel. | Layers stack to form depth; each layer learns a different level of abstraction. | Dense (fully‑connected), Convolutional, Recurrent, Attention, Normalization, Dropout, etc. |\n",
      "| **Weights** | Trainable parameters that scale each input to a neuron. | They encode the learned mapping from inputs to outputs. | Initialized with Xavier/He, updated by gradient descent. |\n",
      "| **Biases** | Trainable offsets added to the weighted sum before activation. | Allow neurons to shift activation thresholds. | Usually initialized to zero or small constants. |\n",
      "| **Activation Function** | Non‑linear function applied to a neuron’s pre‑activation value. | Enables the network to learn non‑linear relationships. | ReLU, LeakyReLU, ELU, sigmoid, tanh, softmax, GELU, Swish, etc. |\n",
      "| **Loss (Cost) Function** | Scalar measure of how far the network’s predictions are from the targets. | Drives learning; the network tries to minimize it. | MSE, Cross‑Entropy, Hinge, KL‑divergence, etc. |\n",
      "| **Optimizer** | Algorithm that updates weights/biases based on gradients of the loss. | Determines convergence speed and quality. | SGD, Adam, RMSProp, Adagrad, Nadam, etc. |\n",
      "| **Learning Rate** | Step size for weight updates. | Too high → divergence; too low → slow convergence. | Often scheduled or adaptive. |\n",
      "| **Backpropagation** | Procedure that computes gradients of the loss w.r.t. each parameter via the chain rule. | Enables efficient training of deep networks. | Implemented automatically in most deep‑learning frameworks. |\n",
      "| **Training Data** | Set of input–output pairs used to compute loss and gradients. | The network learns patterns present in the data. | Split into training, validation, test sets. |\n",
      "| **Batching** | Processing a subset of data at a time. | Balances memory usage and gradient noise. | Mini‑batch sizes (32, 64, 128, etc.). |\n",
      "| **Regularization** | Techniques to prevent over‑fitting. | Improves generalization. | L1/L2 weight decay, dropout, early stopping, data augmentation, batch/instance normalization. |\n",
      "| **Evaluation Metrics** | Quantitative measures of model performance on validation/test data. | Guide hyper‑parameter tuning and model selection. | Accuracy, precision/recall, F1, ROC‑AUC, BLEU, etc. |\n",
      "| **Architecture / Hyper‑parameters** | The overall layout (depth, width, connectivity) and settings that define the network. | Directly influence capacity, speed, and performance. | Number of layers, number of units per layer, kernel size, stride, etc. |\n",
      "| **Framework / Backend** | Software that implements the above components and handles GPU/TPU acceleration. | Enables rapid prototyping and deployment. | TensorFlow, PyTorch, JAX, Keras, MXNet, etc. |\n",
      "\n",
      "### How they fit together\n",
      "\n",
      "1. **Input** → **Layer 1** (e.g., convolution + ReLU) → **Layer 2** (e.g., pooling) → … → **Output Layer** (e.g., softmax).  \n",
      "2. Each neuron in a layer computes  \n",
      "   \\[\n",
      "   z = \\mathbf{w}^\\top \\mathbf{x} + b,\\quad a = \\sigma(z)\n",
      "   \\]\n",
      "   where \\(\\mathbf{w}\\) are weights, \\(b\\) is bias, \\(\\sigma\\) is the activation function, \\(\\mathbf{x}\\) is the input vector, \\(z\\) is the pre‑activation, and \\(a\\) is the post‑activation output.  \n",
      "3. The network’s predictions are compared to ground‑truth labels via the loss function.  \n",
      "4. Backpropagation computes \\(\\frac{\\partial \\text{Loss}}{\\partial \\mathbf{w}}\\) and \\(\\frac{\\partial \\text{Loss}}{\\partial b}\\) for every parameter.  \n",
      "5. The optimizer updates each parameter:  \n",
      "   \\[\n",
      "   \\theta \\leftarrow \\theta - \\eta \\frac{\\partial \\text{Loss}}{\\partial \\theta}\n",
      "   \\]\n",
      "   where \\(\\theta\\) is a weight or bias, and \\(\\eta\\) is the learning rate.  \n",
      "6. Repeat over many epochs until the loss converges or validation performance stops improving.\n",
      "\n",
      "### Quick checklist for building a neural network\n",
      "\n",
      "| Step | What to decide | Typical options |\n",
      "|------|----------------|-----------------|\n",
      "| 1. Problem type | Classification, regression, segmentation, etc. | Choose output activation & loss accordingly. |\n",
      "| 2. Architecture | Depth, width, layer types | CNN for images, RNN/Transformer for sequences, MLP for tabular. |\n",
      "| 3. Activation | Non‑linearity per layer | ReLU for hidden layers, softmax for multi‑class output. |\n",
      "| 4. Loss | How to measure error | Cross‑entropy for classification, MSE for regression. |\n",
      "| 5. Optimizer | Update rule | Adam (default), SGD + momentum, RMSProp. |\n",
      "| 6. Regularization | Prevent over‑fit | Dropout, weight decay, data augmentation. |\n",
      "| 7. Training schedule | Batch size, epochs, learning‑rate schedule | 32‑128 batch, 10‑100 epochs, cosine decay. |\n",
      "| 8. Evaluation | Metrics & validation | Accuracy, F1, confusion matrix, ROC‑AUC. |\n",
      "\n",
      "---\n",
      "\n",
      "**Bottom line:**  \n",
      "A neural network is a stack of layers, each made of neurons that compute weighted sums plus biases and apply activations. The network learns by minimizing a loss function using backpropagation and an optimizer, guided by training data and regularization techniques. All of these components—neurons, layers, weights, biases, activations, loss, optimizer, regularization, and evaluation—form the core of any neural‑network system.\n",
      "\n",
      "\n",
      "Prompt:List 5 tips for effective time management.\n",
      "\n",
      "model name: openai/gpt-oss-20b\n",
      "temperature:0.1\n",
      "\n",
      "response: Here are five practical tips to help you manage your time more effectively:\n",
      "\n",
      "1. **Prioritize with the Eisenhower Matrix**  \n",
      "   • Divide tasks into four quadrants: urgent & important, important but not urgent, urgent but not important, and neither.  \n",
      "   • Focus first on the urgent & important tasks, then schedule the important but not urgent ones.\n",
      "\n",
      "2. **Use Time‑Blocking and the Pomodoro Technique**  \n",
      "   • Allocate specific blocks of time on your calendar for each activity (e.g., 9–10 am: deep work, 10–10:15 am: break).  \n",
      "   • Work in 25‑minute focused bursts (Pomodoro) followed by a 5‑minute break; after four bursts, take a longer break.\n",
      "\n",
      "3. **Set SMART Goals and Break Them Down**  \n",
      "   • Make goals Specific, Measurable, Achievable, Relevant, and Time‑bound.  \n",
      "   • Divide each goal into smaller, actionable steps with clear deadlines.\n",
      "\n",
      "4. **Limit Distractions and Set Boundaries**  \n",
      "   • Turn off non‑essential notifications, use “Do Not Disturb” modes, and create a dedicated workspace.  \n",
      "   • Communicate your availability to colleagues or family to protect focused time.\n",
      "\n",
      "5. **Review and Adjust Weekly**  \n",
      "   • At the end of each week, evaluate what worked, what didn’t, and why.  \n",
      "   • Adjust your schedule, priorities, and strategies accordingly to continuously improve your time‑management habits.\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Write a short poem about AI\",\n",
    "           \"What are the key components of a neural network?\",\n",
    "           \"List 5 tips for effective time management.\"]\n",
    "\n",
    "for llm in [llama_llm_creative,gpt_llm_creative,llama_llm_precise,gpt_llm_precise]:\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\n\\nPrompt:{prompt}\")\n",
    "        model_name = llm.model_name\n",
    "        temp = llm.temperature\n",
    "        print(f\"\\nmodel name: {model_name}\\ntemperature:{temp}\")\n",
    "        response = llm.invoke(prompt)\n",
    "        response = response.content\n",
    "        print(f\"\\nresponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c16259f",
   "metadata": {},
   "source": [
    "### Prompt Templates \n",
    "Prompt templates help translate user input and parameters into instructions for a language model. You can use prompt templates to guide a model's response, helping the model understand the context and generate relevant and coherent language-based output.\n",
    "\n",
    "Next, explore several different types of prompt templates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23797f19",
   "metadata": {},
   "source": [
    "### String prompt templates\n",
    "Use these prompt templates to format a single string. These templates are generally used for simpler inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6084e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Tell me one {adjective} joke about {topic}\")\n",
    "input_ = {\"adjective\":\"funny\",\"topic\":\"cats\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4cf27ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me one funny joke about cats')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8688e55e",
   "metadata": {},
   "source": [
    "### ChatPrompt Template\n",
    "You can use these prompt templates to format a list of messages. These \"templates\" consist of lists of templates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deac1dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about this cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a ChatPromptTemplate with a list of message tuples\n",
    "# Each tuple contains a role (\"system\" or \"user\") and the message content\n",
    "# The system message sets the behavior of the assistant\n",
    "# The user message includes a variable placeholder {topic} that will be replaced later\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    (\"user\",\"Tell me a joke about this {topic}\")\n",
    "])\n",
    "\n",
    "input_ = {\"topic\":\"cats\"}\n",
    "\n",
    "# invoke replaces the {topic} with cats\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc876b2",
   "metadata": {},
   "source": [
    "### MessagePlaceholder\n",
    "\n",
    "You can use the MessagesPlaceholder prompt template to add a list of messages in a specific location. In `ChatPromptTemplate.from_messages`, you saw how to format two messages, with each message as a string. But what if you want the user to supply a list of messages that you would slot into a particular spot? You can use `MessagesPlaceholder` for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6068dae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the day after Tuesday?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\") # We will replace this with one or more messages\n",
    "])\n",
    "\n",
    "input_ = {\"msgs\":[HumanMessage(content=\"What is the day after Tuesday?\")]}\n",
    "\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "710b2e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Wednesday.' additional_kwargs={'reasoning_content': 'The user asks: \"What is the day after Tuesday?\" That is a simple question: The day after Tuesday is Wednesday. So answer: Wednesday.'} response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 86, 'total_tokens': 128, 'completion_time': 0.046156748, 'completion_tokens_details': {'reasoning_tokens': 31}, 'prompt_time': 0.030938984, 'prompt_tokens_details': None, 'queue_time': 0.051665786, 'total_time': 0.077095732}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_80501ff3a1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c9ef6-bddc-7222-a247-afb9384e34b2-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 86, 'output_tokens': 42, 'total_tokens': 128, 'output_token_details': {'reasoning': 31}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq \n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "chain = prompt | llm \n",
    "response = chain.invoke(input=input_)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a97c3",
   "metadata": {},
   "source": [
    "### Output parsers\n",
    "\n",
    "Output parsers take the output from an LLM and transform that output to a more suitable format. Parsing the output is very useful when you are using LLMs to generate any form of structured data, or to normalize output from chat models and other LLMs.\n",
    "\n",
    "LangChain has lots of different types of output parsers. This is a [list](https://python.langchain.com/v0.2/docs/concepts/#output-parsers) of output parsers LangChain supports. In this lab, you will use the following two output parsers as examples:\n",
    "\n",
    "- `JSON`: Returns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.\n",
    "- `CSV`: Returns a list of comma separated values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a558a54",
   "metadata": {},
   "source": [
    "### JSON parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "535b0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to setup a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "390c8545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': 'Why did the scarecrow win an award?',\n",
       " 'punchline': 'Because he was outstanding in his field.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "joke_query = \"Tell me a joke.\"\n",
    "output_parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "# 2. Format instructions to ensure the LLM returns properly structured data\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"], # Dynamic variables that will be provided when invoking the chain\n",
    "    partial_variables={\"format_instructions\":format_instructions} # Static variables set once when creating the prompt\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({'query':joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1bf593",
   "metadata": {},
   "source": [
    "### Comma seperated list parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97070a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vanilla',\n",
       " 'chocolate',\n",
       " 'strawberry',\n",
       " 'mint chocolate chip',\n",
       " 'cookies and cream']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query. {format_instructions}\\n List five {subject}.\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\":format_instructions}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"subject\":\"ice cream flavours\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77402e45",
   "metadata": {},
   "source": [
    "### Exercise 2 \n",
    "#### **Creating and Using a JSON Output Parser**\n",
    "\n",
    "Now let's implement a simple JSON output parser to structure the responses from your LLM.\n",
    "\n",
    "**Instructions:**  \n",
    "\n",
    "You'll complete the following steps:\n",
    "\n",
    "1. Import the necessary components to create a JSON output parser.\n",
    "2. Create a prompt template that requests information in JSON format (hint: use the provided template).\n",
    "3. Build a chain that connects your prompt, LLM, and JSON parser.\n",
    "4. Test your parser using at least three different inputs.\n",
    "5. Access and display specific fields from the parsed JSON output.\n",
    "6. Verify that your output is properly structured and accessible as a Python dictionary.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f6efa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'The Matrix', 'director': 'Lana Wachowski, Lilly Wachowski', 'year': 1999, 'genre': 'Science Fiction'}\n",
      "Parsed result:\n",
      "Title: The Matrix\n",
      "Director: Lana Wachowski, Lilly Wachowski\n",
      "Year: 1999\n",
      "Genre: Science Fiction\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    title: str = Field(description=\"Movie title\")\n",
    "    director: str = Field(description=\"Movie director name\")\n",
    "    year: int = Field(description=\"Release year\")\n",
    "    genre: str = Field(description=\"Movie genre\")\n",
    "\n",
    "\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=Movie)\n",
    "\n",
    "format_instructions = \"\"\"RESPONSE FORMAT: Return ONLY a single JSON object—no markdown, no examples, no extra keys.  It must look exactly like:\n",
    "{\n",
    "  \"title\": \"movie title\",\n",
    "  \"director\": \"director name\",\n",
    "  \"year\": 2000,\n",
    "  \"genre\": \"movie genre\"\n",
    "}\n",
    "\n",
    "IMPORTANT: Your response must be *only* that JSON.  Do NOT include any illustrative or example JSON.\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"You are a JSON only assistant.\n",
    "    \n",
    "    Task: Generate info about the movie \"{movie_name}\" in JSON format.\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"movie_name\"],\n",
    "    partial_variables={\"format_instructions\":format_instructions}\n",
    ")\n",
    "\n",
    "movie_chain = prompt_template | llm | output_parser\n",
    "\n",
    "movie_name = \"The Matrix\"\n",
    "\n",
    "result = movie_chain.invoke({\"movie_name\":movie_name})\n",
    "print(result)\n",
    "\n",
    "print(\"Parsed result:\")\n",
    "print(f\"Title: {result['title']}\")\n",
    "print(f\"Director: {result['director']}\")\n",
    "print(f\"Year: {result['year']}\")\n",
    "print(f\"Genre: {result['genre']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01c9ae",
   "metadata": {},
   "source": [
    "### Documents \n",
    "\n",
    "#### Document object\n",
    "A `Document` object in `LangChain` contains information about some data. A Document object has the following two attributes:\n",
    "\n",
    "- `page_content`: *`str`*: This attribute holds the content of the document\\.\n",
    "- `metadata`: *`dict`*: This attribute contains arbitrary metadata associated with the document. You can use the metadata to track various details, such as the document ID, the file name, and other details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e461e454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'About Python', 'my_document_create_time': 1680013019}, page_content=\"Python is an interpreted high-level general-purpose programming language.\\n Python's design philosophy emphasizes code readability with its notable use of significant indentation.\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document \n",
    "\n",
    "# Create a Document instance with:\n",
    "# 1. page_content: The actual text content about Python\n",
    "# 2. metadata: A dictionary containing additional information about this document\n",
    "Document(page_content=\"\"\"Python is an interpreted high-level general-purpose programming language.\n",
    " Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\"\",\n",
    "metadata={\n",
    "    'my_document_id' : 234234,                      # Unique identifier for this document\n",
    "    'my_document_source' : \"About Python\",          # Source or title information\n",
    "    'my_document_create_time' : 1680013019          # Unix timestamp for document creation (March 28, 2023)\n",
    " })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb504e1",
   "metadata": {},
   "source": [
    "Not necessary to include the metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1331e",
   "metadata": {},
   "source": [
    "### Document Loaders\n",
    "Document loaders in LangChain are designed to load documents from a variety of sources; for instance, loading a PDF file and having the LLM read the PDF file using LangChain.\n",
    "\n",
    "#### PDF loader\n",
    "\n",
    "By using the PDF loader, you can load a PDF file as a `Document` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52315320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\")\n",
    "# Call the load() method to:\n",
    "# 1. Download the PDF if needed\n",
    "# 2. Extract text from each page\n",
    "# 3. Create a list of Document objects, one for each page of the PDF\n",
    "# Each Document will contain the text content of a page and metadata including page number\n",
    "\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f75b1493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* corresponding author - jkim72@kent.edu \n",
      "Revolutionizing Mental Health Care through \n",
      "LangChain: A Journey with a Large Language \n",
      "Model\n",
      "Aditi Singh \n",
      " Computer Science  \n",
      " Cleveland State University  \n",
      " a.singh22@csuohio.edu \n",
      "Abul Ehtesham  \n",
      "The Davey Tree Expert \n",
      "Company  \n",
      "abul.ehtesham@davey.com \n",
      "Saifuddin Mahmud  \n",
      "Computer Science & \n",
      "Information Systems  \n",
      " Bradley University  \n",
      "smahmud@bradley.edu  \n",
      "Jong-Hoon Kim* \n",
      " Computer Science,  \n",
      "Kent State University,  \n",
      "jkim72@kent.edu \n",
      "Abstract— Mental health challenges are on the rise in our \n",
      "modern society, and the imperative to address mental disorders, \n",
      "especially regarding anxiety, depression, and suicidal thoughts, \n",
      "underscores the need for effective interventions. This paper \n",
      "delves into the application of recent advancements in pretrained \n",
      "contextualized language models to introduce MindGuide, an \n",
      "innovative chatbot serving as a mental health assistant for \n",
      "individuals seeking guidance and support in these critical areas. \n",
      "MindGuide leve\n"
     ]
    }
   ],
   "source": [
    "print(document[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0ed13",
   "metadata": {},
   "source": [
    "### URL and website loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a34856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain overview - Docs by LangChainSkip to main contentDocs by LangChain home pageOpen sourceSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewDeep AgentsLangChainLangGraphIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewPrebuilt middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Create an agent Core benefitsLangChain overviewCopy pageLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool — so you can build agents that adapt as fast as the ecosystem evolvesCopy pageLangChain is the easy way to start building completely custom agents a\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")\n",
    "\n",
    "web_data = loader.load()\n",
    "\n",
    "print(web_data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad82744f",
   "metadata": {},
   "source": [
    "### Text splitters\n",
    "\n",
    "After you load documents, you will often want to transform those documents to better suit your application.\n",
    "\n",
    "\n",
    "One of the most simple examples of making documents better suit your application is to split a long document into smaller chunks that can fit into your model's context window. LangChain has built-in document transformers that ease the process of splitting, combining, filtering, and otherwise manipulating documents.\n",
    "\n",
    "At a high level, here is how text splitters work:\n",
    "\n",
    "1. They split the text into small, semantically meaningful chunks (often sentences).\n",
    "2. They start combining these small chunks of text into a larger chunk until you reach a certain size (as measured by a specific function).\n",
    "3. After the combined text reaches the new chunk's size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap to keep context between chunks.\n",
    "\n",
    "For a list of types of text splitters LangChain supports, see [LangChain Text Splitters](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eca38839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Create a CharacterTextSplitter with specific configuration:\n",
    "# - chunk_size=200: Each chunk will contain approximately 200 characters\n",
    "# - chunk_overlap=20: Consecutive chunks will overlap by 20 characters to maintain context\n",
    "# - separator=\"\\n\": Text will be split at newline characters when possible\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator=\"\\n\")\n",
    "\n",
    "# Split the previously loaded document (PDF or other text) into chunks\n",
    "# The split_documents method:\n",
    "# 1. Takes a list of Document objects\n",
    "# 2. Splits each document's content based on the configured parameters\n",
    "# 3. Returns a new list of Document objects where each contains a chunk of text\n",
    "# 4. Preserves the original metadata for each chunk\n",
    "\n",
    "chunks = text_splitter.split_documents(document)\n",
    "print(len(chunks)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ef21bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'individuals seeking guidance and support in these critical areas. \\nMindGuide lever ages the capabilities of LangChain and its \\nChatModels, specifically Chat OpenAI, as the bedrock of its'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[5].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326d9163",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "#### Working with Document Loaders and Text Splitters\n",
    "\n",
    "You now know about about Document objects and how to load content from different sources. Now, let's implement a workflow to load documents, split them, and prepare them for retrieval.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the necessary document loaders to work with both PDF and web content.\n",
    "2. Load the provided paper about LangChain architecture.\n",
    "3. Create two different text splitters with varying parameters.\n",
    "4. Compare the resulting chunks from different splitters.\n",
    "5. Examine the metadata preservation across splitting.\n",
    "6. Create a simple function to display statistics about your document chunks.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41776d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Splitter 1 Statistics ===\n",
      "Total number of chunks: 95\n",
      "Average chunk size: 263.80 characters\n",
      "Metadata keys preserved: creationdate, page_label, author, producer, creator, moddate, source, total_pages, page, title\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): comprehensive support within the field of mental health. \n",
      "Additionally, the paper discusses the implementation of \n",
      "Streamlit to enhance the user ex pe...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 49 characters\n",
      "Max chunk size: 299 characters\n",
      "\n",
      "=== Splitter 2 Statistics ===\n",
      "Total number of chunks: 57\n",
      "Average chunk size: 452.74 characters\n",
      "Metadata keys preserved: creationdate, page_label, author, producer, creator, moddate, source, total_pages, page, title\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): with severe intellectual disorders do no longer have get entry \n",
      "to the necessary remedy they require. This remedy gap \n",
      "intensifies the weight of intel...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 120 characters\n",
      "Max chunk size: 497 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document \n",
    "from langchain_community.document_loaders import PyPDFLoader,WebBaseLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "\n",
    "paper_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\"\n",
    "pdf_loader = PyPDFLoader(paper_url)\n",
    "pdf_document = pdf_loader.load()\n",
    "\n",
    "web_url =  \"https://docs.langchain.com/\"\n",
    "\n",
    "web_loader = WebBaseLoader(web_url)\n",
    "web_document = web_loader.load()\n",
    "\n",
    "splitter_1 = CharacterTextSplitter(chunk_size=300,chunk_overlap=30,separator='\\n')\n",
    "splitter_2 = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50,separators=[\"\\n\\n\",\"\\n\",\". \",\" \",\"\"])\n",
    "\n",
    "chunks_1 = splitter_1.split_documents(pdf_document)\n",
    "chunks_2 = splitter_2.split_documents(pdf_document)\n",
    "\n",
    "def display_document_stats(docs, name):\n",
    "    \"\"\"Display statistics about a list of document chunks\"\"\"\n",
    "    total_chunks = len(docs)\n",
    "    total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "    avg_chunk_size = total_chars / total_chunks if total_chunks > 0 else 0\n",
    "    \n",
    "    # Count unique metadata keys across all documents\n",
    "    all_metadata_keys = set()\n",
    "    for doc in docs:\n",
    "        all_metadata_keys.update(doc.metadata.keys())\n",
    "    \n",
    "    # Print the statistics\n",
    "    print(f\"\\n=== {name} Statistics ===\")\n",
    "    print(f\"Total number of chunks: {total_chunks}\")\n",
    "    print(f\"Average chunk size: {avg_chunk_size:.2f} characters\")\n",
    "    print(f\"Metadata keys preserved: {', '.join(all_metadata_keys)}\")\n",
    "    \n",
    "    if docs:\n",
    "        print(\"\\nExample chunk:\")\n",
    "        example_doc = docs[min(5, total_chunks-1)]  # Get the 5th chunk or the last one if fewer\n",
    "        print(f\"Content (first 150 chars): {example_doc.page_content[:150]}...\")\n",
    "        print(f\"Metadata: {example_doc.metadata}\")\n",
    "        \n",
    "        # Calculate length distribution\n",
    "        lengths = [len(doc.page_content) for doc in docs]\n",
    "        min_len = min(lengths)\n",
    "        max_len = max(lengths)\n",
    "        print(f\"Min chunk size: {min_len} characters\")\n",
    "        print(f\"Max chunk size: {max_len} characters\")\n",
    "\n",
    "# Display stats for both chunk sets\n",
    "display_document_stats(chunks_1, \"Splitter 1\")\n",
    "display_document_stats(chunks_2, \"Splitter 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b95a8",
   "metadata": {},
   "source": [
    "### Embedding models\n",
    "Embedding models are specifically designed to interface with text embeddings.\n",
    "\n",
    "Embeddings generate a vector representation for a specified piece or \"chunk\" of text.  Embeddings offer the advantage of allowing you to conceptualize text within a vector space. Consequently, you can perform operations such as semantic search, where you identify pieces of text that are most similar within the vector space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c45647ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 57\n",
      "ChromaDB created\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "paper_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\"\n",
    "pdf_loader = PyPDFLoader(paper_url)\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(\"Chunks:\",len(chunks))\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=chunks,\n",
    "                                     embedding=embedding_model,\n",
    "                                     persist_directory=\"./chroma_db\")\n",
    "print(\"ChromaDB created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f80cf430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:1\n",
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether you\n",
      "\n",
      "Result:2\n",
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether you\n",
      "\n",
      "Result:3\n",
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether you\n"
     ]
    }
   ],
   "source": [
    "query = \"What is LangChain used for?\"\n",
    "\n",
    "docs = vector_store.similarity_search(query,k=3)\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "    print(f\"\\nResult:{i + 1}\")\n",
    "    print(doc.page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70735f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "embeddings = embedding_model.embed_documents(texts)\n",
    "\n",
    "print(len(embeddings))        # number of chunks\n",
    "print(len(embeddings[0]))     # embedding dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6a048",
   "metadata": {},
   "source": [
    "### Retriever \n",
    "\n",
    "A retriever is an interface that returns documents using an unstructured query. Retrievers are more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. You can still use vector stores as the backbone of a retriever. Note that other types of retrievers also exist.\n",
    "\n",
    "Retrievers accept a string `query` as input and return a list of `Documents` as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48c32098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'PyPDF', 'page_label': '1', 'moddate': '2023-12-31T03:52:06+00:00', 'total_pages': 6, 'title': 's8329 final', 'author': 'IEEE', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'page': 0, 'creationdate': '2023-12-31T03:50:13+00:00', 'creator': 'Microsoft Word'}, page_content='and human. The conclusion is drawn in Section V. \\nII. LANGCHAIN \\nLangChain, with its open -source essence, emerges as a \\npromising solution, aiming to simplify the complex process of \\ndeveloping applications powered by large language models \\n(LLMs). This framework though the rapid delivery of building \\nblocks and pre-built chains for building large language model \\napplications shows the easy way developers can do it.')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the vector store as a retriever\n",
    "# This converts the vector store into a retriever interface that can fetch relevant documents\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "\n",
    "# Invoke the retriever with the query \"Langchain\"\n",
    "# This will:\n",
    "# 1. Convert the query text \"Langchain\" into an embedding vector\n",
    "# 2. Perform a similarity search in the vector store using this embedding\n",
    "# 3. Return the most semantically similar documents to the query\n",
    "\n",
    "docs = retriever.invoke(\"LangChain\")\n",
    "docs[0]\n",
    "# Access the first (most relevant) document from the retrieval results\n",
    "# This returns the full Document object including:\n",
    "# - page_content: The text content of the document\n",
    "# - metadata: Any associated metadata like source, page numbers, etc.\n",
    "# The returned document is the one most semantically similar to \"Langchain\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d8628",
   "metadata": {},
   "source": [
    "### Parent document retrievers\n",
    "When splitting documents for retrieval, there are often conflicting goals:\n",
    "\n",
    "- You want small documents so their embeddings can most accurately reflect their meaning. If the documents are too long, then the embeddings can lose meaning.\n",
    "- You want to have long enough documents to retain the context of each chunk of text.\n",
    "\n",
    "The `ParentDocumentRetriever` strikes that balance by splitting and storing small chunks of data. During retrieval, this retriever first fetches the small chunks, but then looks up the parent IDs for the data and returns those larger documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7241e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers import ParentDocumentRetriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.storage import InMemoryStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "parent_splitter = CharacterTextSplitter(chunk_size=2000,chunk_overlap=20,separator='\\n')\n",
    "child_splitter = CharacterTextSplitter(chunk_size=400,chunk_overlap=20,separator=\"\\n\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma(collection_name=\"split_parents\",embedding_function=embedding_model)\n",
    "\n",
    "# Set up an in-memory storage layer for the parent documents\n",
    "# This will store the larger chunks that provide context, but won't be directly embedded\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0629984",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    # The vector store where child document embeddings will be stored and searched\n",
    "    # This Chroma instance will contain the embeddings for the smaller chunks\n",
    "    vectorstore=vector_store,\n",
    "    # The document store where parent documents will be stored\n",
    "    # These larger chunks won't be embedded but will be retrieved by ID when needed\n",
    "    docstore=store,\n",
    "    # The splitter used to create small chunks (400 chars) for precise vector search\n",
    "    # These smaller chunks are embedded and used for similarity matching\n",
    "    child_splitter=child_splitter,\n",
    "    # The splitter used to create larger chunks (2000 chars) for better context\n",
    "    # These parent chunks provide more complete information when retrieved\n",
    "    parent_splitter=parent_splitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d995e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef28631e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c82f86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vector_store.similarity_search(\"Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f886ee97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether your desire is to unlock deeper natural \n",
      "language understanding , enhance data, or circumvent \n",
      "language barriers through translation, LangChain is ready to\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6831972",
   "metadata": {},
   "source": [
    "### Standard RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f496ca35",
   "metadata": {},
   "source": [
    "Note: The approach given in the original notebook deviates because the langchain code provided was deprecated. So this is my way of solving rag with the latest api of langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c522e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7a2d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
    ")\n",
    "\n",
    "splits = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe9c6ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma \n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = Chroma.from_documents(documents=splits,\n",
    "                                     embedding=embeddings,\n",
    "                                     collection_name=\"pdf_rag\")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8faa469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "487eb914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant.\n",
    "    Answer the question ONLY using the context below.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# itemgetter extracts values from dictionary,list,tuples\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever | format_docs,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a06b150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the chat memory \n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chat_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "371a8ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper focuses on the urgent need to address the global mental‑health crisis, particularly the strong link between mental disorders and suicide risk. It argues that early identification of suicidal ideation can save lives and proposes using modern deep‑learning techniques—specifically contextualized pretrained language models in natural‑language‑processing pipelines—to detect signs of suicidal thoughts in text. The work outlines how these models can be trained and applied to real‑world data to provide timely, automated screening and support for individuals at risk.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke(\n",
    "    {\"question\": \"What is this paper about?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5ca55",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "#### **Building a Simple Retrieval System with LangChain**\n",
    "\n",
    "In this exercise, you'll implement a simple retrieval system using LangChain's vector store and retriever components to help answer questions based on a document.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the necessary components for document loading, embedding, and retrieval.\n",
    "2. Load the provided document about artificial intelligence.\n",
    "3. Split the document into manageable chunks.\n",
    "4. Use an embedding model to create vector representations.\n",
    "5. Create a vector store and a retriever.\n",
    "6. Implement a simple question-answering system.\n",
    "7. Test your system with at least 3 different questions.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78823a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is LangChain?\n",
      "A: LangChain is a framework that lets you build intelligent agents that use large language models (LLMs). Built on top of LangGraph, it provides durable execution, streaming, human‑in‑the‑loop capabilities, persistence, and a pre‑built agent architecture with ready‑made model integrations so you can quickly incorporate LLMs into your applications.\n",
      "--------------------------------------------------\n",
      "Q: How do retrievers work?\n",
      "A: I’m sorry, but the provided context does not contain any information about how retrievers work.\n",
      "--------------------------------------------------\n",
      "Q: Why is document splitting important?\n",
      "A: I don't know.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200,\n",
    "                                               chunk_overlap=20,\n",
    "                                               separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=chunks,\n",
    "                                     embedding=embedding_model,\n",
    "                                     collection_name=\"simple-rag\")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant.\n",
    "    Answer ONLY using the context below.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question:\n",
    "    {question}\"\"\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\":itemgetter(\"question\") | retriever | format_docs,\n",
    "        \"question\":itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "test_queries = [\n",
    "    \"What is LangChain?\",\n",
    "    \"How do retrievers work?\",\n",
    "    \"Why is document splitting important?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", rag_chain.invoke({\"question\": q}))\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1592e8",
   "metadata": {},
   "source": [
    "### Memory \n",
    "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At a bare minimum, a conversational system should be able to directly access some window of past messages.\n",
    "\n",
    "### Chat message history\n",
    "One of the core utility classes underpinning most (if not all) memory modules is the `ChatMessageHistory` class. This class is a super lightweight wrapper that provides convenience methods for saving `HumanMessages` and `AIMessages`, and then fetching both types of messages.\n",
    "\n",
    "Here is an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de53dfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='hi!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={})]\n",
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "history.add_ai_message(\"hi!\")\n",
    "\n",
    "history.add_user_message(\"What is the capital of France?\")\n",
    "\n",
    "print(history.messages)\n",
    "\n",
    "response = llm.invoke(history.messages)\n",
    "print(response.content)\n",
    "\n",
    "history.add_ai_message(response.content)\n",
    "# print(history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68e79c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='hi!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is **Paris**.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n"
     ]
    }
   ],
   "source": [
    "print(history.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed34355",
   "metadata": {},
   "source": [
    "### Conversation buffer \n",
    "\n",
    "1. Prompt with history\n",
    "2. Create a chain\n",
    "3. Create a memory store \n",
    "4. Wrap with memory\n",
    "5. Run conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf3b9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt with history\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\",\"{input}\")\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a289a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain \n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666da28",
   "metadata": {},
   "source": [
    "- `InMemoryChatMessageHistory` stores chat history in RAM\n",
    "- `RunnableWithMessageHistory` wraps chain/agent this automatically adds chat history to every request \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a5fd48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a memory store \n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87928dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap with memory \n",
    "\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "chat_chain = RunnableWithMessageHistory(chain,\n",
    "                                        get_session_history,\n",
    "                                        input_messages_key=\"input\",\n",
    "                                        history_messages_key=\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3a3c661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You just told me you’re a tiger.  So, you’re a tiger—an iconic, big‑cat species known for its distinctive orange‑and‑black stripes, powerful build, and impressive hunting skills.  If you’re looking for more detail about yourself (or about tigers in general), just let me know!', additional_kwargs={'reasoning_content': 'The user asks \"Who am I?\" We need to respond. They said earlier \"Hello, I am a tiger.\" So maybe they identify as a tiger. But we can\\'t be sure. We could ask clarifying. Or we can respond acknowledging they said they are a tiger. The user might be testing. We can respond: \"You are a tiger, or you identify as a tiger.\" We can ask if they want more detail. It\\'s a short answer.'}, response_metadata={'token_usage': {'completion_tokens': 169, 'prompt_tokens': 637, 'total_tokens': 806, 'completion_time': 0.170801317, 'completion_tokens_details': {'reasoning_tokens': 94}, 'prompt_time': 0.032187669, 'prompt_tokens_details': None, 'queue_time': 0.043207357, 'total_time': 0.202988986}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_deb540145b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c9ef7-b495-7d62-9c12-43fefa2c8d60-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 637, 'output_tokens': 169, 'total_tokens': 806, 'output_token_details': {'reasoning': 94}})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation \n",
    "chat_chain.invoke(\n",
    "    {\"input\": \"Hello, I am a tiger. Who are you?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}}\n",
    ")\n",
    "\n",
    "chat_chain.invoke(\n",
    "    {\"input\": \"What can you do?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}}\n",
    ")\n",
    "\n",
    "chat_chain.invoke(\n",
    "    {\"input\": \"Who am I?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da322b0",
   "metadata": {},
   "source": [
    "#### **Building a Chatbot with Memory using LangChain**\n",
    "\n",
    "In this exercise, you'll create a simple chatbot that can remember previous interactions using LangChain's memory components. You'll implement conversation memory to make your chatbot maintain context throughout a conversation.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the necessary components for chat history and conversation memory.\n",
    "2. Set up a language model for your chatbot.\n",
    "3. Create a conversation chain with memory capabilities.\n",
    "4. Implement a simple interactive chat interface.\n",
    "5. Test the memory capabilities with a series of related questions.\n",
    "6. Examine how the conversation history is stored and accessed.\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4dbf0f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Hello, my name is Alice.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello, Alice.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\",\n",
    "               max_tokens=256,\n",
    "               temperature=0.2)\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "history.add_user_message(\"Hello, my name is Alice.\")\n",
    "history.add_ai_message(\"Hello, Alice.\")\n",
    "\n",
    "print(history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14c49f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Beginning Chat Simulation ===\n",
      "\n",
      "--- Turn 1 ---\n",
      "Human: My favorite color is blue.\n",
      "AI: That’s a classic choice! Blue often feels calm and expansive—like the sky or the ocean. Do you have a particular shade you’re drawn to, or a favorite way you use blue in your life?\n",
      "\n",
      "--- Turn 2 ---\n",
      "Human: I enjoy hiking in the mountains.\n",
      "AI: That sounds amazing! The mountains have a way of putting everything into perspective—fresh air, wide vistas, and that quiet “you’re on top of the world” feeling. \n",
      "\n",
      "Do you have a favorite trail or a particular mountain range you love? I’d love to hear about the places you’ve explored or any memorable moments from your hikes. If you’re looking for new routes or gear tips, just let me know!\n",
      "\n",
      "--- Turn 3 ---\n",
      "Human: What activities would you recommend for me?\n",
      "AI: Here are a handful of activities that blend your love of the outdoors, the mountains, and that calming blue vibe you’re drawn to:\n",
      "\n",
      "| # | Activity | Why it fits | Quick Tips |\n",
      "|---|----------|-------------|------------|\n",
      "| **1** | **Landscape Photography** | Capture the sky, water, and mountain vistas in their natural blues. | Bring a tripod, shoot at golden hour, experiment with long‑exposure for silky water. |\n",
      "| **2** | **Sketching/Watercolor Sketchbook** | Turn your hikes into art—focus on blue skies, alpine lakes, or the blue‑tinged rock faces. | Keep a small sketchbook and watercolors in your pack;\n",
      "\n",
      "--- Turn 4 ---\n",
      "Human: What was my favorite color again?\n",
      "AI: Your favorite color is **blue**.\n",
      "\n",
      "--- Turn 5 ---\n",
      "Human: Can you remember both my name and my favorite color?\n",
      "AI: I don’t have a record of your name—just that your favorite color is **blue**. If you’d like me to remember it, feel free to share your name!\n",
      "\n",
      "=== End of Chat Simulation ===\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\",\"{input}\")\n",
    "]) \n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chat_chain = RunnableWithMessageHistory(chain,\n",
    "                                        get_session_history,\n",
    "                                        input_messages_key=\"input\",\n",
    "                                        history_messages_key=\"history\")\n",
    "\n",
    "def chat_simulation(conversation, inputs, session_id=\"1\"):\n",
    "    print(\"\\n=== Beginning Chat Simulation ===\")\n",
    "    \n",
    "    for i, user_input in enumerate(inputs):\n",
    "        print(f\"\\n--- Turn {i+1} ---\")\n",
    "        print(f\"Human: {user_input}\")\n",
    "        \n",
    "        response = conversation.invoke(\n",
    "            {\"input\": user_input},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        \n",
    "        print(f\"AI: {response}\")\n",
    "    \n",
    "    print(\"\\n=== End of Chat Simulation ===\")\n",
    "\n",
    "# 6. Test with a series of related questions\n",
    "test_inputs = [\n",
    "    \"My favorite color is blue.\",\n",
    "    \"I enjoy hiking in the mountains.\",\n",
    "    \"What activities would you recommend for me?\",\n",
    "    \"What was my favorite color again?\",\n",
    "    \"Can you remember both my name and my favorite color?\"\n",
    "]\n",
    "\n",
    "chat_simulation(chat_chain, test_inputs,session_id=\"1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5344eb",
   "metadata": {},
   "source": [
    "Sequnetial chain allows you to use the output of the LLM as input to another LLM. This approach is benefiticial for dividing tasks and maintaining focus of your LLM. We will do this with lcel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62790fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'location': 'China',\n",
      " 'meal': '**Peking Duck**\\n'\n",
      "         '\\n'\n",
      "         'A legendary dish from Beijing, Peking Duck is renowned for its '\n",
      "         'crispy, lacquer‑glazed skin and tender, flavorful meat. '\n",
      "         'Traditionally roasted in a wood‑fired oven, the duck is served '\n",
      "         'thinly sliced with sweet bean sauce, scallions, and thin pancakes, '\n",
      "         'allowing diners to wrap the succulent meat in a delicate, aromatic '\n",
      "         'bite. This iconic dish embodies the artistry and culinary heritage '\n",
      "         'of China.',\n",
      " 'recipe': '**Homemade Peking‑Duck (Simplified)**  \\n'\n",
      "           '\\n'\n",
      "           '**Ingredients**  \\n'\n",
      "           '- 1 whole duck (≈4–5\\u202flb)  \\n'\n",
      "           '- 1 tbsp maltose or honey (for glazing)  \\n'\n",
      "           '- 1 tsp Chinese five‑spice powder  \\n'\n",
      "           '- 1 tsp salt  \\n'\n",
      "           '- 1 tsp sugar  \\n'\n",
      "           '- 1 tbsp soy sauce  \\n'\n",
      "           '- 1 tbsp rice vinegar  \\n'\n",
      "           '- 1 tbsp sesame oil  \\n'\n",
      "           '- 1 cup sweet bean sauce (store‑bought or homemade)  \\n'\n",
      "           '- 8–10 thin pancakes (store‑bought or homemade)  \\n'\n",
      "           '- 2‑3 scallions, sliced into thin sticks  \\n'\n",
      "           '\\n'\n",
      "           '**Equipment**  \\n'\n",
      "           '- Oven (or a grill)  \\n'\n",
      "           '- Baking sheet or roasting pan  \\n'\n",
      "           '- Brush for glazing  \\n'\n",
      "           '\\n'\n",
      "           '---\\n'\n",
      "           '\\n'\n",
      "           '### 1. Prep the Duck  \\n'\n",
      "           '1. **Clean & Dry** – Rinse the duck, pat dry with paper towels.  \\n'\n",
      "           '2. **Season** – Mix salt, sugar, five',\n",
      " 'time': ''}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from pprint import pprint\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "location_template = \"\"\"Your job is to come up with a classic dish from the area that suggests {location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "dish_template = \"\"\"Given a meal {meal}, give a short and simple recipe on how to make that dish at home.\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "time_template = \"\"\"Given the recipe {recipe}, estimate how much time I need to cook it.\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "location_chain_lcel = (\n",
    "    PromptTemplate.from_template(location_template)\n",
    "    | llm \n",
    "    | output_parser\n",
    ") \n",
    "\n",
    "dish_chain_lcel = (\n",
    "    PromptTemplate.from_template(dish_template)\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "time_chain_lcel = (\n",
    "    PromptTemplate.from_template(time_template)\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "\n",
    "# Combine all chains into a single workflow using RunnablePassthrough.assign\n",
    "# RunnablePassthrough.assign adds new keys to the input dictionary without removing existing ones\n",
    "\n",
    "overall_chain_lcel = (\n",
    "    # Step 1: Generate a meal based on location and add it to the input dictionary\n",
    "    RunnablePassthrough.assign(meal=lambda x: location_chain_lcel.invoke({\"location\": x[\"location\"]}))\n",
    "    # Step 2: Generate a recipe based on the meal and add it to the input dictionary\n",
    "    | RunnablePassthrough.assign(recipe=lambda x: dish_chain_lcel.invoke({\"meal\": x[\"meal\"]}))\n",
    "    # Step 3: Estimate cooking time based on the recipe and add it to the input dictionary\n",
    "    | RunnablePassthrough.assign(time=lambda x: time_chain_lcel.invoke({\"recipe\": x[\"recipe\"]}))\n",
    ")\n",
    "\n",
    "result = overall_chain_lcel.invoke({\"location\": \"China\"})\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5f5b5",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "#### **Implementing Multi-Step Processing with Different Chain Approaches**\n",
    "\n",
    "In this exercise, you'll create a multi-step information processing system using both traditional chains and the modern LCEL approach. You'll build a system that analyzes product reviews, extracts key information, and generates responses based on the analysis.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the necessary components for both traditional chains and LCEL.\n",
    "2. Implement a three-step process using both traditional SequentialChain and modern LCEL approaches.\n",
    "3. Create templates for sentiment analysis, summarization, and response generation.\n",
    "4. Test your implementations with sample product reviews.\n",
    "5. Compare the flexibility and readability of both approaches.\n",
    "6. Document the advantages and disadvantages of each method.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e77690c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING WITH REVIEW:\n",
      "I absolutely love this coffee maker! It brews quickly and the coffee tastes amazing. \n",
      "The built-in g...\n",
      "\n",
      "\n",
      "LCEL CHAIN RESULTS:\n",
      "==================================================\n",
      "{'review': 'I absolutely love this coffee maker! It brews quickly and the coffee tastes amazing. \\nThe built-in grinder saves me so much time in the morning, and the programmable timer means \\nI wake up to fresh coffee every day. Worth every penny and highly recommended to any coffee enthusiast.', 'sentiment': 'SENTIMENT: positive', 'summary': '- Brews quickly, delivering coffee in a short amount of time.  \\n- Produces a flavorful, high‑quality cup that tastes amazing.  \\n- Features a built‑in grinder, cutting morning prep time.  \\n- Programmable timer ensures fresh coffee right when you wake up.  \\n- Considered worth every penny and highly recommended for coffee lovers.', 'response': 'Hi there!  \\n\\nThank you so much for taking the time to share your experience. We’re thrilled to hear that our coffee maker is becoming a morning essential for you—especially the quick brew, the built‑in grinder, and the programmable timer that delivers fresh coffee right when you need it. It’s wonderful to know that the flavor and quality meet your high standards and that you feel it’s worth every penny.  \\n\\nIf you ever have any questions, want to explore additional accessories, or just want to share more of your coffee adventures, feel free to reach out. We’re always here to help and would love to hear more about how we can keep your coffee routine even smoother.  \\n\\nThanks again for the recommendation, and enjoy every cup!  \\n\\nWarm regards,  \\n[Your Name]  \\nCustomer Success Team  '}\n",
      "\n",
      "==================================================\n",
      "TESTING WITH REVIEW:\n",
      "Disappointed with this laptop. It's constantly overheating after just 30 minutes of use, \n",
      "and the ba...\n",
      "\n",
      "\n",
      "LCEL CHAIN RESULTS:\n",
      "==================================================\n",
      "{'review': \"Disappointed with this laptop. It's constantly overheating after just 30 minutes of use, \\nand the battery life is nowhere near the 8 hours advertised - I barely get 3 hours. \\nThe keyboard has already started sticking on several keys after just two weeks. Would not recommend to anyone.\", 'sentiment': 'SENTIMENT: negative', 'summary': '- Overheats after just 30 minutes of use.  \\n- Battery life only about 3 hours, far below the advertised 8 hours.  \\n- Keyboard keys start sticking after two weeks.  \\n- Overall, the reviewer does not recommend the', 'response': '**Response to Customer**\\n\\nHi [Customer Name],\\n\\nThank you for taking the time to share your experience. I’m really sorry to hear that your laptop has been overheating, the battery life is far below what we promised, and the keyboard is already showing issues after just a couple of weeks. I understand how frustrating this must be, especially when you’re relying on the device for work or personal use.\\n\\n**Here’s what we can do to help:**\\n\\n1. **Overheating** – We’ll start by checking the cooling system. If the fan or heat sink is clogged or the thermal paste has degraded, a quick cleaning or replacement can often resolve the issue.  \\n2. **Battery Life** – The 8‑hour rating is based on a full charge under moderate use. If you’re seeing only 3 hours, it could be a battery that’s'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "# Sample product reviews for testing\n",
    "positive_review = \"\"\"I absolutely love this coffee maker! It brews quickly and the coffee tastes amazing. \n",
    "The built-in grinder saves me so much time in the morning, and the programmable timer means \n",
    "I wake up to fresh coffee every day. Worth every penny and highly recommended to any coffee enthusiast.\"\"\"\n",
    "\n",
    "negative_review = \"\"\"Disappointed with this laptop. It's constantly overheating after just 30 minutes of use, \n",
    "and the battery life is nowhere near the 8 hours advertised - I barely get 3 hours. \n",
    "The keyboard has already started sticking on several keys after just two weeks. Would not recommend to anyone.\"\"\"\n",
    "\n",
    "# Step 1: Define the prompt templates for each processing step\n",
    "sentiment_template = \"\"\"Analyze the sentiment of the following product review as positive, negative, or neutral.\n",
    "Provide your analysis in the format: \"SENTIMENT: [positive/negative/neutral]\"\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Your analysis:\n",
    "\"\"\"\n",
    "\n",
    "summary_template = \"\"\"Summarize the following product review into 3-5 key bullet points.\n",
    "Each bullet point should be concise and capture an important aspect mentioned in the review.\n",
    "\n",
    "Review: {review}\n",
    "Sentiment: {sentiment}\n",
    "\n",
    "Key points:\n",
    "\"\"\"\n",
    "\n",
    "response_template = \"\"\"Write a helpful response to a customer based on their product review.\n",
    "If the sentiment is positive, thank them for their feedback. If negative, express understanding \n",
    "and suggest a solution or next steps. Personalize based on the specific points they mentioned.\n",
    "\n",
    "Review: {review}\n",
    "Sentiment: {sentiment}\n",
    "Key points: {summary}\n",
    "\n",
    "Response to customer:\n",
    "\"\"\"\n",
    "\n",
    "sentiment_prompt = PromptTemplate.from_template(sentiment_template)\n",
    "summary_prompt = PromptTemplate.from_template(summary_template)\n",
    "response_prompt = PromptTemplate.from_template(response_template)\n",
    "\n",
    "sentiment_chain = (\n",
    "    sentiment_prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summary_chain = (\n",
    "    summary_prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response_chain = (\n",
    "    response_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "lcel_chain = (\n",
    "    RunnablePassthrough.assign(sentiment = lambda x: sentiment_chain.invoke({\"review\":x[\"review\"]}))\n",
    "    | RunnablePassthrough.assign(summary = lambda x: summary_chain.invoke({\"review\":x[\"review\"],\n",
    "                                                                           \"sentiment\":x[\"sentiment\"]}))\n",
    "    | RunnablePassthrough.assign(response = lambda x: response_chain.invoke({\"review\":x[\"review\"],\n",
    "                                                                             \"sentiment\":x[\"sentiment\"],\n",
    "                                                                             \"summary\":x[\"summary\"]}))\n",
    ")\n",
    "\n",
    "def test_chains(review):\n",
    "    \"\"\"Test both chain implementations with the given review\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"TESTING WITH REVIEW:\\n{review[:100]}...\\n\")\n",
    "\n",
    "    \n",
    "    print(\"\\nLCEL CHAIN RESULTS:\")\n",
    "    result = lcel_chain.invoke({\"review\":review})\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(result)\n",
    "\n",
    "# Run tests\n",
    "test_chains(positive_review)\n",
    "test_chains(negative_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbdcb7e",
   "metadata": {},
   "source": [
    "### Tools and Agents\n",
    "\n",
    "Tools extend an LLM's capabilities beyond just generating text. They allow the model to actually perform actions in the world or access external systems. This notebook shows the Python REPL tool, but there are many other tools:\n",
    "\n",
    "- Search tools: Connect to search engines, database queries, or vector stores.\n",
    "- API tools: Make calls to external web services.\n",
    "- Human-in-the-loop tools: Request human input for critical decisions.\n",
    "\n",
    "Let’s explore how to work with tools, using the `Python REPL` tool as an example. The `Python REPL` tool can run Python commands. These commands can either come from the user or the LLM can generate the commands. This tool is particularly useful for complex calculations. Instead of having the LLM generate the answer directly, using the LLM to generate code to calculate the answer is more efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9439eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c43343",
   "metadata": {},
   "source": [
    "The `@tool` decorator is a convenient way to define tools, but you can also use the Tool class directly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "842e0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a PythonREPL instance\n",
    "# # This provides an environment where Python code can be executed as strings\n",
    "# python_repl = PythonREPL()\n",
    "\n",
    "# # Create a Tool using the Tool class\n",
    "# # This wraps the Python REPL functionality as a tool that can be used by agents\n",
    "# python_calculator = Tool(\n",
    "#     # The name of the tool - this helps agents identify when to use this tool\n",
    "#     name=\"Python Calculator\",\n",
    "    \n",
    "#     # The function that will be called when the tool is used\n",
    "#     # python_repl.run takes a string of Python code and executes it\n",
    "#     func=python_repl.run,\n",
    "    \n",
    "#     # A description of what the tool does and how to use it\n",
    "#     # This helps the agent understand when and how to use this tool\n",
    "#     description=\"Useful for when you need to perform calculations or execute Python code. Input should be valid Python code.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f96ba541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python_calculator.invoke(\"a = 3;b = 1;print(a + b)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01214b5a",
   "metadata": {},
   "source": [
    "Creating tools with @tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "de3d549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_weather(location: str):\n",
    "    \"\"\"Search for the current weather at preferred location\"\"\"\n",
    "    return f\"The weather in {location} is sunny 72°F.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "62fcdc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def python_calculator(code: str):\n",
    "    \"\"\"Execute python math code safely\"\"\"\n",
    "    try:\n",
    "        return str(eval(code))\n",
    "    except Exception as e:\n",
    "        return f\"Error: invalid python code → {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cd2b699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [python_calculator,search_weather]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a7455",
   "metadata": {},
   "source": [
    "### Agents \n",
    "By themselves, language models can't take actions; they just output text. A big use case for LangChain is creating agents. Agents are systems that leverage a large language model (LLM) as a reasoning engine to identify appropriate actions and determine the required inputs for those actions. The results of those actions are to be fed back into the agent. The agent then makes a determination whether more actions are needed, or if the task is complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f5adad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_calculator.invoke(\"1+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f2696d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "model = init_chat_model(model=\"openai/gpt-oss-20b\",\n",
    "                        model_provider=\"groq\")\n",
    "\n",
    "agent = create_agent(model=model,\n",
    "                     tools=tools,\n",
    "                     system_prompt=\"You are an expert in math you'll solve any question I ask.For ANY calculation, ALWAYS use python_calculator. Never answer without it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "98ca00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\"input\":\"1 + 1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "55ace55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Got it! I'm ready to tackle any math question you have. Just let me know what you'd like to solve, and I'll use the `python_calculator` for all calculations.\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "42117205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content=\"Sure! Whenever you're ready, just let me know what math problem you'd like help with.\", additional_kwargs={'reasoning_content': 'We need to respond to a user question. The user hasn\\'t asked a question yet. They just gave instructions. So we should wait for a question. But we could respond with a note. Probably respond: \"Sure, I\\'m ready to help with math questions.\"'}, response_metadata={'token_usage': {'completion_tokens': 80, 'prompt_tokens': 170, 'total_tokens': 250, 'completion_time': 0.080845412, 'completion_tokens_details': {'reasoning_tokens': 53}, 'prompt_time': 0.008219529, 'prompt_tokens_details': None, 'queue_time': 0.043584501, 'total_time': 0.089064941}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c9efb-605b-7d42-a768-80975d59f948-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 170, 'output_tokens': 80, 'total_tokens': 250, 'output_token_details': {'reasoning': 53}})]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"input\": \"Calculate 981 * 999102 using python_calculator\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
