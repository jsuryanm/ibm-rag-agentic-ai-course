{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0fec7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142258a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq \n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\",\n",
    "               max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3fd6037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'reasoning_content': 'The user: \"I enjoy mystery novels, what should I read?\" They want a recommendation. The instruction: \"You are a helpful AI bot that assists the user in choosing the perfect book to read in a single sentence\". So answer in a single sentence. Provide a recommendation: maybe \"I recommend \\'The Girl with the Dragon Tattoo\\' by Stieg Larsson\" or \"The Hound of the Baskervilles\" or \"The No. 1 Ladies\\' Detective Agency\". The user just says \"I enjoy mystery novels, what should I read?\" They didn\\'t specify any sub-genre, age, etc. So a general recommendation: \"I recommend \\'The Girl with the Dragon Tattoo\\' by Stieg Larsson.\" But it\\'s also a thriller. Or \"The Murder of Roger Ackroyd\" by Agatha Christie. Or \"The Name of the Rose\" by Umberto Eco. Or \"Gone Girl\" by Gillian Flynn. Or \"The Da Vinci Code\" (but not pure mystery). Maybe \"The Hound of the Baskervilles\" by Arthur Conan Doyle. Or \"The Maltese Falcon\". Or \"The Westing Game\". Or \"The Shadow of the Wind\". But the user just says \"I enjoy mystery novels\". So maybe give a suggestion. Let\\'s pick \"The Hound of the Baskervilles\" as classic. Or \"The Girl with the Dragon Tattoo\" as modern. Maybe mention a few? But must be a single sentence. Could be \"If you love mystery novels, you should read \\'The Hound of the Baskervilles\\' by Arthur Conan Doyle for a classic detective experience.\" That fits. Or \"I recommend \\'The Girl with the Dragon Tattoo\\' by Stieg Larsson for a gripping modern mystery.\" Or \"Try Agatha Christie\\'s \\'Murder on the Orient Express\\' for an iconic whodunit.\" Let\\'s pick one. Provide one recommendation. The user didn\\'t specify preferences like classic or modern. Maybe give a single suggestion. The instruction: \"assist the user in choosing the perfect book to read in a single sentence\". So answer with one recommendation. Let\\'s choose \"The Girl with the Dragon Tattoo\" because it\\'s a modern mystery. Or \"Murder on the Orient Express\". But the user might want something like \"The No. 1 Ladies\\' Detective Agency\" if they like a gentle mystery. But we have to pick one. I\\'ll choose \"Murder on the Orient Express\" because it\\'s classic and'} response_metadata={'token_usage': {'completion_tokens': 500, 'prompt_tokens': 105, 'total_tokens': 605, 'completion_time': 0.520443616, 'completion_tokens_details': {'reasoning_tokens': 498}, 'prompt_time': 0.005444465, 'prompt_tokens_details': None, 'queue_time': 0.044576532, 'total_time': 0.525888081}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_deb540145b', 'service_tier': 'on_demand', 'finish_reason': 'length', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c9e16-a2f6-73d1-8f60-d5559024362c-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 105, 'output_tokens': 500, 'total_tokens': 605, 'output_token_details': {'reasoning': 498}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "\n",
    "msg = llm.invoke([SystemMessage(content=\"You are a helpful AI bot that assists the user in choosing the perfect book to read in a single sentence\"),\n",
    "                  HumanMessage(content=\"I enjoy mystery novels, what should I read?\")])\n",
    "\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab50fbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Aim for 2–3 HIT sessions per week, spacing them with a rest day or lighter activity between each.' additional_kwargs={'reasoning_content': 'User: \"How often should I attend?\" They like HIT workouts. Provide supportive suggestion in one short sentence. Need to give frequency recommendation. Probably 2-3 times per week, ensuring rest days. Keep short.'} response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 118, 'total_tokens': 195, 'completion_time': 0.077434839, 'completion_tokens_details': {'reasoning_tokens': 45}, 'prompt_time': 0.005727446, 'prompt_tokens_details': None, 'queue_time': 0.043052734, 'total_time': 0.083162285}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c9e16-a5a9-7a51-91f0-2eb710d5cd51-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 118, 'output_tokens': 77, 'total_tokens': 195, 'output_token_details': {'reasoning': 45}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "msg = llm.invoke([\n",
    "    SystemMessage(content=\"You are a supportive AI bot that suggests fitness activities to a user in one short sentence\"),\n",
    "    HumanMessage(content=\"I like HIT workouts\"),\n",
    "    AIMessage(content=\"You should try a crossfit class\"),\n",
    "    HumanMessage(content=\"How often should I attend?\")\n",
    "])\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "512f3131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me one funny joke about dogs')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Tell me one {adjective} joke about {topic}\")\n",
    "input_ = {\"adjective\":\"funny\",\"topic\":\"dogs\"}\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d639fc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me joke about dogs', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful AI assistant\"),\n",
    "    (\"user\",\"Tell me joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt.invoke({\"topic\":\"dog\"}) # why does this not work\n",
    "\n",
    "input_ = {\"topic\":\"dogs\"}\n",
    "\n",
    "prompt.invoke(input_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f85879b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the day after tomorrow?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage \n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful AI assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "input_ = {\"msgs\":[HumanMessage(\"What is the day after tomorrow?\")]}\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b89df0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='I’m not sure what today’s date is. Could you let me know which day you’re referring to (or your current date), so I can tell you what the day after tomorrow will be?' additional_kwargs={'reasoning_content': 'We have a user question: \"What is the day after tomorrow?\" This is ambiguous without a date context. The user might be asking generically: if today is X, what is the day after tomorrow? But we need a date. The user didn\\'t specify a date. We could ask for clarification: what is today\\'s date? Or we could assume a date? Usually we ask for clarification. According to policy, if the user asks a question that depends on a date, we should ask for clarification or mention that we need the current date. So we ask: \"What is today\\'s date?\" Or \"What day is today?\" Then we can compute. So respond with a question asking for clarification.'} response_metadata={'token_usage': {'completion_tokens': 190, 'prompt_tokens': 87, 'total_tokens': 277, 'completion_time': 0.192965951, 'completion_tokens_details': {'reasoning_tokens': 141}, 'prompt_time': 0.004096224, 'prompt_tokens_details': None, 'queue_time': 0.043764362, 'total_time': 0.197062175}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_5c8ca06ea1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c9e16-a83f-7450-8e38-12f93f627bfd-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 87, 'output_tokens': 190, 'total_tokens': 277, 'output_token_details': {'reasoning': 141}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "chain = prompt | llm \n",
    "response = chain.invoke(input=input_)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9805e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel,Field \n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to setup a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3173aef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "joke_query = \"Tell me a joke\"\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=Joke)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=['query'],\n",
    "    partial_variables={\"format_instructions\":format_instructions}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"query\":joke_query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9ba03e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lindt', 'Toblerone', 'Nestlé', 'Frey', 'Cailler']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(template=\"Answer the user query. {format_instructions}\\n\\nList five {subject}.\",\n",
    "                        input_variables=['subject'],\n",
    "                        partial_variables={\"format_instructions\":format_instructions})\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"subject\":\"swiss chocolates\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9f12eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Spider-Man 2', 'director': 'Sam Raimi', 'year': 2004, 'genre': 'Superhero, Action, Adventure'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel,Field \n",
    "\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    title: str = Field(description=\"Movie title\")\n",
    "    director: str = Field(description=\"Director name\")\n",
    "    year: int  = Field(description=\"Release year\")\n",
    "    genre: str = Field(description=\"Movie genre\")\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=Movie)\n",
    "format_instructions = \"\"\"RESPONSE FORMAT: Return ONLY a single JSON object—no markdown, no examples, no extra keys.  It must look exactly like:\n",
    "{\n",
    "  \"title\": \"movie title\",\n",
    "  \"director\": \"director name\",\n",
    "  \"year\": 2000,\n",
    "  \"genre\": \"movie genre\"\n",
    "}\n",
    "\n",
    "IMPORTANT: Your response must be *only* that JSON.  Do NOT include any illustrative or example JSON.\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(template=\"\"\"You are JSON only assistant.\n",
    "                                 Task: Generate info about movie \"{movie_name}\" in JSON format\n",
    "                                 {format_instructions}\n",
    "                                 \"\"\",\n",
    "                                 input_variables=[\"movie_name\"],\n",
    "                                 partial_variables={\"format_instructions\":format_instructions})\n",
    "\n",
    "movie_chain = prompt_template | llm | output_parser\n",
    "movie_name = \"Spider-Man-2\"\n",
    "response = movie_chain.invoke({\"movie_name\":movie_name})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20b14c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'About Python', 'my_document_create_time': 1680013019}, page_content=\"Python is an interpreted high-level general-purpose programming language.\\n Python's design philosophy emphasizes code readability with its notable use of significant indentation.\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "Document(page_content=\"\"\"Python is an interpreted high-level general-purpose programming language.\n",
    " Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\"\",\n",
    "metadata={\n",
    "    'my_document_id' : 234234,                      # Unique identifier for this document\n",
    "    'my_document_source' : \"About Python\",          # Source or title information\n",
    "    'my_document_create_time' : 1680013019          # Unix timestamp for document creation (March 28, 2023)\n",
    " })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "017b38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "378e51c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* corresponding author - jkim72@kent.edu \n",
      "Revolutionizing Mental Health Care through \n",
      "LangChain: A Journey with a Large Language \n",
      "Model\n",
      "Aditi Singh \n",
      " Computer Science  \n",
      " Cleveland State University  \n",
      " a.singh22@csuohio.edu \n",
      "Abul Ehtesham  \n",
      "The Davey Tree Expert \n",
      "Company  \n",
      "abul.ehtesham@davey.com \n",
      "Saifuddin Mahmud  \n",
      "Computer Science & \n",
      "Information Systems  \n",
      " Bradley University  \n",
      "smahmud@bradley.edu  \n",
      "Jong-Hoon Kim* \n",
      " Computer Science,  \n",
      "Kent State University,  \n",
      "jkim72@kent.edu \n",
      "Abstract— Mental health challenges are on the rise in our \n",
      "modern society, and the imperative to address mental disorders, \n",
      "especially regarding anxiety, depression, and suicidal thoughts, \n",
      "underscores the need for effective interventions. This paper \n",
      "delves into the application of recent advancements in pretrained \n",
      "contextualized language models to introduce MindGuide, an \n",
      "innovative chatbot serving as a mental health assistant for \n",
      "individuals seeking guidance and support in these critical areas. \n",
      "MindGuide leve\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "308f8536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home - Docs by LangChainSkip to main contentDocs by LangChain home pageHomeSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationDocumentationLangChain is the platform for agent engineering. AI teams at Replit, Clay, Rippling, Cloudflare, Workday, and more trust LangChain’s products to engineer reliable agents.LangSmithLangSmith is a platform that helps AI teams use live production data for continuous testing and improvement. LangSmith provides:ObservabilitySee exactly how your agent thinks and acts with detailed tracing and aggregate trend metrics.Learn moreEvaluationTest and score agent behavior on production data or offline datasets to continuously improve performance.Learn morePrompt EngineeringIterate on prompts with version control, prompt optimization, and collaboration features.Learn moreDeploymentShip your agent in one click, using scalable infrastructure built for long-running tasks.Learn moreLangSmith meets the highest standards of data security and privacy with HIPAA, SOC 2 Type 2, and GDPR compliance. For more information, see the Trust Center.LangSmith Agent BuilderAgent BuilderCreate helpful AI agents without code. Start from a template, connect your accounts, and let the agent handle routine work while you stay in control.Learn moreOpen source agent frameworks Python TypeScriptDeep Agents (Python)Quickly build agents that can tackle any task.Learn moreLangChain (Python)Customize the building blocks of your agents.Learn moreLangGraph (Python)Control every step of your custom agent with low-level orchestration, memory, and human-in-the-loop support.Learn moreDeep Agents (TypeScript)Quickly build agents that can tackle any task.Learn moreLangChain (TypeScript)Customize the building blocks of your agents.Learn moreLangGraph (TypeScript)Control every step of your custom agent with low-level orchestration, memory, and human-in-the-loop support.Learn moreGet startedBuild your first agent with LangChainGet startedSign up for LangSmithTry LangSmithBuild an advanced agent with LangGraphGet startedEnroll in LangChain AcademyGet startedAdditional resourcesCommunity forumAsk questions, share solutions, and discuss best practices.JoinCommunity SlackConnect with other builders and get quick help.JoinSupport portalSubmit tickets and track support requests.VisitLangSmith statusReal-time status of LangSmith services and APIs.View\n",
      "\n",
      "Edit this page on GitHub or file an issue.\n",
      "Connect these docs to Claude, VSCode, and more via MCP for real-time answers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "web_loader = WebBaseLoader(\"https://docs.langchain.com/\")\n",
    "web_data = web_loader.load()\n",
    "print(web_data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b5922c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200,chunk_overlap=20,separator='\\n')\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e3b3577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Splitter 1 Statistics ===\n",
      "Total number of chunks: 95\n",
      "Average chunk size: 263.80 characters\n",
      "Metadata keys preserved: creationdate, page, creator, total_pages, page_label, moddate, source, author, producer, title\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): comprehensive support within the field of mental health. \n",
      "Additionally, the paper discusses the implementation of \n",
      "Streamlit to enhance the user ex pe...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 49 characters\n",
      "Max chunk size: 299 characters\n",
      "\n",
      "=== Splitter 2 Statistics ===\n",
      "Total number of chunks: 57\n",
      "Average chunk size: 452.74 characters\n",
      "Metadata keys preserved: creationdate, page, creator, total_pages, page_label, moddate, source, author, producer, title\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): with severe intellectual disorders do no longer have get entry \n",
      "to the necessary remedy they require. This remedy gap \n",
      "intensifies the weight of intel...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 120 characters\n",
      "Max chunk size: 497 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document \n",
    "from langchain_community.document_loaders import PyPDFLoader,WebBaseLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "\n",
    "paper_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\"\n",
    "pdf_loader = PyPDFLoader(paper_url)\n",
    "pdf_docs= pdf_loader.load()\n",
    "\n",
    "web_url = \"https://docs.langchain.com/\"\n",
    "web_loader = WebBaseLoader(web_url)\n",
    "web_docs = web_loader.load()\n",
    "splitter_1 = CharacterTextSplitter(chunk_size=300,chunk_overlap=30,separator='\\n')\n",
    "splitter_2 = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50,separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
    "chunks_1 = splitter_1.split_documents(pdf_docs)\n",
    "chunks_2 = splitter_2.split_documents(pdf_docs)\n",
    "\n",
    "\n",
    "def display_document_stats(docs, name):\n",
    "    \"\"\"Display statistics about a list of document chunks\"\"\"\n",
    "    total_chunks = len(docs)\n",
    "    total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "    avg_chunk_size = total_chars / total_chunks if total_chunks > 0 else 0\n",
    "    \n",
    "    # Count unique metadata keys across all documents\n",
    "    all_metadata_keys = set()\n",
    "    for doc in docs:\n",
    "        all_metadata_keys.update(doc.metadata.keys())\n",
    "    \n",
    "    # Print the statistics\n",
    "    print(f\"\\n=== {name} Statistics ===\")\n",
    "    print(f\"Total number of chunks: {total_chunks}\")\n",
    "    print(f\"Average chunk size: {avg_chunk_size:.2f} characters\")\n",
    "    print(f\"Metadata keys preserved: {', '.join(all_metadata_keys)}\")\n",
    "    \n",
    "    if docs:\n",
    "        print(\"\\nExample chunk:\")\n",
    "        example_doc = docs[min(5, total_chunks-1)]  # Get the 5th chunk or the last one if fewer\n",
    "        print(f\"Content (first 150 chars): {example_doc.page_content[:150]}...\")\n",
    "        print(f\"Metadata: {example_doc.metadata}\")\n",
    "        \n",
    "        # Calculate length distribution\n",
    "        lengths = [len(doc.page_content) for doc in docs]\n",
    "        min_len = min(lengths)\n",
    "        max_len = max(lengths)\n",
    "        print(f\"Min chunk size: {min_len} characters\")\n",
    "        print(f\"Max chunk size: {max_len} characters\")\n",
    "\n",
    "# Display stats for both chunk sets\n",
    "display_document_stats(chunks_1, \"Splitter 1\")\n",
    "display_document_stats(chunks_2, \"Splitter 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e86c00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 57\n",
      "ChromaDB created\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma \n",
    "\n",
    "paper_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\"\n",
    "pdf_loader = PyPDFLoader(paper_url)\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                          chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(\"Chunks:\",len(chunks))\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=chunks,\n",
    "                      embedding=embedding_model,\n",
    "                      persist_directory=\"./chroma_db\")\n",
    "\n",
    "print(\"ChromaDB created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48b857fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:1\n",
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether you\n",
      "\n",
      "Result:2\n",
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether you\n",
      "\n",
      "Result:3\n",
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether you\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Langchain used for?\"\n",
    "\n",
    "docs = vector_store.similarity_search(query,k=3)\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "    print(f\"\\nResult:{i + 1}\")\n",
    "    print(doc.page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d39216a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='and human. The conclusion is drawn in Section V. \n",
      "II. LANGCHAIN \n",
      "LangChain, with its open -source essence, emerges as a \n",
      "promising solution, aiming to simplify the complex process of \n",
      "developing applications powered by large language models \n",
      "(LLMs). This framework though the rapid delivery of building \n",
      "blocks and pre-built chains for building large language model \n",
      "applications shows the easy way developers can do it.' metadata={'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'creationdate': '2023-12-31T03:50:13+00:00', 'page_label': '1', 'page': 0, 'creator': 'Microsoft Word', 'moddate': '2023-12-31T03:52:06+00:00', 'author': 'IEEE', 'producer': 'PyPDF', 'total_pages': 6}\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "docs = retriever.invoke(\"Langchain\")\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c609f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c4f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq \n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\",\n",
    "               max_tokens=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b1b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800,\n",
    "                                          chunk_overlap=100,\n",
    "                                          separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
    "\n",
    "splits = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0c5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma \n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = Chroma.from_documents(documents=splits,\n",
    "                                     embedding=embedding_model,\n",
    "                                     collection_name=\"pdf_rag\")\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "579833e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant.\n",
    "    Answer the question ONLY uisng the context below. \n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\":itemgetter(\"question\") | retriever | format_docs,\n",
    "        \"question\":itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2a49f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper focuses on the urgent mental‑health crisis—particularly the strong link between mental disorders and suicide—and proposes using deep‑learning, natural‑language‑processing techniques to detect suicidal ideation early. It highlights how contextualized pre‑trained language models can be harnessed for effective early identification of at‑risk individuals, thereby helping to prevent suicide and improve mental‑health support.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "# InMemoryChatMessageHistory stores chat history in RAM\n",
    "# RunnableWithMessageHistory wraps chain/agent this\n",
    "# automatically adds chat history to every request \n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chat_chain = RunnableWithMessageHistory(rag_chain,\n",
    "                                        get_session_history,\n",
    "                                        input_messages_key=\"question\",\n",
    "                                        history_messages_key=\"history\")\n",
    "\n",
    "chat_chain.invoke({\"question\":\"What is this paper about?\"},\n",
    "                  config={\"configurable\":{\"session_id\":\"1\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9579eab4",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "#### **Building a Simple Retrieval System with LangChain**\n",
    "\n",
    "In this exercise, you'll implement a simple retrieval system using LangChain's vector store and retriever components to help answer questions based on a document.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the necessary components for document loading, embedding, and retrieval.\n",
    "2. Load the provided document about artificial intelligence.\n",
    "3. Split the document into manageable chunks.\n",
    "4. Use an embedding model to create vector representations.\n",
    "5. Create a vector store and a retriever.\n",
    "6. Implement a simple question-answering system.\n",
    "7. Test your system with at least 3 different questions.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70599f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is LangChain?\n",
      "A: LangChain overview - Docs by LangChain\n",
      "--------------------------------------------------\n",
      "Q: How do retrievers work?\n",
      "A: I’m sorry, but the provided context does not contain any information about how retrievers work.\n",
      "--------------------------------------------------\n",
      "Q: Why is document splitting important?\n",
      "A: The context provided does not discuss document splitting, so I’m unable to answer that question based on the given information.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200,\n",
    "                                               chunk_overlap=20,\n",
    "                                               separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=chunks,\n",
    "                                     embedding=embedding_model,\n",
    "                                     collection_name=\"simple-rag\")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant.\n",
    "    Answer ONLY using the context below.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question:\n",
    "    {question}\"\"\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\":itemgetter(\"question\") | retriever | format_docs,\n",
    "        \"question\":itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "test_queries = [\n",
    "    \"What is LangChain?\",\n",
    "    \"How do retrievers work?\",\n",
    "    \"Why is document splitting important?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", rag_chain.invoke({\"question\": q}))\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
