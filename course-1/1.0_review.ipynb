{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6827e697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d2e3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq \n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87324410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Try Dashiell\\u202fHammett’s *The Maltese Falcon*—a gritty, atmospheric mystery that’s a cornerstone of the genre and will keep you guessing until the very last page.' additional_kwargs={'reasoning_content': 'We need to respond in one sentence. The user: \"I enjoy mystery novels, what should I read?\" We must give a recommendation. Should we ask clarifying? No, the instruction: \"You are a helpful AI bot that assists a user in choosing the perfect book to read in one sentence\". So we need to give a recommendation in one sentence. Probably mention a popular mystery novel. Could mention \"The Girl with the Dragon Tattoo\" or \"Gone Girl\" or \"The Hound of the Baskervilles\". But maybe pick something classic like \"The Maltese Falcon\" or \"The Big Sleep\". We can also suggest something like \"The Silent Patient\" but that\\'s more psychological thriller. The user didn\\'t specify subgenre. Maybe give a recommendation that covers intrigue. I should mention one book and maybe a brief reason. But keep it one sentence. Example: \"Try Agatha Christie’s \\'And Then There Were None\\'—a masterful whodunnit that will keep you guessing until the very last page.\" That\\'s one sentence. Or \"If you like classic detective work, try Dashiell Hammett’s \\'The Maltese Falcon\\'—a gritty, atmospheric mystery that’s a cornerstone of the genre.\" That\\'s one sentence. Let\\'s do that.'} response_metadata={'token_usage': {'completion_tokens': 302, 'prompt_tokens': 104, 'total_tokens': 406, 'completion_time': 0.31802254, 'completion_tokens_details': {'reasoning_tokens': 255}, 'prompt_time': 0.005431578, 'prompt_tokens_details': None, 'queue_time': 0.043741162, 'total_time': 0.323454118}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_5c8ca06ea1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019ca38f-012c-75d3-bb9d-7d953c0215f3-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 104, 'output_tokens': 302, 'total_tokens': 406, 'output_token_details': {'reasoning': 255}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "\n",
    "msg = llm.invoke([\n",
    "    SystemMessage(content=\"You are a helpful AI bot that assists a user in choosing the perfect book to read in one sentence\"),\n",
    "    HumanMessage(content=\"I enjoy mystery novels, what should I read?\")\n",
    "])\n",
    "\n",
    "print(msg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f56d3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Aim for 2–3 CrossFit sessions per week, leaving at least one rest or lighter‑day in between.' additional_kwargs={'reasoning_content': 'The user: \"How often should I attend this class?\" The user wants a recommendation. The system says supportive AI bot suggests fitness activities. We can give a short sentence: \"Aim for 2–3 sessions per week, spaced by rest days.\" The user asked \"How often should I attend this class?\" So answer: 2-3 times per week. Should we mention rest? It\\'s supportive. So a short sentence.'} response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 127, 'total_tokens': 248, 'completion_time': 0.12206148, 'completion_tokens_details': {'reasoning_tokens': 88}, 'prompt_time': 0.007180719, 'prompt_tokens_details': None, 'queue_time': 0.043175631, 'total_time': 0.129242199}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019ca38f-0312-7982-8d34-45d3418bd225-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 127, 'output_tokens': 121, 'total_tokens': 248, 'output_token_details': {'reasoning': 88}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "msg = llm.invoke([\n",
    "    SystemMessage(content=\"You are a supportive AI bot that suggests fitness activities to a user in a short sentence\"),\n",
    "    HumanMessage(content=\"I like high intensity workouts, what should I do?\"),\n",
    "    AIMessage(content=\"You should try a crossfit class\"),\n",
    "    HumanMessage(content=\"How often should I attend this class?\")\n",
    "])\n",
    "\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "257934ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me funny joke about Spider-Man')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# when we use the PromptTemplate.from_template lc automatically detects the input_variables\n",
    "# if we use PromptTemplate base class we need to specify separately\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Tell me {adjective} joke about {topic}\")\n",
    "input_ = {\"adjective\":\"funny\",\"topic\":\"Spider-Man\"}\n",
    "prompt.invoke(input_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2404efc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about this Spider-Man', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    (\"user\",\"Tell me a joke about this {topic}\")\n",
    "])\n",
    "\n",
    "input_ = {\"topic\":\"Spider-Man\"}\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b5a920c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the day after Tuesday?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "input_ = {\"msgs\":[HumanMessage(content=\"What is the day after Tuesday?\")]}\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ac6326f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The day after Tuesday is **Wednesday**.', additional_kwargs={'reasoning_content': 'User asks: \"What is the day after Tuesday?\" It\\'s a simple question: Wednesday. We answer: Wednesday.'}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 86, 'total_tokens': 128, 'completion_time': 0.042387637, 'completion_tokens_details': {'reasoning_tokens': 24}, 'prompt_time': 0.004136588, 'prompt_tokens_details': None, 'queue_time': 0.043538618, 'total_time': 0.046524225}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_deb540145b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019ca38f-0434-7e22-90bb-bc8e731acf8e-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 86, 'output_tokens': 42, 'total_tokens': 128, 'output_token_details': {'reasoning': 24}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "response = chain.invoke(input_)\n",
    "response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad9138e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel,Field \n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to setup a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "joke_query = \"Tell me joke.\"\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=Joke)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\":format_instructions}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "response = chain.invoke({\"query\":joke_query})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a8e89b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hyderabadi Chicken Biryani',\n",
       " 'Lucknowi Chicken Biryani',\n",
       " 'Kolkata Chicken Biryani',\n",
       " 'Bhopali Chicken Biryani',\n",
       " 'Malabar Chicken Biryani']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query. {format_instructions}\\nList five {subject}.\",\n",
    "    input_variables=['subject'],\n",
    "    partial_variables={\"format_instructions\":format_instructions}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"subject\":\"best chicken biryanis\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "952114ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Spider-Man: Into the Spider-Verse',\n",
       " 'director': 'Bob Persichetti, Peter Ramsey, Rodney Rothman',\n",
       " 'year': '2018',\n",
       " 'genre': 'Animation, Action, Adventure, Superhero'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import Field,BaseModel\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    title: str = Field(description=\"Movie title\")\n",
    "    director: str = Field(description=\"Movie directors name\")\n",
    "    year: int = Field(description=\"Movie release year\")\n",
    "    genre: str = Field(description=\"Movie genre\")\n",
    "\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "format_instructions = \"\"\"RESPONSE FORMAT: Return ONLY a single JSON object-no markdown, no example, no extra keys. It must look exactly like:\n",
    "{\n",
    "  \"title\":\"movie title\",\n",
    "  \"director\":\"director name\",\n",
    "  \"year\":\"release year\",\n",
    "  \"genre\":\"movie genre\"\n",
    "}\n",
    "\n",
    "IMPORTANT: Your response must be *only* JSON. Do NOT include any illustrate or example JSON.\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"You are a JSON only assistant.\n",
    "    \n",
    "    Task: Generate info about the {movie_name} in JSON format.\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"movie_name\"],\n",
    "    partial_variables={\"format_instructions\":format_instructions}\n",
    ")\n",
    "\n",
    "movie_chain = prompt_template | llm | output_parser\n",
    "\n",
    "movie_name = \"Spider-Man:Into The Spider-Verse\"\n",
    "result = movie_chain.invoke({\"movie_name\":movie_name})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95714879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fde63b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* corresponding author - jkim72@kent.edu \n",
      "Revolutionizing Mental Health Care through \n",
      "LangChain: A Journey with a Large Language \n",
      "Model\n",
      "Aditi Singh \n",
      " Computer Science  \n",
      " Cleveland State University  \n",
      " a.singh22@csuohio.edu \n",
      "Abul Ehtesham  \n",
      "The Davey Tree Expert \n",
      "Company  \n",
      "abul.ehtesham@davey.com \n",
      "Saifuddin Mahmud  \n",
      "Computer Science & \n",
      "Information Systems  \n",
      " Bradley University  \n",
      "smahmud@bradley.edu  \n",
      "Jong-Hoon Kim* \n",
      " Computer Science,  \n",
      "Kent State University,  \n",
      "jkim72@kent.edu \n",
      "Abstract— Mental health challenges are on the rise in our \n",
      "modern society, and the imperative to address mental disorders, \n",
      "especially regarding anxiety, depression, and suicidal thoughts, \n",
      "underscores the need for effective interventions. This paper \n",
      "delves into the application of recent advancements in pretrained \n",
      "contextualized language models to introduce MindGuide, an \n",
      "innovative chatbot serving as a mental health assistant for \n",
      "individuals seeking guidance and support in these critical areas. \n",
      "MindGuide lever ages the capabilities of LangChain and its \n",
      "ChatModels, specifically Chat OpenAI, as the bedrock of its \n",
      "reasoning engine. The system incorporates key features such as \n",
      "LangChain's ChatPrompt Template, HumanMessage  Prompt \n",
      "Template, ConversationBufferMemory, and LLMChain, \n",
      "creating an advanced solution for early detection and \n",
      "comprehensive support within the field of mental health. \n",
      "Additionally, the paper discusses the implementation of \n",
      "Streamlit to enhance the user ex perience and interaction with \n",
      "the chatbot. Th is novel approach holds great promise for \n",
      "proactive mental health intervention and assistance. \n",
      "Keywords —Large Language models , LangChain, Chatbot, \n",
      "Pretrained models, Mental health, Mental health support. \n",
      "I. INTRODUCTION \n",
      "The issue of mental health is an international situation, \n",
      "affecting people in each particularly developed nations and \n",
      "emerging markets. According to the World Health \n",
      "Organization's Mental Health Action Plan (2013-2020), it's far \n",
      "estimated that around one in four humans international face  \n",
      "numerous kinds of mental disorders. This statistic underscores \n",
      "the vast nature of mental health demanding situations \n",
      "throughout extraordinary demographic businesses and areas. \n",
      "However, what makes this situation even extra complex is \n",
      "the concerning truth that three out of each four people dealing \n",
      "with severe intellectual disorders do no longer have get entry \n",
      "to the necessary remedy they require. This remedy gap \n",
      "intensifies the weight of intellectual health troubles, leaving a \n",
      "sizable part of the populace without the assist and care needed \n",
      "to efficiently address their intellectual health issues. \n",
      "Furthermore, periods like the recent global pandemic, the \n",
      "effect of mental health issues becomes even more said. The \n",
      "COVID-19 pandemic, in particular, has highlighted how \n",
      "public health crises can extensively have an effect on mental \n",
      "properly-being. During such hard instances, a widespread part \n",
      "of the population faces extended problems in having access to \n",
      "mental fitness professionals. This emphasizes the urgent want \n",
      "for progressed intellectual health offerings and support \n",
      "structures. It underscores the urgency of addressing the mental \n",
      "health disaster and developing complete answers to make \n",
      "certain that people global have the means to successfully deal \n",
      "with their mental fitness challenges.. \n",
      "In studies [1], it's pretty clear that there's a deep connection \n",
      "between mental troubles and the chances of someone taking \n",
      "their own life. And when you look at the big picture, it's quite \n",
      "shocking - nearly a million people across the globe end their \n",
      "lives every year, especially the young ones, making it the \n",
      "second biggest reason for their passing . It's intriguing that \n",
      "when someone attempts suicide, they often grapple with \n",
      "mental challenges. It's like shifting from struggling with \n",
      "difficult thoughts to considering ending everything. This shift \n",
      "is observable in how people express themselves and \n",
      "interact[2]. \n",
      "One practical approach to addressing mental illness and \n",
      "preventing suicidal ideation is early identification. Recent \n",
      "advancements in deep learning have facilitated the \n",
      "development of effective early detection methods [3]. A \n",
      "notable trend in natural language processing (NLP)  involves \n",
      "the use of contextualized pretrained language models  [4], \n",
      "which have garnered substantial attention for their \n",
      "effectiveness in various text processing tasks. \n",
      "This paper delves into the application of these recent \n",
      "advancements in pretrained contextualized large language \n",
      "models to introduce MindGuide, an innovative chatbot \n",
      "designed to function as a mental health assistant for \n",
      "individuals in need of guidance and support in these critical \n",
      "areas. MindGuide relies on the capabilities of LangChain and \n",
      "its ChatModels  [5], specifically Chat OpenAI [6], as the \n",
      "foundation of its reasoning engine. The system incorporates \n",
      "key components such as LangChain’s ChatPrompt Template \n",
      "[7], HumanMessage , PromptTemplate, ConversationBuffer \n",
      "Memory, and LLMChain [8], creating an advanced solution \n",
      "for early detection and comprehensive support within the field \n",
      "of mental health. Additionally, the paper discusses the \n",
      "implementation of Streamlit to enhance the user experience \n",
      "and interaction with the chatbot. \n",
      "The remainder of the paper is arranged accordingly. In \n",
      "Section II, LangChain and its important components  are \n",
      "introduced. T he proposed methodology for developing the \n",
      "whole architecture is described  in Section III . Section IV \n",
      "provides an overview of Streamlit. Section V provides an \n",
      "illustration of sequential interaction of MindGuide chatbot \n",
      "and human. The conclusion is drawn in Section V. \n",
      "II. LANGCHAIN \n",
      "LangChain, with its open -source essence, emerges as a \n",
      "promising solution, aiming to simplify the complex process of \n",
      "developing applications powered by large language models \n",
      "(LLMs). This framework though the rapid delivery of building \n",
      "blocks and pre-built chains for building large language model \n",
      "applications shows the easy way developers can do it.\n"
     ]
    }
   ],
   "source": [
    "print(document[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb4f63c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain overview - Docs by LangChainSkip to main contentDocs by LangChain home pageOpen sourceSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewDeep AgentsLangChainLangGraphIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewPrebuilt middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Create an agent Core benefitsLangChain overviewCopy pageLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool — so you can build agents that adapt as fast as the ecosystem evolvesCopy pageLangChain is the easy way to start building completely custom agents a\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")\n",
    "web_data = loader.load()\n",
    "print(web_data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "941c0da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitters = CharacterTextSplitter(chunk_size=200,chunk_overlap=20,separator=\"\\n\")\n",
    "chunks = text_splitters.split_documents(document)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8093cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Splitter 1 Statistics ===\n",
      "Total number of chunks: 95\n",
      "Average chunk size: 263.80 characters\n",
      "Metadata keys preserved: creator, total_pages, page_label, author, source, moddate, creationdate, page, title, producer\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): comprehensive support within the field of mental health. \n",
      "Additionally, the paper discusses the implementation of \n",
      "Streamlit to enhance the user ex pe...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 49 characters\n",
      "Max chunk size: 299 characters\n",
      "\n",
      "=== Splitter 2 Statistics ===\n",
      "Total number of chunks: 57\n",
      "Average chunk size: 452.74 characters\n",
      "Metadata keys preserved: creator, total_pages, page_label, author, source, moddate, creationdate, page, title, producer\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): with severe intellectual disorders do no longer have get entry \n",
      "to the necessary remedy they require. This remedy gap \n",
      "intensifies the weight of intel...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 120 characters\n",
      "Max chunk size: 497 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader,WebBaseLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "\n",
    "paper_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\"\n",
    "pdf_loader = PyPDFLoader(paper_url)\n",
    "pdf_document = pdf_loader.load()\n",
    "\n",
    "web_url =  \"https://docs.langchain.com/\"\n",
    "\n",
    "web_loader = WebBaseLoader(web_url)\n",
    "web_document = web_loader.load()\n",
    "\n",
    "splitter_1 = CharacterTextSplitter(chunk_size=300,chunk_overlap=30,separator=\"\\n\")\n",
    "splitter_2 = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50,separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
    "\n",
    "chunks_1 = splitter_1.split_documents(pdf_document)\n",
    "chunks_2 = splitter_2.split_documents(pdf_document)\n",
    "\n",
    "def display_document_stats(docs, name):\n",
    "    \"\"\"Display statistics about a list of document chunks\"\"\"\n",
    "    total_chunks = len(docs)\n",
    "    total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "    avg_chunk_size = total_chars / total_chunks if total_chunks > 0 else 0\n",
    "    \n",
    "    # Count unique metadata keys across all documents\n",
    "    all_metadata_keys = set()\n",
    "    for doc in docs:\n",
    "        all_metadata_keys.update(doc.metadata.keys())\n",
    "    \n",
    "    # Print the statistics\n",
    "    print(f\"\\n=== {name} Statistics ===\")\n",
    "    print(f\"Total number of chunks: {total_chunks}\")\n",
    "    print(f\"Average chunk size: {avg_chunk_size:.2f} characters\")\n",
    "    print(f\"Metadata keys preserved: {', '.join(all_metadata_keys)}\")\n",
    "    \n",
    "    if docs:\n",
    "        print(\"\\nExample chunk:\")\n",
    "        example_doc = docs[min(5, total_chunks-1)]  # Get the 5th chunk or the last one if fewer\n",
    "        print(f\"Content (first 150 chars): {example_doc.page_content[:150]}...\")\n",
    "        print(f\"Metadata: {example_doc.metadata}\")\n",
    "        \n",
    "        # Calculate length distribution\n",
    "        lengths = [len(doc.page_content) for doc in docs]\n",
    "        min_len = min(lengths)\n",
    "        max_len = max(lengths)\n",
    "        print(f\"Min chunk size: {min_len} characters\")\n",
    "        print(f\"Max chunk size: {max_len} characters\")\n",
    "\n",
    "# Display stats for both chunk sets\n",
    "display_document_stats(chunks_1, \"Splitter 1\")\n",
    "display_document_stats(chunks_2, \"Splitter 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f39caa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks: 57\n",
      "Chromadb created\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "paper_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\"\n",
    "pdf_loader = PyPDFLoader(paper_url)\n",
    "docs = pdf_loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(\"chunks:\",len(chunks))\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = Chroma.from_documents(documents=chunks,\n",
    "                                     embedding=embedding_model,\n",
    "                                     persist_directory=\"./chroma_db\")\n",
    "\n",
    "print(\"Chromadb created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "952f8c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:1\n",
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether you\n",
      "\n",
      "Results:2\n",
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether you\n",
      "\n",
      "Results:3\n",
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether you\n"
     ]
    }
   ],
   "source": [
    "query = \"What is LangChain is used for?\"\n",
    "docs = vector_store.similarity_search(query,k=3)\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "    print(f\"\\nResults:{i + 1}\")\n",
    "    print(doc.page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d537397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "texts = [doc.page_content for doc in chunks]\n",
    "embeddings = embedding_model.embed_documents(texts)\n",
    "print(len(embeddings))\n",
    "print(len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9619769b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x0000019A8CA3AF60>, search_kwargs={})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3db5eec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='870b2de9-df4e-4655-b936-0f6b0935dd1c', metadata={'total_pages': 6, 'title': 's8329 final', 'creator': 'Microsoft Word', 'author': 'IEEE', 'creationdate': '2023-12-31T03:50:13+00:00', 'page': 0, 'page_label': '1', 'producer': 'PyPDF', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'moddate': '2023-12-31T03:52:06+00:00'}, page_content='and human. The conclusion is drawn in Section V. \\nII. LANGCHAIN \\nLangChain, with its open -source essence, emerges as a \\npromising solution, aiming to simplify the complex process of \\ndeveloping applications powered by large language models \\n(LLMs). This framework though the rapid delivery of building \\nblocks and pre-built chains for building large language model \\napplications shows the easy way developers can do it.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.invoke(\"LangChain\")\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b76a988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5e86c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800,chunk_overlap=100,separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
    "splits = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a186fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = Chroma.from_documents(documents=splits,\n",
    "                                     embedding=embeddings,\n",
    "                                     collection_name=\"pdf_rag\")\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fbc8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2da22a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    template=\n",
    "    \"\"\"\n",
    "    You are a helpful assistant. \n",
    "    Answer the question ONLY using the context below.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\":itemgetter(\"question\") | retriever | format_docs,\n",
    "        \"question\":itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c54879c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper examines the urgent link between mental health problems and suicide risk, highlighting how many people—especially young adults—die or attempt suicide each year. It argues that early identification of suicidal ideation is a practical way to reduce these deaths. The study reviews recent advances in deep‑learning and natural‑language‑processing techniques, focusing on how contextualized, pretrained language models can be leveraged to detect warning signs of suicidal thoughts in textual data. In short, the paper proposes that cutting‑edge NLP and AI methods can help clinicians spot at‑risk individuals sooner and intervene more effectively.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "# RunnableWithMessageHistory: wrapper that adds memory to the langchain agent\n",
    "# InMemoryChatMessageHistory: memory storage class that stores chat messages in RAM \n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chat_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "chat_chain.invoke({\"question\":\"What is the paper about?\"},\n",
    "                  config={\"configurable\":{\"session_id\":\"1\"}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ca25672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is LangChain?\n",
      "A: LangChain is a framework that lets you build applications powered by large language models (LLMs). It provides a pre‑built agent architecture built on top of LangGraph, enabling features such as durable execution, streaming, human‑in‑the‑loop interactions, and persistence. The framework also supplies easy model integrations so you can quickly incorporate LLMs into your agents and applications.\n",
      "--------------------------------------------------\n",
      "Q: How do retrievers work?\n",
      "A: The provided context does not include details about how retrievers work.\n",
      "--------------------------------------------------\n",
      "Q: Why is document splitting important?\n",
      "A: The context does not mention document splitting.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200,\n",
    "                                               chunk_overlap=20,\n",
    "                                               separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=chunks,\n",
    "                                     embedding=embedding_model,\n",
    "                                     collection_name=\"simple-rag\")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant. \n",
    "    Answer ONLY using the context below. \n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\":itemgetter(\"question\") | retriever | format_docs,\n",
    "        \"question\":itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "test_queries = [\n",
    "    \"What is LangChain?\",\n",
    "    \"How do retrievers work?\",\n",
    "    \"Why is document splitting important?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", rag_chain.invoke({\"question\": q}))\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d98b3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyderabad itself is a city, so it doesn’t have a “capital” in the way a country or state does. However, Hyderabad is the capital city of the Indian state of **Telangana** (and it was also the capital of the former state of Andhra Pradesh until 2014).\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "history.add_ai_message(\"hello!\")\n",
    "\n",
    "history.add_user_message(\"What is the capital of Hyderabad?\")\n",
    "\n",
    "# print(history.messages)\n",
    "response = llm.invoke(history.messages)\n",
    "print(response.content)\n",
    "\n",
    "history.add_ai_message(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16237baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hello!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='What is the capital of Hyderabad?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hyderabad itself is a city, so it doesn’t have a “capital” in the way a country or state does. However, Hyderabad is the capital city of the Indian state of **Telangana** (and it was also the capital of the former state of Andhra\\u202fPradesh until 2014).', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0eccbd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\",\"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed7d63a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7032fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f597165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "chat_chain = RunnableWithMessageHistory(chain,\n",
    "                                        get_session_history,\n",
    "                                        input_messages_key=\"input\",\n",
    "                                        history_messages_key=\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef666846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You’re a Siberian tiger—one of the world’s largest and most iconic big‑cat species, native to the forests and grasslands of the Russian Far East. Known for its powerful build, striking golden coat with black stripes, and majestic presence, the Siberian tiger is a symbol of strength and natural beauty.', additional_kwargs={'reasoning_content': 'The user says: \"Who am I?\" They previously said \"Hello, I am a Siberian Tiger. Who are you?\" They responded \"Hello, I\\'m ChatGPT\". Now they ask \"Who am I?\" They might be expecting the assistant to answer that they are a Siberian tiger, as per the earlier statement. But we need to handle the context: The user claims to be a Siberian tiger. So the assistant can confirm that they are a Siberian tiger. But we should also be mindful of the policy: The assistant should not be defaming or providing disallowed content. No disallowed content. It\\'s fine. We can respond: \"You are a Siberian tiger.\" But perhaps we can add a bit of context. The user may want a more elaborate answer. But the user is basically asking for identity. We can respond that they are a Siberian tiger. But we can also ask clarifying questions: \"Are you a real Siberian tiger or a fictional representation?\" But the user says \"Hello, I am a Siberian Tiger.\" So we can respond accordingly.\\n\\nWe should keep it short, but also friendly. We can say: \"You are a Siberian tiger, the largest cat species native to the Russian Far East, known for its power and beauty.\" That is a helpful answer. Let\\'s comply.'}, response_metadata={'token_usage': {'completion_tokens': 343, 'prompt_tokens': 602, 'total_tokens': 945, 'completion_time': 0.350378343, 'completion_tokens_details': {'reasoning_tokens': 270}, 'prompt_time': 0.031744288, 'prompt_tokens_details': None, 'queue_time': 0.043647442, 'total_time': 0.382122631}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_5c8ca06ea1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019ca38f-914d-7c33-8c95-bbdc82a2bf5b-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 602, 'output_tokens': 343, 'total_tokens': 945, 'output_token_details': {'reasoning': 270}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation \n",
    "chat_chain.invoke(\n",
    "    {\"input\": \"Hello, I am a Siberian Tiger. Who are you?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}}\n",
    ")\n",
    "\n",
    "chat_chain.invoke(\n",
    "    {\"input\": \"What can you do?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}}\n",
    ")\n",
    "\n",
    "chat_chain.invoke(\n",
    "    {\"input\": \"Who am I?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8814c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Hello, my name is Alice.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello, Alice.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\",\n",
    "               max_tokens=256,\n",
    "               temperature=0.2)\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"Hello, my name is Alice.\")\n",
    "history.add_ai_message(\"Hello, Alice.\")\n",
    "print(history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fc0e382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Beginning Chat Simulation ===\n",
      "\n",
      "--- Turn 1 ---\n",
      "Human: My favorite color is blue.\n",
      "AI: That’s a classic choice! Blue often feels calm and refreshing. Do you have a particular shade you love most—like navy, sky blue, or something else?\n",
      "\n",
      "--- Turn 2 ---\n",
      "Human: I enjoy hiking in the mountains.\n",
      "AI: That’s awesome! The mountains have such a unique mix of challenge and serenity. Do you have a favorite range or trail you’ve tackled recently? And if you’re looking for new routes or gear tips, I’d love to help out!\n",
      "\n",
      "--- Turn 3 ---\n",
      "Human: What activities would you recommend for me?\n",
      "AI: Here are a few ideas that blend your love of mountain hiking with fresh, blue‑tinged adventures:\n",
      "\n",
      "| Activity | Why it fits | Quick Tips |\n",
      "|----------|-------------|------------|\n",
      "| **Mountain Biking** | Combines the trail‑loving vibe with a bit of speed. Many trails wind past alpine lakes that sparkle blue. | Start with a beginner trail, bring a helmet, and pack a small first‑aid kit. |\n",
      "| **Rock Climbing / Bouldering** | Gives you a new perspective on the same terrain—climb up the cliffs you hike past. | Take a local class to learn proper gear and safety. |\n",
      "| **Backpacking with a Blue‑Themed Pack** | Pack a bright blue backpack or gear to add a splash of color to your trek. | Choose a durable, weather‑proof pack; add a\n",
      "\n",
      "--- Turn 4 ---\n",
      "Human: What was my favorite color again?\n",
      "AI: Your favorite color is **blue**.\n",
      "\n",
      "--- Turn 5 ---\n",
      "Human: Can you remember both my name and my favorite color?\n",
      "AI: I know your favorite color is **blue**, but I don’t have your name on record. If you’d like me to remember it, just let me know!\n",
      "\n",
      "=== End of Chat Simulation ===\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\",\"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chat_chain = RunnableWithMessageHistory(chain,\n",
    "                                        get_session_history,\n",
    "                                        input_messages_key=\"input\",\n",
    "                                        history_messages_key=\"history\")\n",
    "\n",
    "def chat_simulation(conversation, inputs, session_id=\"1\"):\n",
    "    print(\"\\n=== Beginning Chat Simulation ===\")\n",
    "    \n",
    "    for i, user_input in enumerate(inputs):\n",
    "        print(f\"\\n--- Turn {i+1} ---\")\n",
    "        print(f\"Human: {user_input}\")\n",
    "        \n",
    "        response = conversation.invoke(\n",
    "            {\"input\": user_input},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        \n",
    "        print(f\"AI: {response}\")\n",
    "    \n",
    "    print(\"\\n=== End of Chat Simulation ===\")\n",
    "\n",
    "# 6. Test with a series of related questions\n",
    "test_inputs = [\n",
    "    \"My favorite color is blue.\",\n",
    "    \"I enjoy hiking in the mountains.\",\n",
    "    \"What activities would you recommend for me?\",\n",
    "    \"What was my favorite color again?\",\n",
    "    \"Can you remember both my name and my favorite color?\"\n",
    "]\n",
    "\n",
    "chat_simulation(chat_chain, test_inputs,session_id=\"1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "285cc168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'location': 'China',\n",
      " 'meal': '**Peking Duck** – a celebrated classic from Beijing, known for its '\n",
      "         'crispy skin, tender meat, and the tradition of serving it with thin '\n",
      "         'pancakes, scallions, cucumber, and sweet bean sauce.',\n",
      " 'recipe': '**Home‑Style Peking Duck (Quick & Easy)**  \\n'\n",
      "           '*(Makes 2–3 servings)*  \\n'\n",
      "           '\\n'\n",
      "           '---\\n'\n",
      "           '\\n'\n",
      "           '### Ingredients  \\n'\n",
      "           '- 1 whole duck (≈3–4\\u202flb), cleaned and patted dry  \\n'\n",
      "           '- 1 tbsp honey (or 2 tbsp sugar)  \\n'\n",
      "           '- 1 tbsp maltose or corn syrup (optional, for extra crispness)  \\n'\n",
      "           '- 1 tsp salt  \\n'\n",
      "           '- 1 tsp Chinese five‑spice powder  \\n'\n",
      "           '- 1 tsp ground ginger  \\n'\n",
      "           '- 1 tsp ground garlic  \\n'\n",
      "           '- 1 tbsp soy sauce  \\n'\n",
      "           '- 1 tbsp Shaoxing wine (or dry sherry)  \\n'\n",
      "           '- 1 tbsp rice vinegar  \\n'\n",
      "           '- 1 tsp sesame oil  \\n'\n",
      "           '- 1 cup water (for the glaze)  \\n'\n",
      "           '\\n'\n",
      "           '**For serving**  \\n'\n",
      "           '- Thin wheat or rice pancakes (store‑bought or homemade)  \\n'\n",
      "           '- Thinly sliced scallions, cucumber, and shredded carrot  \\n'\n",
      "           '- Sweet',\n",
      " 'time': '**Estimated Total Time:** ~\\u202f2\\u202f½\\u202f–\\u202f3\\u202f'\n",
      "         'hours  \\n'\n",
      "         '*(This is for a whole duck that you roast from scratch. If you’re '\n",
      "         'using a pre‑cooked or frozen duck, the time drops to about 1\\u202f'\n",
      "         '½\\u202fhours.)*\\n'\n",
      "         '\\n'\n",
      "         '| Step | What Happens | Approx. Time |\\n'\n",
      "         '|------|--------------|--------------|\\n'\n",
      "         '| **1. Prep & Clean** | Trim excess fat, pat the duck dry, set '\n",
      "         'aside.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from pprint import pprint\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "location_template = \"\"\"Your job is to come up with a classic dish from the area that suggests {location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "dish_template = \"\"\"Given a meal {meal}, give a short and simple recipe on how to make that dish at home.\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "time_template = \"\"\"Given the recipe {recipe}, estimate how much time I need to cook it.\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "location_chain_lcel = (\n",
    "    PromptTemplate.from_template(location_template)\n",
    "    | llm \n",
    "    | output_parser\n",
    ") \n",
    "\n",
    "dish_chain_lcel = (\n",
    "    PromptTemplate.from_template(dish_template)\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "time_chain_lcel = (\n",
    "    PromptTemplate.from_template(time_template)\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "\n",
    "# Combine all chains into a single workflow using RunnablePassthrough.assign\n",
    "# RunnablePassthrough.assign adds new keys to the input dictionary without removing existing ones\n",
    "\n",
    "overall_chain_lcel = (\n",
    "    # Step 1: Generate a meal based on location and add it to the input dictionary\n",
    "    RunnablePassthrough.assign(meal=lambda x: location_chain_lcel.invoke({\"location\": x[\"location\"]}))\n",
    "    # Step 2: Generate a recipe based on the meal and add it to the input dictionary\n",
    "    | RunnablePassthrough.assign(recipe=lambda x: dish_chain_lcel.invoke({\"meal\": x[\"meal\"]}))\n",
    "    # Step 3: Estimate cooking time based on the recipe and add it to the input dictionary\n",
    "    | RunnablePassthrough.assign(time=lambda x: time_chain_lcel.invoke({\"recipe\": x[\"recipe\"]}))\n",
    ")\n",
    "\n",
    "result = overall_chain_lcel.invoke({\"location\": \"China\"})\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9dd53b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING WITH REVIEW:\n",
      "I absolutely love this coffee maker! It brews quickly and the coffee tastes amazing. \n",
      "The built-in g...\n",
      "\n",
      "\n",
      "LCEL CHAIN RESULTS:\n",
      "==================================================\n",
      "{'review': 'I absolutely love this coffee maker! It brews quickly and the coffee tastes amazing. \\nThe built-in grinder saves me so much time in the morning, and the programmable timer means \\nI wake up to fresh coffee every day. Worth every penny and highly recommended to any coffee enthusiast.', 'sentiment': 'SENTIMENT: positive', 'summary': '- Brews quickly with excellent taste.  \\n- Built‑in grinder saves morning prep time.  \\n- Programmable timer delivers fresh coffee at wake‑up.  \\n- Worth the price and highly recommended for coffee lovers.', 'response': 'Hi there!\\n\\nThank you so much for taking the time to share your experience with our coffee maker. We’re thrilled to hear that it’s become a morning lifesaver for you—especially the quick brew, the built‑in grinder, and the programmable timer that delivers fresh coffee right when you need it. It’s always great to know that our customers feel the product is worth every penny and that it’s earning a spot on their recommendation list.\\n\\nIf you ever have any questions, want to explore additional accessories, or need tips for keeping the grinder in top shape, just let us know. We’re here to help you keep enjoying that perfect cup every day.\\n\\nEnjoy your coffee, and thanks again for the kind words!\\n\\nWarm regards,  \\n[Your Name]  \\nCustomer Success Team'}\n",
      "\n",
      "==================================================\n",
      "TESTING WITH REVIEW:\n",
      "Disappointed with this laptop. It's constantly overheating after just 30 minutes of use, \n",
      "and the ba...\n",
      "\n",
      "\n",
      "LCEL CHAIN RESULTS:\n",
      "==================================================\n",
      "{'review': \"Disappointed with this laptop. It's constantly overheating after just 30 minutes of use, \\nand the battery life is nowhere near the 8 hours advertised - I barely get 3 hours. \\nThe keyboard has already started sticking on several keys after just two weeks. Would not recommend to anyone.\", 'sentiment': 'SENTIMENT: negative', 'summary': '- Overheats after just 30 minutes of use.  \\n- Battery life only about 3\\u202fhours, far below the advertised 8\\u202fhours.  \\n- Keyboard keys start sticking after two weeks.  \\n- Overall, the reviewer would not recommend this laptop', 'response': 'Dear [Customer Name],\\n\\nThank you for taking the time to share your experience with our laptop. I’m truly sorry to hear that it’s been overheating after just 30 minutes, that the battery life is falling far short of the advertised 8\\u202fhours, and that the keyboard has started sticking after only two weeks. I understand how frustrating this must be, especially when you were expecting a reliable, long‑lasting device.\\n\\n**Here’s what we can do to help:**\\n\\n1. **Warranty & Return** – The laptop is still covered under our 12‑month warranty. If you’d like to return it for a full refund or an exchange, simply let us know and we’ll send you a prepaid return label.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "# Sample product reviews for testing\n",
    "positive_review = \"\"\"I absolutely love this coffee maker! It brews quickly and the coffee tastes amazing. \n",
    "The built-in grinder saves me so much time in the morning, and the programmable timer means \n",
    "I wake up to fresh coffee every day. Worth every penny and highly recommended to any coffee enthusiast.\"\"\"\n",
    "\n",
    "negative_review = \"\"\"Disappointed with this laptop. It's constantly overheating after just 30 minutes of use, \n",
    "and the battery life is nowhere near the 8 hours advertised - I barely get 3 hours. \n",
    "The keyboard has already started sticking on several keys after just two weeks. Would not recommend to anyone.\"\"\"\n",
    "\n",
    "# Step 1: Define the prompt templates for each processing step\n",
    "sentiment_template = \"\"\"Analyze the sentiment of the following product review as positive, negative, or neutral.\n",
    "Provide your analysis in the format: \"SENTIMENT: [positive/negative/neutral]\"\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Your analysis:\n",
    "\"\"\"\n",
    "\n",
    "summary_template = \"\"\"Summarize the following product review into 3-5 key bullet points.\n",
    "Each bullet point should be concise and capture an important aspect mentioned in the review.\n",
    "\n",
    "Review: {review}\n",
    "Sentiment: {sentiment}\n",
    "\n",
    "Key points:\n",
    "\"\"\"\n",
    "\n",
    "response_template = \"\"\"Write a helpful response to a customer based on their product review.\n",
    "If the sentiment is positive, thank them for their feedback. If negative, express understanding \n",
    "and suggest a solution or next steps. Personalize based on the specific points they mentioned.\n",
    "\n",
    "Review: {review}\n",
    "Sentiment: {sentiment}\n",
    "Key points: {summary}\n",
    "\n",
    "Response to customer:\n",
    "\"\"\"\n",
    "\n",
    "sentiment_prompt = PromptTemplate.from_template(sentiment_template)\n",
    "summary_prompt = PromptTemplate.from_template(summary_template)\n",
    "response_prompt = PromptTemplate.from_template(response_template)\n",
    "\n",
    "sentiment_chain = (\n",
    "    sentiment_prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summary_chain = (\n",
    "    summary_prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response_chain = (\n",
    "    response_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "lcel_chain = (\n",
    "    RunnablePassthrough.assign(sentiment = lambda x: sentiment_chain.invoke({\"review\":x[\"review\"]}))\n",
    "    | RunnablePassthrough.assign(summary = lambda x: summary_chain.invoke({\"review\":x[\"review\"],\n",
    "                                                                           \"sentiment\":x[\"sentiment\"]}))\n",
    "    | RunnablePassthrough.assign(response = lambda x: response_chain.invoke({\"review\":x[\"review\"],\n",
    "                                                                             \"sentiment\":x[\"sentiment\"],\n",
    "                                                                             \"summary\":x[\"summary\"]}))\n",
    ")\n",
    "\n",
    "def test_chains(review):\n",
    "    \"\"\"Test both chain implementations with the given review\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"TESTING WITH REVIEW:\\n{review[:100]}...\\n\")\n",
    "\n",
    "    \n",
    "    print(\"\\nLCEL CHAIN RESULTS:\")\n",
    "    result = lcel_chain.invoke({\"review\":review})\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(result)\n",
    "\n",
    "# Run tests\n",
    "test_chains(positive_review)\n",
    "test_chains(negative_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3927d872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING WITH REVIEW:\n",
      "I absolutely love this coffee maker! It brews quickly and the coffee tastes amazing. \n",
      "The built-in g...\n",
      "\n",
      "\n",
      "LCEL CHAIN RESULTS:\n",
      "==================================================\n",
      "{'review': 'I absolutely love this coffee maker! It brews quickly and the coffee tastes amazing. \\nThe built-in grinder saves me so much time in the morning, and the programmable timer means \\nI wake up to fresh coffee every day. Worth every penny and highly recommended to any coffee enthusiast.', 'sentiment': 'SENTIMENT: positive', 'summary': '- Brews quickly, delivering a consistently great-tasting cup.  \\n- Built‑in grinder cuts morning prep time significantly.  \\n- Programmable timer ensures fresh coffee right when you wake up.  \\n- Overall value: worth the price and highly recommended for coffee lovers.', 'response': 'Hi there!  \\n\\nThank you so much for taking the time to share your experience. We’re thrilled to hear that our coffee maker is becoming a morning essential for you—especially the quick brew, the built‑in grinder that saves you prep time, and the programmable timer that delivers fresh coffee right when you need it.  \\n\\nIt’s great to know that you feel the value matches the price and that you’d recommend it to fellow coffee lovers. If you ever have any suggestions or need help tweaking the settings for a different roast, just let us know—we’re always here to help keep your coffee routine perfect.  \\n\\nEnjoy every cup! ☕️  \\n\\nWarm regards,  \\n[Your Name]  \\nCustomer Support Team  '}\n",
      "\n",
      "==================================================\n",
      "TESTING WITH REVIEW:\n",
      "Disappointed with this laptop. It's constantly overheating after just 30 minutes of use, \n",
      "and the ba...\n",
      "\n",
      "\n",
      "LCEL CHAIN RESULTS:\n",
      "==================================================\n",
      "{'review': \"Disappointed with this laptop. It's constantly overheating after just 30 minutes of use, \\nand the battery life is nowhere near the 8 hours advertised - I barely get 3 hours. \\nThe keyboard has already started sticking on several keys after just two weeks. Would not recommend to anyone.\", 'sentiment': 'SENTIMENT: negative', 'summary': '- Overheats rapidly, reaching high temperatures after only 30 minutes of use.  \\n- Battery life falls far short of claims—only about 3 hours versus the advertised 8.  \\n- Keyboard issues appear early, with several keys sticking after just two weeks.  \\n- Overall, the reviewer strongly advises against purchasing this laptop.', 'response': 'Dear [Customer Name],\\n\\nThank you for taking the time to share your experience with our laptop. I’m truly sorry to hear that it has not met your expectations, and I understand how frustrating it must be to deal with overheating, short battery life, and a keyboard that’s already showing signs of wear after just a couple of weeks.\\n\\n**Here’s how we can help:**\\n\\n1. **Overheating** – We recommend checking that the cooling vents are clear and that the laptop is on a hard, flat surface. If the issue persists, we can arrange for a diagnostic test at no extra cost.  \\n2. **Battery Life** – The 8‑hour claim is based on a'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Sample product reviews for testing\n",
    "positive_review = \"\"\"I absolutely love this coffee maker! It brews quickly and the coffee tastes amazing. \n",
    "The built-in grinder saves me so much time in the morning, and the programmable timer means \n",
    "I wake up to fresh coffee every day. Worth every penny and highly recommended to any coffee enthusiast.\"\"\"\n",
    "\n",
    "negative_review = \"\"\"Disappointed with this laptop. It's constantly overheating after just 30 minutes of use, \n",
    "and the battery life is nowhere near the 8 hours advertised - I barely get 3 hours. \n",
    "The keyboard has already started sticking on several keys after just two weeks. Would not recommend to anyone.\"\"\"\n",
    "\n",
    "# Step 1: Define the prompt templates for each processing step\n",
    "sentiment_template = \"\"\"Analyze the sentiment of the following product review as positive, negative, or neutral.\n",
    "Provide your analysis in the format: \"SENTIMENT: [positive/negative/neutral]\"\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Your analysis:\n",
    "\"\"\"\n",
    "\n",
    "summary_template = \"\"\"Summarize the following product review into 3-5 key bullet points.\n",
    "Each bullet point should be concise and capture an important aspect mentioned in the review.\n",
    "\n",
    "Review: {review}\n",
    "Sentiment: {sentiment}\n",
    "\n",
    "Key points:\n",
    "\"\"\"\n",
    "\n",
    "response_template = \"\"\"Write a helpful response to a customer based on their product review.\n",
    "If the sentiment is positive, thank them for their feedback. If negative, express understanding \n",
    "and suggest a solution or next steps. Personalize based on the specific points they mentioned.\n",
    "\n",
    "Review: {review}\n",
    "Sentiment: {sentiment}\n",
    "Key points: {summary}\n",
    "\n",
    "Response to customer:\n",
    "\"\"\"\n",
    "\n",
    "sentiment_prompt = PromptTemplate.from_template(template=sentiment_template)\n",
    "summary_prompt = PromptTemplate.from_template(template=summary_template)\n",
    "response_prompt = PromptTemplate.from_template(template=response_template)\n",
    "\n",
    "senitment_chain = (\n",
    "    sentiment_prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summary_chain = (\n",
    "    summary_prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response_chain = (\n",
    "    response_prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# RunnablePassthrough is designed to accept keyword arguments, \n",
    "# where each key becomes a new field added to the input dict\n",
    "overall_chain = (\n",
    "    RunnablePassthrough(sentiment=lambda x:sentiment_chain.invoke({\"review\":x[\"review\"]}))\n",
    "    | RunnablePassthrough(summary=lambda x:summary_chain.invoke({\"review\":x[\"review\"],\n",
    "                                                                 \"summary\":x[\"summary\"]}))\n",
    "    | RunnablePassthrough(response=lambda x:response_chain.invoke({\"review\":x[\"review\"],\n",
    "                                                                   \"summary\":x[\"summary\"],\n",
    "                                                                   \"response\":x[\"response\"]}))\n",
    ")\n",
    "\n",
    "def test_chains(review):\n",
    "    \"\"\"Test both chain implementations with the given review\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"TESTING WITH REVIEW:\\n{review[:100]}...\\n\")\n",
    "\n",
    "    \n",
    "    print(\"\\nLCEL CHAIN RESULTS:\")\n",
    "    result = lcel_chain.invoke({\"review\":review})\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(result)\n",
    "\n",
    "# Run tests\n",
    "test_chains(positive_review)\n",
    "test_chains(negative_review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35969f3",
   "metadata": {},
   "source": [
    "### Agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d6044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(model=\"openai/gpt-oss-20b\",\n",
    "                        model_provider=\"groq\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8f9861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool \n",
    "def calculator(eqn: str):\n",
    "    \"\"\"Solves the math equation and returns result\"\"\"\n",
    "    return eval(eqn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "deac43f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent \n",
    "\n",
    "agent = create_agent(model=model,\n",
    "                     tools=[calculator],\n",
    "                     system_prompt=\"You are a math expert. You help solve any equation with the tools available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33373609",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\"messages\":\"Solve 23*90**2 + 1092 - 129 =\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb7010a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The value of the expression is  \\n\\n\\\\[\\n23 \\\\times 90^{2} + 1092 - 129 = 187\\\\,263\\n\\\\]'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['messages'][-1].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
